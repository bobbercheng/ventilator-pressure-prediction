{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Transform_encoder_lstm_GPU-V16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bobbercheng/ventilator-pressure-prediction/blob/master/Transform_encoder_lstm_GPU_V16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh6Zg50KJQBQ"
      },
      "source": [
        "V1: Keep R/C, improve\n",
        "\n",
        "V2: Add R**2, no improve\n",
        "\n",
        "V3: Update model from https://www.kaggle.com/dlaststark/gb-vpp-pulp-fiction. Not much improve compare V1\n",
        "\n",
        "V4: Use whole db for test Fold-1\n",
        "Fold-1 | OOF Score: 0.15912771348961205\n",
        "\n",
        "Run it again in Kaggle:\n",
        "\n",
        "Fold-1 | OOF Score: 0.1606402072946816\n",
        "\n",
        "V5: Add loading ScaleLayer from saved model\n",
        "\n",
        "V6: Add TransformerEncoder. TransformerEncoder(128, 512, 8, name=\"transformer_layer\")(x). Testing in GPU.\n",
        "\n",
        "V7: Try to scale targe to 0-1, but it slow the train as MASE cannot be scaled.\n",
        "\n",
        "V8: Removed target scale.\n",
        "Epoch 00022: val_loss improved from 0.32912 to 0.30967, saving model to ./model-gpu/Bidirect_LSTM_model_1C.h5\n",
        "Epoch 23/150\n",
        "127/127 [==============================] - 161s 1s/step - loss: 0.3176 - val_loss: 0.2917\n",
        "\n",
        "Not Good as without TransformerEncoder. Change paramemter back to \n",
        "\n",
        "V8: Change attention result to multiple instead of concate.\n",
        "\n",
        "Loss is smaller than without LayerNormalization, but CV is bigger. It means it overfit the train data.\n",
        "\n",
        "TransformerEncoder(128, 512, 8, 0.3, name=\"transformer_layer\")\n",
        "\n",
        "Epoch 00181: val_loss did not improve from 0.16212\n",
        "Epoch 182/200\n",
        "127/127 [==============================] - 163s 1s/step - loss: 0.0448 - val_loss: 0.1655\n",
        "\n",
        "Epoch 00182: ReduceLROnPlateau reducing learning rate to 5.6313510867767036e-05.\n",
        "\n",
        "Epoch 00182: val_loss did not improve from 0.16212\n",
        "Restoring model weights from the end of the best epoch.\n",
        "Epoch 00182: early stopping\n",
        "Fold-1 | OOF Score: 0.16211654680482343\n",
        "\n",
        "V9: Increase attention dropout to 0.6, add 0.01 droupout to input\n",
        "Epoch 00123: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
        "\n",
        "Epoch 00123: val_loss did not improve from 0.16714\n",
        "Epoch 124/200\n",
        "127/127 - 161s - loss: 0.0838 - val_loss: 0.1694\n",
        "\n",
        "Epoch 00124: val_loss did not improve from 0.16714\n",
        "Epoch 125/200\n",
        "127/127 - 161s - loss: 0.0796 - val_loss: 0.1727\n",
        "\n",
        "V10: add TransformerEncoder to all layer with Multiply\n",
        "\n",
        "Epoch 164/200\n",
        "253/253 - 387s - loss: 0.0568 - val_loss: 0.1679\n",
        "\n",
        "Epoch 00164: val_loss did not improve from 0.16158\n",
        "Restoring model weights from the end of the best epoch.\n",
        "Epoch 00164: early stopping\n",
        "Fold-1 | OOF Score: 0.16158229343436334\n",
        "\n",
        "V10: analy_predict.\n",
        "\n",
        "Here is analyze result for V4:\n",
        "20__10 - MAE  0.16386306650915122, count: 25870\n",
        "20__20 - MAE  0.15764106283234391, count: 26272\n",
        "20__50 - MAE  0.16384391190488698, count: 34729\n",
        "50__10 - MAE  0.1585241009544335, count: 60574\n",
        "50__20 - MAE  0.24078390163257296, count: 37108\n",
        "50__50 - MAE  0.23760641023725088, count: 34699\n",
        "5__10 - MAE  0.15540978004136713, count: 36361\n",
        "5__20 - MAE  0.10102388763185534, count: 35935\n",
        "5__50 - MAE  0.11535413256937187, count: 35522\n",
        "\n",
        "\n",
        "V11: add dnn_model_GaussianNoise(), no improve\n",
        "\n",
        "Epoch 00183: val_loss did not improve from 0.16240\n",
        "Epoch 184/200\n",
        "127/127 - 150s - loss: 0.0498 - val_loss: 0.1642\n",
        "\n",
        "Epoch 00184: ReduceLROnPlateau reducing learning rate to 7.508467933803331e-05.\n",
        "\n",
        "Epoch 00184: val_loss did not improve from 0.16240\n",
        "Restoring model weights from the end of the best epoch.\n",
        "Epoch 00184: early stopping\n",
        "Fold-1 | OOF Score: 0.16240143234043636\n",
        "\n",
        "V12: dnn_model_aen(), use auto encoder to remove noise and learn encode.\n",
        "\n",
        "Epoch 200/200\n",
        "127/127 - 153s - loss: 4.1734 - decoder_loss: 3.8822 - ae_action_loss: 0.2257 - output_loss: 0.0654 - val_loss: 0.7126 - val_decoder_loss: 0.3742 - val_ae_action_loss: 0.1702 - val_output_loss: 0.1682\n",
        "\n",
        "Epoch 00200: val_loss did not improve from 0.69120\n",
        "\n",
        "Fold-1 | OOF Score: 0.16817480317739367\n",
        "20__10 - MAE  0.16644539428012722, count: 25870\n",
        "20__20 - MAE  0.15768315124243204, count: 26272\n",
        "20__50 - MAE  0.16684243562006307, count: 34729\n",
        "50__10 - MAE  0.16641273893399433, count: 60574\n",
        "50__20 - MAE  0.2518146440943941, count: 37108\n",
        "50__50 - MAE  0.2752187431993935, count: 34699\n",
        "5__10 - MAE  0.16763255549569941, count: 36361\n",
        "5__20 - MAE  0.10697168010290617, count: 35935\n",
        "5__50 - MAE  0.11503387157283243, count: 35522\n",
        "\n",
        "decoder_loss is very large. It means GaussianNoise() and dropout causes side effect.\n",
        "\n",
        "V13. Remove GaussianNoise and dropout, use encoder feature only. CV score is a little worse.\n",
        "\n",
        "Epoch 200/200\n",
        "127/127 - 150s - loss: 0.5841 - decoder_loss: 0.3983 - ae_action_loss: 0.1025 - output_loss: 0.0832 - val_loss: 0.4930 - val_decoder_loss: 0.1541 - val_ae_action_loss: 0.1693 - val_output_loss: 0.1696\n",
        "\n",
        "Epoch 00200: val_output_loss did not improve from 0.16793\n",
        "Fold-1 | OOF Score: 0.16959694784227083\n",
        "20__10 - MAE  0.18958046275757853, count: 25870\n",
        "20__20 - MAE  0.16960937531364753, count: 26272\n",
        "20__50 - MAE  0.1714088869810135, count: 34729\n",
        "50__10 - MAE  0.17159657573233353, count: 60574\n",
        "50__20 - MAE  0.2590598477978727, count: 37108\n",
        "50__50 - MAE  0.26932839218852966, count: 34699\n",
        "5__10 - MAE  0.16335297055531042, count: 36361\n",
        "5__20 - MAE  0.11217990298083265, count: 35935\n",
        "5__50 - MAE  0.12327488392594069, count: 35522\n",
        "\n",
        "V14: add RC_loss\n",
        "\n",
        "V15: change weight of RC_loss to 50\n",
        "Epoch 00209: val_loss did not improve from 2.45837\n",
        "Epoch 210/300\n",
        "Fold-1 | OOF Score: 0.1673458640660445\n",
        "20__10 - MAE  0.16871429221171652, count: 25870\n",
        "20__20 - MAE  0.17440932964643532, count: 26272\n",
        "20__50 - MAE  0.172235793440684, count: 34729\n",
        "50__10 - MAE  0.17126511889137416, count: 60574\n",
        "50__20 - MAE  0.2432283477323368, count: 37108\n",
        "50__50 - MAE  0.2597899337166222, count: 34699\n",
        "5__10 - MAE  0.16038654333096616, count: 36361\n",
        "5__20 - MAE  0.11624601028942623, count: 35935\n",
        "5__50 - MAE  0.12415466606839251, count: 35522\n",
        "\n",
        "V16: add transformer-lstm\n",
        "\n",
        "Use batch size 128 can get 0.13 even less with epoch 300\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uslq2jL49nGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aa6eba7-c43a-45ed-9cdd-ed83126a12f3"
      },
      "source": [
        "# Update pandas version for Colab TPU\n",
        "!pip install pandas==1.3.2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas==1.3.2 in /usr/local/lib/python3.7/dist-packages (1.3.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.2) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.2) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.2) (1.19.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.2) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBJlpUNI9com"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXzLriPA9coq"
      },
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "LOAD_PRE_FEATURE = True"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9esF__aG9cor",
        "outputId": "b6d9ac3b-f4ef-4c14-dc66-6097e1a4b6f3"
      },
      "source": [
        "if IN_COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount(\"/gdrive\", force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ_f3Z2M9cos",
        "outputId": "e0c64128-c381-49f2-98c1-ec16a2f5c8be"
      },
      "source": [
        "if IN_COLAB:\n",
        "  !pip install kaggle\n",
        "  !mkdir /root/.kaggle\n",
        "  !cp /gdrive/MyDrive/ventilator-pressure-prediction/kaggle.json /root/.kaggle\n",
        "  !kaggle competitions download -c ventilator-pressure-prediction\n",
        "  !kaggle datasets download -d  ventilatorpressurepredictionfeatures\n",
        "  !mkdir -p ../input/ventilator-pressure-prediction\n",
        "  !unzip '*.zip' -d ../input/ventilator-pressure-prediction\n",
        "  !ls ../input/ventilator-pressure-prediction\n",
        "  !mkdir -p ../input/ventilatorpressurepredictionfeatures/\n",
        "  !mv ../input/ventilator-pressure-prediction/*.npy ../input/ventilatorpressurepredictionfeatures\n",
        "  !mkdir /gdrive/MyDrive/ventilator-pressure-prediction/transformer_encoder_lstm_gpu\n",
        "  #Map Google driver folder to local folder model to save model\n",
        "  !ln -s /gdrive/MyDrive/ventilator-pressure-prediction/transformer_encoder_lstm_gpu model-gpu\n",
        "else:\n",
        "  !mkdir ./model-gpu\n",
        "  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading test.csv.zip to /content\n",
            " 76% 57.0M/75.4M [00:00<00:00, 84.1MB/s]\n",
            "100% 75.4M/75.4M [00:00<00:00, 111MB/s] \n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/8.50M [00:00<?, ?B/s]\n",
            "100% 8.50M/8.50M [00:00<00:00, 78.3MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 95% 132M/139M [00:00<00:00, 140MB/s]\n",
            "100% 139M/139M [00:00<00:00, 179MB/s]\n",
            "Downloading ventilatorpressurepredictionfeatures.zip to /content\n",
            "100% 1.94G/1.95G [00:12<00:00, 140MB/s]\n",
            "100% 1.95G/1.95G [00:12<00:00, 162MB/s]\n",
            "Archive:  sample_submission.csv.zip\n",
            "  inflating: ../input/ventilator-pressure-prediction/sample_submission.csv  \n",
            "\n",
            "Archive:  ventilatorpressurepredictionfeatures.zip\n",
            "  inflating: ../input/ventilator-pressure-prediction/test.npy  \n",
            "  inflating: ../input/ventilator-pressure-prediction/train.npy  \n",
            "\n",
            "Archive:  test.csv.zip\n",
            "  inflating: ../input/ventilator-pressure-prediction/test.csv  \n",
            "\n",
            "Archive:  train.csv.zip\n",
            "  inflating: ../input/ventilator-pressure-prediction/train.csv  \n",
            "\n",
            "4 archives were successfully processed.\n",
            "sample_submission.csv  test.csv  test.npy  train.csv  train.npy\n",
            "mkdir: cannot create directory ‘/gdrive/MyDrive/ventilator-pressure-prediction/transformer_encoder_lstm_gpu’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-21T17:30:55.782617Z",
          "iopub.execute_input": "2021-10-21T17:30:55.783998Z",
          "iopub.status.idle": "2021-10-21T17:30:55.793669Z",
          "shell.execute_reply.started": "2021-10-21T17:30:55.783937Z",
          "shell.execute_reply": "2021-10-21T17:30:55.792667Z"
        },
        "trusted": true,
        "id": "1TfjZidC9cos"
      },
      "source": [
        "import gc\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "import tensorflow as tf\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "  tf.config.experimental.set_memory_growth(gpu, True)\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.layers import Concatenate, LSTM, GRU\n",
        "from tensorflow.keras.layers import Bidirectional, Multiply\n",
        "\n",
        "\n",
        "\n",
        "# Random Seed Initialize\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "def seed_everything(seed=RANDOM_SEED):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    \n",
        "seed_everything()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XycyrO7g9cot"
      },
      "source": [
        "## Load datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-21T17:24:55.352925Z",
          "iopub.execute_input": "2021-10-21T17:24:55.353455Z",
          "iopub.status.idle": "2021-10-21T17:25:06.101910Z",
          "shell.execute_reply.started": "2021-10-21T17:24:55.353418Z",
          "shell.execute_reply": "2021-10-21T17:25:06.100969Z"
        },
        "trusted": true,
        "id": "p_vUMe_39cot",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "679994b2-1a37-4274-ebeb-69f3911b9c12"
      },
      "source": [
        "train_df = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\n",
        "print(f\"train_df: {train_df.shape}\")\n",
        "train_df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_df: (6036000, 8)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>breath_id</th>\n",
              "      <th>R</th>\n",
              "      <th>C</th>\n",
              "      <th>time_step</th>\n",
              "      <th>u_in</th>\n",
              "      <th>u_out</th>\n",
              "      <th>pressure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.083334</td>\n",
              "      <td>0</td>\n",
              "      <td>5.837492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>50</td>\n",
              "      <td>0.033652</td>\n",
              "      <td>18.383041</td>\n",
              "      <td>0</td>\n",
              "      <td>5.907794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>50</td>\n",
              "      <td>0.067514</td>\n",
              "      <td>22.509278</td>\n",
              "      <td>0</td>\n",
              "      <td>7.876254</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>50</td>\n",
              "      <td>0.101542</td>\n",
              "      <td>22.808822</td>\n",
              "      <td>0</td>\n",
              "      <td>11.742872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>50</td>\n",
              "      <td>0.135756</td>\n",
              "      <td>25.355850</td>\n",
              "      <td>0</td>\n",
              "      <td>12.234987</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  breath_id   R   C  time_step       u_in  u_out   pressure\n",
              "0   1          1  20  50   0.000000   0.083334      0   5.837492\n",
              "1   2          1  20  50   0.033652  18.383041      0   5.907794\n",
              "2   3          1  20  50   0.067514  22.509278      0   7.876254\n",
              "3   4          1  20  50   0.101542  22.808822      0  11.742872\n",
              "4   5          1  20  50   0.135756  25.355850      0  12.234987"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-21T17:25:11.213742Z",
          "iopub.execute_input": "2021-10-21T17:25:11.214675Z",
          "iopub.status.idle": "2021-10-21T17:25:11.297152Z",
          "shell.execute_reply.started": "2021-10-21T17:25:11.214617Z",
          "shell.execute_reply": "2021-10-21T17:25:11.295985Z"
        },
        "trusted": true,
        "id": "R5oBMjYJ9cov",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f61b9e4e-6fa5-4b19-9a31-078f83dac968"
      },
      "source": [
        "all_pressure = np.sort(train_df['pressure'].unique())\n",
        "pressure_min =  all_pressure[0].item()\n",
        "pressure_max = all_pressure[-1].item()\n",
        "pressure_step = (all_pressure[1] - all_pressure[0]).item()\n",
        "\n",
        "pressure_min,pressure_max,pressure_step"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.895744294564641, 64.8209917386395, 0.07030214545121005)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80hIcGg09cou"
      },
      "source": [
        "DEBUG = False\n",
        "\n",
        "if DEBUG:\n",
        "    train_df = train_df[:80*10000]\n",
        "\n",
        "targets = train_df[['pressure']].to_numpy().reshape(-1, 80)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-21T17:25:06.103441Z",
          "iopub.execute_input": "2021-10-21T17:25:06.104426Z",
          "iopub.status.idle": "2021-10-21T17:25:11.211412Z",
          "shell.execute_reply.started": "2021-10-21T17:25:06.104376Z",
          "shell.execute_reply": "2021-10-21T17:25:11.209506Z"
        },
        "trusted": true,
        "id": "y-0XhQ3a9cov",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "2904b323-e8cb-4665-b3aa-c5f743c4dadd"
      },
      "source": [
        "test_df = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\n",
        "print(f\"test_df: {test_df.shape}\")\n",
        "test_df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_df: (4024000, 7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>breath_id</th>\n",
              "      <th>R</th>\n",
              "      <th>C</th>\n",
              "      <th>time_step</th>\n",
              "      <th>u_in</th>\n",
              "      <th>u_out</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "      <td>0.031904</td>\n",
              "      <td>7.515046</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "      <td>0.063827</td>\n",
              "      <td>14.651675</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "      <td>0.095751</td>\n",
              "      <td>21.230610</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "      <td>0.127644</td>\n",
              "      <td>26.320956</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  breath_id  R   C  time_step       u_in  u_out\n",
              "0   1          0  5  20   0.000000   0.000000      0\n",
              "1   2          0  5  20   0.031904   7.515046      0\n",
              "2   3          0  5  20   0.063827  14.651675      0\n",
              "3   4          0  5  20   0.095751  21.230610      0\n",
              "4   5          0  5  20   0.127644  26.320956      0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVPWgQDL9cow"
      },
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-21T17:25:11.298767Z",
          "iopub.execute_input": "2021-10-21T17:25:11.299164Z",
          "iopub.status.idle": "2021-10-21T17:27:48.006762Z",
          "shell.execute_reply.started": "2021-10-21T17:25:11.299125Z",
          "shell.execute_reply": "2021-10-21T17:27:48.005658Z"
        },
        "trusted": true,
        "id": "1NHNQSZp9cow"
      },
      "source": [
        "def add_features(df):\n",
        "    df['cross']= df['u_in'] * df['u_out']\n",
        "    df['cross2']= df['time_step'] * df['u_out']\n",
        "    df['area'] = df['time_step'] * df['u_in']\n",
        "    df['area'] = df.groupby('breath_id')['area'].cumsum()\n",
        "    df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n",
        "    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
        "    print(\"Step-1...Completed\")\n",
        "    \n",
        "    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n",
        "    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n",
        "    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n",
        "    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n",
        "    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n",
        "    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n",
        "    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n",
        "    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n",
        "    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n",
        "    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n",
        "    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n",
        "    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n",
        "    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n",
        "    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n",
        "    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n",
        "    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n",
        "    df = df.fillna(0)\n",
        "    print(\"Step-2...Completed\")\n",
        "    \n",
        "    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n",
        "    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n",
        "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
        "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
        "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
        "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
        "    print(\"Step-3...Completed\")\n",
        "    \n",
        "    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n",
        "    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n",
        "    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n",
        "    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n",
        "    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n",
        "    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n",
        "    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n",
        "    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n",
        "    print(\"Step-4...Completed\")\n",
        "    \n",
        "    df['one'] = 1\n",
        "    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n",
        "    df['u_in_cummean'] =df['u_in_cumsum'] /df['count']\n",
        "    \n",
        "    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n",
        "    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n",
        "    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n",
        "    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n",
        "    df['breath_id__u_in_lag'] = df['u_in'].shift(1).fillna(0)\n",
        "    df['breath_id__u_in_lag'] = df['breath_id__u_in_lag'] * df['breath_id_lagsame']\n",
        "    df['breath_id__u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n",
        "    df['breath_id__u_in_lag2'] = df['breath_id__u_in_lag2'] * df['breath_id_lag2same']\n",
        "    print(\"Step-5...Completed\")\n",
        "    \n",
        "    df['time_step_diff'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n",
        "    df['ewm_u_in_mean'] = (df\\\n",
        "                           .groupby('breath_id')['u_in']\\\n",
        "                           .ewm(halflife=9)\\\n",
        "                           .mean()\\\n",
        "                           .reset_index(level=0,drop=True))\n",
        "    df[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\"]] = (df\\\n",
        "                                                              .groupby('breath_id')['u_in']\\\n",
        "                                                              .rolling(window=15,min_periods=1)\\\n",
        "                                                              .agg({\"15_in_sum\":\"sum\",\n",
        "                                                                    \"15_in_min\":\"min\",\n",
        "                                                                    \"15_in_max\":\"max\",\n",
        "                                                                    \"15_in_mean\":\"mean\"\n",
        "                                                                    #\"15_in_std\":\"std\"\n",
        "                                                               })\\\n",
        "                                                               .reset_index(level=0,drop=True))\n",
        "    print(\"Step-6...Completed\")\n",
        "    \n",
        "    #df['u_in_diff_1_2'] = df['u_in_lag1'] - df['u_in_lag2']\n",
        "    #df['u_out_diff_1_2'] = df['u_out_lag1'] - df['u_out_lag2']\n",
        "    #df['u_in_lagback_diff_1_2'] = df['u_in_lag_back1'] - df['u_in_lag_back2']\n",
        "    #df['u_out_lagback_diff_1_2'] = df['u_out_lag_back1'] - df['u_out_lag_back2']\n",
        "    \n",
        "    df['u_in_lagback_diff1'] = df['u_in'] - df['u_in_lag_back1']\n",
        "    df['u_out_lagback_diff1'] = df['u_out'] - df['u_out_lag_back1']\n",
        "    df['u_in_lagback_diff2'] = df['u_in'] - df['u_in_lag_back2']\n",
        "    df['u_out_lagback_diff2'] = df['u_out'] - df['u_out_lag_back2']\n",
        "    print(\"Step-7...Completed\")\n",
        "    \n",
        "    df['R_cat'] = df['R'].astype(str)\n",
        "    df['C_cat'] = df['C'].astype(str)\n",
        "    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n",
        "    df = pd.get_dummies(df)\n",
        "    print(\"Step-8...Completed\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "if not LOAD_PRE_FEATURE:\n",
        "  print(\"Train data...\\n\")\n",
        "  train = add_features(train_df)\n",
        "\n",
        "  print(\"\\nTest data...\\n\")\n",
        "  test = add_features(test_df)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcyv-iig9co0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5eb1b8-9562-43f0-f0ad-660d1508d227"
      },
      "source": [
        "del train_df\n",
        "del test_df\n",
        "gc.collect()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qip05b0r9co0"
      },
      "source": [
        "if not LOAD_PRE_FEATURE:\n",
        "  train.drop(['pressure','id', 'breath_id','one','count',\n",
        "              'breath_id_lag','breath_id_lag2','breath_id_lagsame',\n",
        "              'breath_id_lag2same'], axis=1, inplace=True)\n",
        "\n",
        "  test = test.drop(['id', 'breath_id','one','count','breath_id_lag',\n",
        "                    'breath_id_lag2','breath_id_lagsame',\n",
        "                    'breath_id_lag2same'], axis=1)\n",
        "\n",
        "  print(f\"train: {train.shape} \\ntest: {test.shape}\")\n",
        "  train.head()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYWzQtL49co1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b67ef8ce-041a-46e6-a2d6-ea7369e7f23f"
      },
      "source": [
        "if not LOAD_PRE_FEATURE:\n",
        "  scaler = RobustScaler()\n",
        "  train = scaler.fit_transform(train)\n",
        "  test = scaler.transform(test)\n",
        "\n",
        "  train = train.reshape(-1, 80, train.shape[-1])\n",
        "  test = test.reshape(-1, 80, train.shape[-1])\n",
        "else:\n",
        "  train = np.load('../input/ventilatorpressurepredictionfeatures/train.npy')\n",
        "  test = np.load('../input/ventilatorpressurepredictionfeatures/test.npy')\n",
        "\n",
        "print(f\"train: {train.shape} \\ntest: {test.shape} \\ntargets: {targets.shape}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: (75450, 80, 66) \n",
            "test: (50300, 80, 66) \n",
            "targets: (75450, 80)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VONzQKWBdixE"
      },
      "source": [
        "R_cat_20\tR_cat_5\tR_cat_50\tC_cat_10\tC_cat_20\tC_cat_50\tR__C_20__10\tR__C_20__20\tR__C_20__50\tR__C_50__10\tR__C_50__20\tR__C_50__50\tR__C_5__10\tR__C_5__20\tR__C_5__50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAMo89gTBbqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdca37a4-fac8-4a11-d735-1010f322d853"
      },
      "source": [
        "train[:3, 0, :]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00000000e+00,  7.50000000e-01, -9.89052161e-01,\n",
              "        -9.37384450e-01, -1.00000000e+00,  0.00000000e+00,\n",
              "        -6.65541311e-01, -5.16580501e-01, -4.96657632e-01,\n",
              "        -7.25227791e-01, -8.64120606e-01, -1.00000000e+00,\n",
              "         2.89020724e+00, -1.00000000e+00, -8.42526591e-01,\n",
              "        -1.00000000e+00,  3.66402550e+00, -1.00000000e+00,\n",
              "        -8.19257888e-01, -1.00000000e+00,  3.74391547e+00,\n",
              "        -1.00000000e+00, -7.94151600e-01, -1.00000000e+00,\n",
              "         4.27782420e+00, -1.00000000e+00,  1.12207936e-01,\n",
              "         0.00000000e+00,  3.26940916e-01,  1.27386353e+00,\n",
              "         4.71408803e-01,  0.00000000e+00,  2.17993186e-01,\n",
              "         0.00000000e+00,  1.21027987e-01,  0.00000000e+00,\n",
              "         6.83146754e-02,  0.00000000e+00, -6.48438329e-01,\n",
              "        -8.64120606e-01, -8.42526591e-01, -1.54759382e+01,\n",
              "        -6.74414709e-01, -8.06258228e-01,  7.89341177e-03,\n",
              "        -4.28435589e-01, -6.68749817e-01, -1.24171952e+02,\n",
              "         0.00000000e+00, -6.75689043e+01,  0.00000000e+00,\n",
              "         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "       [ 0.00000000e+00,  0.00000000e+00, -9.89052161e-01,\n",
              "         1.69886631e+00, -1.00000000e+00,  0.00000000e+00,\n",
              "        -6.65541311e-01, -5.16580501e-01, -4.96657632e-01,\n",
              "        -6.93368579e-01, -8.64120606e-01, -1.00000000e+00,\n",
              "         1.98375688e+00, -1.00000000e+00, -8.42526591e-01,\n",
              "        -1.00000000e+00,  1.66759161e+00, -1.00000000e+00,\n",
              "        -8.19257888e-01, -1.00000000e+00,  1.51578530e+00,\n",
              "        -1.00000000e+00, -7.94151600e-01, -1.00000000e+00,\n",
              "         1.61468559e+00, -1.00000000e+00, -2.33263833e-01,\n",
              "         0.00000000e+00, -4.03073965e-01, -1.27954654e+00,\n",
              "         7.55340661e+01,  0.00000000e+00,  3.89316910e+01,\n",
              "         0.00000000e+00,  2.52293431e+01,  0.00000000e+00,\n",
              "         1.74186111e+01,  0.00000000e+00,  4.24631929e-01,\n",
              "        -8.64120606e-01, -8.42526591e-01, -1.54759382e+01,\n",
              "         1.23221688e+00, -6.47954643e-01,  2.88771552e+00,\n",
              "         6.35990341e-01,  1.21524300e+00, -1.21453746e+01,\n",
              "         0.00000000e+00, -1.14038297e+00,  0.00000000e+00,\n",
              "         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "       [ 6.66666667e-01,  0.00000000e+00, -9.89052161e-01,\n",
              "        -9.55539088e-01, -1.00000000e+00,  0.00000000e+00,\n",
              "        -6.65541311e-01, -5.16580501e-01, -4.96657632e-01,\n",
              "        -7.25447190e-01, -8.64120606e-01, -1.00000000e+00,\n",
              "         5.85230064e-01, -1.00000000e+00, -8.42526591e-01,\n",
              "        -1.00000000e+00,  1.82123950e+00, -1.00000000e+00,\n",
              "        -8.19257888e-01, -1.00000000e+00,  2.59800328e+00,\n",
              "        -1.00000000e+00, -7.94151600e-01, -1.00000000e+00,\n",
              "         2.86855243e+00, -1.00000000e+00, -1.27840434e-01,\n",
              "         0.00000000e+00,  5.42062162e-02,  5.88358917e-01,\n",
              "        -4.55129464e-02,  0.00000000e+00, -4.86101177e-02,\n",
              "         0.00000000e+00, -5.18813481e-02,  0.00000000e+00,\n",
              "        -5.11687786e-02,  0.00000000e+00, -6.55828066e-01,\n",
              "        -8.64120606e-01, -8.42526591e-01, -1.54759382e+01,\n",
              "        -6.87544798e-01, -8.07348391e-01, -1.19385891e-02,\n",
              "        -4.35765797e-01, -6.81724002e-01, -4.87415356e+01,\n",
              "         0.00000000e+00, -4.01753866e+01,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
              "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AQScob_Bg43"
      },
      "source": [
        "# np.save('train.npy', train)\n",
        "# np.save('test.npy', test)\n",
        "# !zip features.zip train.npy test.npy"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osu14DeV9co1"
      },
      "source": [
        "## Hardware config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CisxazED9co1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a81b5d2-4157-422b-c270-4e2512809322"
      },
      "source": [
        "IS_TPU = False\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "    tf.config.experimental_connect_to_cluster(tpu)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n",
        "    print(\"Running on TPU:\", tpu.master())\n",
        "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "    IS_TPU = True    \n",
        "except ValueError:\n",
        "    tpu_strategy = tf.distribute.get_strategy()\n",
        "    BATCH_SIZE = 512\n",
        "    print(f\"Running on {tpu_strategy.num_replicas_in_sync} replicas\")\n",
        "    print(f\"Batch Size: {BATCH_SIZE}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on 1 replicas\n",
            "Batch Size: 512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAZMw10A9co1"
      },
      "source": [
        "## Rescaling layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLou5BDt9co2"
      },
      "source": [
        "@tf.custom_gradient\n",
        "def round_with_gradients(x):\n",
        "    def grad(dy):\n",
        "        return dy\n",
        "    return tf.round(x), grad\n",
        "\n",
        "class ScaleLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ScaleLayer, self).__init__(**kwargs)\n",
        "        self.min = tf.constant(pressure_min, dtype=np.float32)\n",
        "        self.max = tf.constant(pressure_max, dtype=np.float32)\n",
        "        self.step = tf.constant(pressure_step, dtype=np.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        steps = tf.math.divide(tf.math.add(inputs, -self.min), self.step)\n",
        "        int_steps = round_with_gradients(steps)\n",
        "        rescaled_steps = tf.math.add(tf.math.multiply(int_steps, self.step), self.min)\n",
        "        clipped = tf.clip_by_value(rescaled_steps, self.min, self.max)\n",
        "        return clipped\n",
        "    # def get_config(self):\n",
        "    #     return super(ScaleLayer, self).get_config()\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfmPPh-U2EUY"
      },
      "source": [
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, head_size, ff_dim, num_heads, dropout=0.3, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = head_size\n",
        "        self.dense_dim = ff_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.attention_dropout = tf.keras.layers.Dropout(dropout)\n",
        "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=head_size, dropout=dropout\n",
        "        )\n",
        "        self.attention_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.ff_conv1 = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu', name='ff_conv1')\n",
        "        self.ff_dropout = tf.keras.layers.Dropout(dropout)\n",
        "        self.ff_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "      self.ff_conv2 = tf.keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1, name='ff_conv2')\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "\n",
        "        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n",
        "        attention_output = self.attention_dropout(attention_output)\n",
        "        proj_input = self.attention_norm(inputs + attention_output)\n",
        "        proj_output = self.ff_conv1(proj_input)\n",
        "        proj_output = self.ff_conv2(proj_output)\n",
        "        proj_output = self.ff_dropout(proj_output)\n",
        "        return self.ff_norm(inputs + proj_output)\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = super(TransformerEncoder, self).get_config()\n",
        "        config.update({\"head_size\": self.embed_dim,\n",
        "            \"ff_dim\": self.dense_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dropout\": self.dropout})\n",
        "        return config\n",
        "\n",
        "#refer to https://www.kaggle.com/aimanlim0/transformer-lstm\n",
        "class Time2Vec(tf.keras.layers.Layer):\n",
        "    def __init__(self, kernel_size=1, **kwargs):\n",
        "        super(Time2Vec, self).__init__(**kwargs)\n",
        "        self.k = kernel_size\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        # print(input_shape)\n",
        "        # trend\n",
        "        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
        "        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
        "        # periodic\n",
        "        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
        "        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
        "        super(Time2Vec, self).build(input_shape)\n",
        "    \n",
        "    def call(self, inputs, **kwargs):\n",
        "        bias = self.wb * inputs + self.bb\n",
        "        dp = K.dot(inputs, self.wa) + self.ba\n",
        "        wgts = K.sin(dp) # or K.cos(.)\n",
        "\n",
        "        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n",
        "        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n",
        "        return ret\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1]*(self.k + 1))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Time2Vec, self).get_config()\n",
        "        config.update({\n",
        "            \"kernel_size\": self.k\n",
        "            })\n",
        "        return config\n",
        "\n",
        "class ModelTrunk(tf.keras.Model):\n",
        "    def __init__(self, name='ModelTrunk', time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0, **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n",
        "        if ff_dim is None:\n",
        "            ff_dim = head_size\n",
        "        self.dropout = dropout\n",
        "        self.attention_layers = [TransformerEncoder(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range(num_layers)]\n",
        "        #self.pooling=tf.keras.layers.AveragePooling1D(pool_size=4,data_format='channels_first')\n",
        "        self.final_layer = tf.keras.Sequential([\n",
        "                Bidirectional(LSTM(256, return_sequences=True)),\n",
        "                Bidirectional(LSTM(256, return_sequences=True)),\n",
        "                Dense(128, activation='selu'),\n",
        "                Dense(1)])\n",
        "    def call(self, inputs):\n",
        "        time_embedding = tf.keras.layers.TimeDistributed(self.time2vec)(inputs)\n",
        "        #time_embedding=inputs\n",
        "        #print(time_embedding.shape)\n",
        "        x = K.concatenate([inputs, time_embedding], -1)\n",
        "        for attention_layer in self.attention_layers:\n",
        "            x = attention_layer(x)\n",
        "        x=self.final_layer(x)\n",
        "        return x\n",
        "\n",
        "def do_TransformerEncoder_LSTM(inputs, time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0):\n",
        "    time2vec = Time2Vec(kernel_size=time2vec_dim)\n",
        "    if ff_dim is None:\n",
        "        ff_dim = head_size\n",
        "    dropout = dropout\n",
        "    attention_layers = [TransformerEncoder(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range(num_layers)]\n",
        "    #self.pooling=tf.keras.layers.AveragePooling1D(pool_size=4,data_format='channels_first')\n",
        "    final_layer = tf.keras.Sequential([\n",
        "            Bidirectional(LSTM(256, return_sequences=True)),\n",
        "            Bidirectional(LSTM(256, return_sequences=True)),\n",
        "            Dense(128, activation='selu')\n",
        "    ])\n",
        "    \n",
        "    time_embedding = tf.keras.layers.TimeDistributed(time2vec)(inputs)\n",
        "    x = K.concatenate([inputs, time_embedding], -1)\n",
        "    for attention_layer in attention_layers:\n",
        "        x = attention_layer(x)\n",
        "    x=final_layer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "        self.token_embeddings = tf.keras.layers.Layer.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = tf.keras.layers.Layer.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "\n",
        "class TransformerDecoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super(TransformerDecoder, self).__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = tf.keras.Sequential(\n",
        "            [Dense(latent_dim, activation=\"relu\"), Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        padding_mask = None\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "    \n",
        "    def get_config(self):\n",
        "        config = super(TransformerDecoder, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"latent_dim\": self.latent_dim,\n",
        "            \"num_heads\": self.num_heads\n",
        "            })\n",
        "        return config\n",
        "\n",
        "def TransformerEncoder_LSTM_model(time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0):\n",
        "    inputs = Input(shape=(train.shape[-2:]))\n",
        "    x = do_TransformerEncoder_LSTM(inputs, time2vec_dim, num_heads, head_size, ff_dim, num_layers, dropout)\n",
        "    x = Dense(1)(x)\n",
        "    x_output = ScaleLayer()(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x_output, \n",
        "                  name='DNN_Model')\n",
        "    return model    \n",
        "\n",
        "\n",
        "\n",
        "def Transformer_LSTM_model(time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0):\n",
        "    encoder_inputs = Input(shape=(train.shape[-2:]))\n",
        "    encoder_outputs = do_TransformerEncoder_LSTM(encoder_inputs, time2vec_dim, num_heads, head_size, ff_dim, num_layers, dropout)\n",
        "\n",
        "    encoder_model = Model(inputs=encoder_inputs, outputs=encoder_outputs, \n",
        "                  name='encoder_model')\n",
        "    \n",
        "    embed_dim = 128\n",
        "    latent_dim = 1024\n",
        "    decoder_inputs = Input(shape=(80,), name=\"decoder_inputs\")\n",
        "    encoded_seq_inputs = Input(shape=(80, embed_dim), name=\"decoder_state_inputs\")\n",
        "    \n",
        "    time2vec = Time2Vec(kernel_size=1)\n",
        "    time_embedding = tf.keras.layers.TimeDistributed(time2vec)\n",
        "    #Expand target and embeding with time. 80x2\n",
        "    x = K.expand_dims(decoder_inputs, -1)\n",
        "    x = time_embedding(x)\n",
        "    #Expand 80x2 to 80xembed_dim\n",
        "    x = Dense(embed_dim)(x)\n",
        "    \n",
        "    x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "#     x = Dropout(0.5)(x)\n",
        "    x = Dense(128, activation='selu')(x)\n",
        "    decoder_outputs = Dense(1)(x)\n",
        "    decoder = Model([decoder_inputs, encoded_seq_inputs], decoder_outputs, name='decoder_model')\n",
        "    decoder.summary()\n",
        "    \n",
        "    decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "    transformer = Model(\n",
        "        [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        "    )\n",
        "    \n",
        "    return transformer"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_lJ_vH8KmZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aec84277-d98b-443e-f71b-c2cd5e9ee5f3"
      },
      "source": [
        "model=ModelTrunk(ff_dim=256,num_heads=4,num_layers=2,)\n",
        "x=tf.random.uniform((64,80,66))\n",
        "x=model(x)\n",
        "print(x.shape)\n",
        "del model"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 80, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ft0JBPC9co2"
      },
      "source": [
        "## Keras DNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJn6J6temxSO"
      },
      "source": [
        "def dnn_model_v4():\n",
        "    \n",
        "    x_input = Input(shape=(train.shape[-2:]))\n",
        "    \n",
        "    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n",
        "    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n",
        "    x3 = Bidirectional(LSTM(units=384, return_sequences=True))(x2)\n",
        "    x4 = Bidirectional(LSTM(units=256, return_sequences=True))(x3)\n",
        "    x5 = Bidirectional(LSTM(units=128, return_sequences=True))(x4)\n",
        "    \n",
        "    z2 = Bidirectional(GRU(units=384, return_sequences=True))(x2)\n",
        "    \n",
        "    z31 = Multiply()([x3, z2])\n",
        "    z31 = BatchNormalization()(z31)\n",
        "    z3 = Bidirectional(GRU(units=256, return_sequences=True))(z31)\n",
        "    \n",
        "    z41 = Multiply()([x4, z3])\n",
        "    z41 = BatchNormalization()(z41)\n",
        "    z4 = Bidirectional(GRU(units=128, return_sequences=True))(z41)\n",
        "    \n",
        "    z51 = Multiply()([x5, z4])\n",
        "    z51 = BatchNormalization()(z51)\n",
        "    z5 = Bidirectional(GRU(units=64, return_sequences=True))(z51)\n",
        "    \n",
        "    x = Concatenate(axis=2)([x5, z2, z3, z4, z5])\n",
        "    \n",
        "    x = Dense(units=128, activation='selu')(x)\n",
        "    \n",
        "    x_output = Dense(units=1)(x)\n",
        "    \n",
        "    x_output = ScaleLayer()(x_output)\n",
        "\n",
        "    model = Model(inputs=x_input, outputs=x_output, \n",
        "                  name='DNN_Model')\n",
        "    return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vvobI5X05Od"
      },
      "source": [
        "#refer to https://www.kaggle.com/masato114/vp-wavenet-training-gpu\n",
        "# def conv_block(x: tf.Tensor, filters: int, kernel_size: int):\n",
        "#     \"\"\"\n",
        "#     Implements convolution block with residual connection.\n",
        "#     :param x: Input tensor.\n",
        "#     :param filters: Number of filters in convolution layer.\n",
        "#     :param kernel_size: Filter size.\n",
        "#     :return: Output tensor.\n",
        "#     \"\"\"\n",
        "#     x = tf.keras.layers.Conv1D(filters=filters,\n",
        "#                kernel_size=1,\n",
        "#                padding='same')(x)\n",
        "#     res_x = x\n",
        "#     x = tf.keras.layers.Conv1D(filters=filters,\n",
        "#                kernel_size=kernel_size,\n",
        "#                padding='same', activation='relu')(x)\n",
        "#     x = tf.keras.layers.Conv1D(filters=filters,\n",
        "#                kernel_size=kernel_size,\n",
        "#                padding='same', activation='relu')(x)\n",
        "#     x = tf.keras.layers.Conv1D(filters=filters,\n",
        "#                kernel_size=kernel_size,\n",
        "#                padding='same', activation='relu')(x)\n",
        "#     res_x = tf.keras.layers.Add()([res_x, x])\n",
        "#     return res_x\n",
        "\n",
        "# def wave_block(x: tf.Tensor, filters: int, kernel_size: int, n: int):\n",
        "#     \"\"\"\n",
        "#     Implements wavenet block.\n",
        "#     :param x: Input tensor.\n",
        "#     :param filters: Number of kernels.\n",
        "#     :param kernel_size: Filter size.\n",
        "#     :param n: Number of dilation rates for convolutions.\n",
        "#     :return: Output tensor.\n",
        "#     \"\"\"\n",
        "#     dilation_rates = [2 ** i for i in range(n)]\n",
        "#     x = tf.keras.layers.Conv1D(filters=filters,\n",
        "#                kernel_size=1,\n",
        "#                padding='same')(x)\n",
        "#     res_x = x\n",
        "#     for dilation_rate in dilation_rates:\n",
        "#         tanh_out = tf.keras.layers.Conv1D(filters=filters,\n",
        "#                           kernel_size=kernel_size,\n",
        "#                           padding='same',\n",
        "#                           activation='tanh',\n",
        "#                           dilation_rate=dilation_rate)(x)\n",
        "#         sigm_out = tf.keras.layers.Conv1D(filters=filters,\n",
        "#                           kernel_size=kernel_size,\n",
        "#                           padding='same',\n",
        "#                           activation='sigmoid',\n",
        "#                           dilation_rate=dilation_rate)(x)\n",
        "#         x = tf.keras.layers.Multiply()([tanh_out, sigm_out])\n",
        "#         x = tf.keras.layers.Conv1D(filters=filters,\n",
        "#                    kernel_size=1,\n",
        "#                    padding='same')(x)\n",
        "#         res_x = tf.keras.layers.Add()([res_x, x])\n",
        "#     return res_x\n",
        "\n",
        "# def do_wavenet(inp):\n",
        "#     x = conv_block(inp, 16, 3)\n",
        "#     x = wave_block(x, 16, 3, 12)\n",
        "#     x = conv_block(x, 32, 3)\n",
        "#     x = wave_block(x, 32, 3, 8)\n",
        "#     x = conv_block(x, 64, 3)\n",
        "#     x = wave_block(x, 64, 3, 4)\n",
        "#     x = conv_block(x, 128, 3)\n",
        "#     x = wave_block(x, 128, 3, 1)\n",
        "#     return x\n",
        "\n",
        "#refer to https://www.kaggle.com/meminozturk/into-the-wild-wavenet/\n",
        "i_wavenet_layer = 0\n",
        "def wave_block(x, filters, kernel_size, n):\n",
        "    global i_wavenet_layer\n",
        "    dilation_rates = [2**i for i in range(n)]\n",
        "    x = tf.keras.layers.Conv1D(filters = filters,\n",
        "                kernel_size = 1,\n",
        "                padding = 'same')(x)\n",
        "    res_x = x\n",
        "    for dilation_rate in dilation_rates:\n",
        "        tanh_out = tf.keras.layers.Conv1D(filters = filters,\n",
        "                          kernel_size = kernel_size,\n",
        "                          padding = 'same', \n",
        "                          activation = 'tanh', \n",
        "                          dilation_rate = dilation_rate)(x)\n",
        "        sigm_out = tf.keras.layers.Conv1D(filters = filters,\n",
        "                          kernel_size = kernel_size,\n",
        "                          padding = 'same',\n",
        "                          activation = 'sigmoid', \n",
        "                          dilation_rate = dilation_rate)(x)\n",
        "        x = tf.keras.layers.Multiply(name=f'wavenet_Multiply_{i_wavenet_layer}')([tanh_out, sigm_out])\n",
        "        x = tf.keras.layers.Conv1D(filters = filters,\n",
        "                    kernel_size = 1,\n",
        "                    padding = 'same')(x)\n",
        "        res_x = tf.keras.layers.Add()([res_x, x])\n",
        "        i_wavenet_layer += 1\n",
        "    return res_x\n",
        "\n",
        "def do_wavenet(inp):\n",
        "  x = wave_block(inp, 16, 3, 8)\n",
        "  x = wave_block(x, 32, 3, 12)\n",
        "  x = wave_block(x, 64, 3, 4)\n",
        "  x = wave_block(x, 128, 3, 1)\n",
        "  return x\n",
        "\n",
        "def load_model_until(checkpoint_name, drop_idx=-4, retrain=False):\n",
        "  print('Load pretrain weight from ', checkpoint_name)\n",
        "  load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "  try:\n",
        "    # At loading time, register the custom objects with a `custom_object_scope`:\n",
        "    custom_objects = {\"ScaleLayer\": ScaleLayer}\n",
        "    with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "        base_model = load_model(checkpoint_name, options=load_locally)\n",
        "  except (OSError):\n",
        "    print('Cannot load pretrain weight from ', checkpoint_name)\n",
        "\n",
        "  # base_model.summary()\n",
        "  base_output = base_model.layers[drop_idx].output\n",
        "  # Frozen layers\n",
        "  for layer in base_model.layers[0:drop_idx]:\n",
        "    layer.trainable = retrain\n",
        "\n",
        "  return base_model.inputs, base_output\n",
        "\n",
        "def load_dnn_model_v4_add_wavenet(drop_idx=-4, retrain=False):\n",
        "  base_inputs, base_output = load_model_until('/gdrive/MyDrive/ventilator-pressure-prediction/gb-rescaling-eda-v3/Bidirect_LSTM_model_1C.h5', drop_idx, \n",
        "                                              drop_idx=drop_idx, retrain=retrain)\n",
        "  \n",
        "  wavenet_output = do_wavenet(base_output)\n",
        "\n",
        "  global i_wavenet_layer\n",
        "  x_output = Dense(units=1, name=f'wavenet_Dense_{i_wavenet_layer}')(wavenet_output)\n",
        "  i_wavenet_layer += 1\n",
        "  x_output = ScaleLayer()(x_output)\n",
        "\n",
        "  new_model = tf.keras.models.Model(inputs=base_inputs, outputs=x_output)\n",
        "  # new_model.summary()\n",
        "\n",
        "  return new_model\n",
        "\n",
        "def lstm_wavenet_model():\n",
        "    x_input = Input(shape=(train.shape[-2:]))\n",
        "    \n",
        "    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n",
        "    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n",
        "    x3 = Bidirectional(LSTM(units=384, return_sequences=True))(x2)\n",
        "    x4 = Bidirectional(LSTM(units=256, return_sequences=True))(x3)\n",
        "    x5 = Bidirectional(LSTM(units=128, return_sequences=True))(x4)\n",
        "    \n",
        "    z2 = Bidirectional(GRU(units=384, return_sequences=True))(x2)\n",
        "    \n",
        "    z31 = Multiply()([x3, z2])\n",
        "    z31 = BatchNormalization()(z31)\n",
        "    z3 = Bidirectional(GRU(units=256, return_sequences=True))(z31)\n",
        "    \n",
        "    z41 = Multiply()([x4, z3])\n",
        "    z41 = BatchNormalization()(z41)\n",
        "    z4 = Bidirectional(GRU(units=128, return_sequences=True))(z41)\n",
        "    \n",
        "    z51 = Multiply()([x5, z4])\n",
        "    z51 = BatchNormalization()(z51)\n",
        "    z5 = Bidirectional(GRU(units=64, return_sequences=True))(z51)\n",
        "    \n",
        "    x = Concatenate(axis=2)([x5, z2, z3, z4, z5])\n",
        "    \n",
        "    x = do_wavenet(x)\n",
        "    \n",
        "    x = Dense(units=128, activation='selu')(x)\n",
        "    x_output = Dense(units=1)(x)\n",
        "    \n",
        "    x_output = ScaleLayer()(x_output)\n",
        "\n",
        "    model = Model(inputs=x_input, outputs=x_output, \n",
        "                  name='lstm_wavenet_model')\n",
        "    return model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwdrVcpdwjvc"
      },
      "source": [
        "def dnn_model_aen():\n",
        "    \n",
        "    x_input = Input(shape=(train.shape[-2:]))\n",
        "\n",
        "    x0 = tf.keras.layers.BatchNormalization()(x_input)\n",
        "    \n",
        "    #remove noise\n",
        "    # encoder = tf.keras.layers.GaussianNoise(0.03527936123679956)(x0)\n",
        "    encoder = tf.keras.layers.Dense(16)(x0)\n",
        "    encoder = tf.keras.layers.BatchNormalization()(encoder)\n",
        "    encoder = tf.keras.layers.Activation('swish')(encoder)\n",
        "    \n",
        "    decoder = tf.keras.layers.Dropout(0.0)(encoder)\n",
        "    decoder = tf.keras.layers.Dense(train.shape[-1], name = 'decoder')(decoder)\n",
        "\n",
        "\n",
        "    # x = tf.keras.layers.Concatenate()([x0, encoder])\n",
        "    x = encoder\n",
        "    \n",
        "    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x)\n",
        "    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n",
        "    x3 = Bidirectional(LSTM(units=384, return_sequences=True))(x2)\n",
        "    x4 = Bidirectional(LSTM(units=256, return_sequences=True))(x3)\n",
        "    x5 = Bidirectional(LSTM(units=128, return_sequences=True))(x4)\n",
        "\n",
        "    x_ae = tf.keras.layers.Dense(128, activation='swish')(x5)\n",
        "    x_ae = tf.keras.layers.Dropout(0.0)(x_ae)\n",
        "\n",
        "    out_ae = tf.keras.layers.Dense(1, name = 'ae_action')(x_ae)\n",
        "\n",
        "    z2 = Bidirectional(GRU(units=384, return_sequences=True))(x2)\n",
        "    \n",
        "    z31 = Multiply()([x3, z2])\n",
        "    z31 = BatchNormalization()(z31)\n",
        "    z3 = Bidirectional(GRU(units=256, return_sequences=True))(z31)\n",
        "    \n",
        "    z41 = Multiply()([x4, z3])\n",
        "    z41 = BatchNormalization()(z41)\n",
        "    z4 = Bidirectional(GRU(units=128, return_sequences=True))(z41)\n",
        "    \n",
        "    z51 = Multiply()([x5, z4])\n",
        "    z51 = BatchNormalization()(z51)\n",
        "    z5 = Bidirectional(GRU(units=64, return_sequences=True))(z51)\n",
        "    \n",
        "    x = Concatenate(axis=2)([x5, z2, z3, z4, z5])\n",
        "    \n",
        "    x = Dense(units=128, activation='swish')(x)\n",
        "    \n",
        "    x_output = Dense(units=1)(x)\n",
        "    \n",
        "    x_output = ScaleLayer(name = 'output')(x_output)\n",
        "\n",
        "    model = Model(inputs=x_input, outputs=[decoder, out_ae, x_output], \n",
        "                  name='dnn_model_aen')\n",
        "    return model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82jplaDkoBg9"
      },
      "source": [
        "def dnn_model_GaussianNoise():\n",
        "    \n",
        "    x_input = Input(shape=(train.shape[-2:]))\n",
        "    \n",
        "    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n",
        "    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n",
        "    x3 = Bidirectional(LSTM(units=384, return_sequences=True))(x2)\n",
        "    x4 = Bidirectional(LSTM(units=256, return_sequences=True))(x3)\n",
        "    x5 = Bidirectional(LSTM(units=128, return_sequences=True))(x4)\n",
        "    \n",
        "    z2 = Bidirectional(GRU(units=384, return_sequences=True))(x2)\n",
        "    \n",
        "    z31 = Multiply()([x3, z2])\n",
        "    z31 = BatchNormalization()(z31)\n",
        "    z31 = tf.keras.layers.GaussianNoise(0.035)(z31)\n",
        "    z3 = Bidirectional(GRU(units=256, return_sequences=True))(z31)\n",
        "    \n",
        "    z41 = Multiply()([x4, z3])\n",
        "    z41 = BatchNormalization()(z41)\n",
        "    z4 = Bidirectional(GRU(units=128, return_sequences=True))(z41)\n",
        "    \n",
        "    z51 = Multiply()([x5, z4])\n",
        "    z51 = BatchNormalization()(z51)\n",
        "    z5 = Bidirectional(GRU(units=64, return_sequences=True))(z51)\n",
        "    \n",
        "    x = Concatenate(axis=2)([x5, z2, z3, z4, z5])\n",
        "    \n",
        "    x = Dense(units=128, activation='selu')(x)\n",
        "    \n",
        "    x_output = Dense(units=1)(x)\n",
        "    \n",
        "    x_output = ScaleLayer()(x_output)\n",
        "\n",
        "    model = Model(inputs=x_input, outputs=x_output, \n",
        "                  name='dnn_model_GaussianNoise')\n",
        "    return model"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HuMFGY39co2"
      },
      "source": [
        "def dnn_TransformerEncoder_model():\n",
        "    \n",
        "    x_input = Input(shape=(train.shape[-2:]))\n",
        "    \n",
        "    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n",
        "    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n",
        "    x3 = Bidirectional(LSTM(units=384, return_sequences=True))(x2)\n",
        "    x4 = Bidirectional(LSTM(units=256, return_sequences=True))(x3)\n",
        "    x5 = Bidirectional(LSTM(units=128, return_sequences=True))(x4)\n",
        "    \n",
        "    z2 = Bidirectional(GRU(units=384, return_sequences=True))(x2)\n",
        "\n",
        "    dropout_r = 0.01\n",
        "    \n",
        "    z31 = Multiply()([x3, z2])\n",
        "    z31 = BatchNormalization()(z31)\n",
        "    z31_transformer = TransformerEncoder(384*2, 512*2, 8, dropout_r, name=\"transformer_layer_z31\")(z31)\n",
        "    z31 = Multiply()([z31, z31_transformer])\n",
        "    \n",
        "    z3 = Bidirectional(GRU(units=256, return_sequences=True))(z31)\n",
        "    \n",
        "    z41 = Multiply()([x4, z3])\n",
        "    z41 = BatchNormalization()(z41)\n",
        "    z41_transformer = TransformerEncoder(256*2, 384*2, 8, dropout_r, name=\"transformer_layer_z41\")(z41)\n",
        "    z41 = Multiply()([z41, z41_transformer])\n",
        "    \n",
        "    z4 = Bidirectional(GRU(units=128, return_sequences=True))(z41)\n",
        "    \n",
        "    z51 = Multiply()([x5, z4])\n",
        "    z51 = BatchNormalization()(z51)\n",
        "    z51_transformer = TransformerEncoder(128*2, 256*2, 8, dropout_r, name=\"transformer_layer_z51\")(z51)\n",
        "    z51 = Multiply()([z51, z51_transformer])\n",
        "    \n",
        "    z5 = Bidirectional(GRU(units=64, return_sequences=True))(z51)\n",
        "    \n",
        "    x = Concatenate(axis=2)([x5, z2, z3, z4, z5])\n",
        "\n",
        "    x = Dense(units=128)(x)\n",
        "\n",
        "    x_transformer = BatchNormalization()(x)\n",
        "    x_transformer = TransformerEncoder(128, 512, 8, dropout_r, name=\"transformer_layer_128\")(x_transformer)\n",
        "    x = Multiply()([Dropout(0.01)(x), x_transformer])\n",
        "    \n",
        "    x = Dense(units=128, activation='selu')(x)\n",
        "    \n",
        "    x_output = Dense(units=1)(x)\n",
        "    \n",
        "    x_output = ScaleLayer()(x_output)\n",
        "\n",
        "    model = Model(inputs=x_input, outputs=x_output, \n",
        "                  name='dnn_TransformerEncoder_model')\n",
        "    return model"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d8ybRJc9co2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d71cf1-2955-45ea-d61d-8dff1bc55890"
      },
      "source": [
        "model = Transformer_LSTM_model(ff_dim=256,num_heads=8,num_layers=2,)\n",
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"decoder_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "decoder_inputs (InputLayer)     [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims (TFOpLambda)     (None, 80, 1)        0           decoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_2 (TimeDistrib (None, 80, 2)        4           tf.expand_dims[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 80, 128)      384         time_distributed_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decoder_state_inputs (InputLaye [(None, 80, 128)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "transformer_decoder (Transforme (None, 80, 128)      1319040     dense_3[0][0]                    \n",
            "                                                                 decoder_state_inputs[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 80, 128)      16512       transformer_decoder[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 80, 1)        129         dense_6[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,336,069\n",
            "Trainable params: 1,336,069\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 80, 66)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 80, 132)      264         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.concat (TFOpLambda)          (None, 80, 198)      0           input_1[0][0]                    \n",
            "                                                                 time_distributed_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder_2 (Transfor (None, 80, 198)      916900      tf.concat[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "transformer_encoder_3 (Transfor (None, 80, 198)      916900      transformer_encoder_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "decoder_inputs (InputLayer)     [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 80, 128)      2572416     transformer_encoder_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "decoder_model (Functional)      (None, 80, 1)        1336069     decoder_inputs[0][0]             \n",
            "                                                                 sequential_1[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 5,742,549\n",
            "Trainable params: 5,742,549\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJtk8i_HSlnc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        },
        "outputId": "697c20a9-5d02-4186-dc9f-7e3eb5d7f9a4"
      },
      "source": [
        "plot_model(\n",
        "    model, \n",
        "    to_file='Google_Brain_Keras_Model.png', \n",
        "    show_shapes=True,\n",
        "    show_layer_names=True\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxWQVBYm-0VK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83d4d4fe-8d32-4284-81ee-b232c200c917"
      },
      "source": [
        "del model\n",
        "gc.collect()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25823"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1v9SMji9co3"
      },
      "source": [
        "#reset Keras Session\n",
        "def reset_keras():\n",
        "    sess = tf.compat.v1.keras.backend.get_session()\n",
        "    tf.compat.v1.keras.backend.clear_session()\n",
        "    sess.close()\n",
        "    sess = tf.compat.v1.keras.backend.get_session()\n",
        "\n",
        "    # use the same config as you used to create the session\n",
        "    config = tf.compat.v1.ConfigProto()\n",
        "    config.gpu_options.per_process_gpu_memory_fraction = 1\n",
        "    config.gpu_options.visible_device_list = \"0\"\n",
        "    tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))\n",
        "    gc.collect()\n",
        "\n",
        "reset_keras()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5bA7_YiN-P9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "def press_diff(series: np.ndarray):\n",
        "  return series.diff().abs()\n",
        "\n",
        "\n",
        "def analy_predict(last_test_idx, train_pred):\n",
        "  train_df = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\n",
        "  last_test_breath_ids = train_df['breath_id'].values.reshape(-1, 80)\n",
        "  last_test_breath_ids = last_test_breath_ids[:, 0]\n",
        "  last_test_breath_ids = last_test_breath_ids.flatten()\n",
        "  last_test_breath_ids = last_test_breath_ids[last_test_idx]\n",
        "\n",
        "  train_pred = train_pred[last_test_idx]\n",
        "  train_pred = train_pred.flatten()\n",
        "\n",
        "  train_df = train_df[train_df['breath_id'].isin(last_test_breath_ids)]\n",
        "  train_df['pred'] = train_pred\n",
        "  train_df['abs_error'] = np.abs(train_pred-train_df['pressure'])\n",
        "  train_df['RC'] = train_df[\"R\"].astype(str) + '__' + train_df[\"C\"].astype(str)\n",
        "  train_df['press_diff'] = train_df.groupby('breath_id')['pressure'].apply(press_diff)\n",
        "\n",
        "  df_feature = train_df[train_df['u_out']<1].groupby('breath_id').agg({'abs_error': ['mean' , 'max', 'min', 'sum', 'count'], \n",
        "                                                                      'RC': ['first'], \n",
        "                                                                      'press_diff':['mean', 'max'],\n",
        "                                                                      'u_in':['first', 'last'],\n",
        "                                                                      'pressure':['first', 'last', 'mean', 'std']\n",
        "                                                                      })\n",
        "  df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "\n",
        "  plt.figure(figsize=(16,16))\n",
        "\n",
        "  plt.title(\"abs_error distribution by RC\")\n",
        "\n",
        "  for key, grp in df_feature.groupby(['RC_first']):\n",
        "    print(f'{key} - MAE  {grp.abs_error_mean.mean()}, count: {grp.abs_error_count.sum()}')\n",
        "    sns.kdeplot(grp['abs_error_mean'], clip=(0.0, 1), \n",
        "                label=key)\n",
        "\n",
        "  plt.axvline(df_feature['abs_error_mean'].mean())\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1G5ttKepai8"
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "    skip_folds = np.arange(1)\n",
        "    VERBOSE = 2\n",
        "    EPOCHS = 200\n",
        "    FOLDS = 7\n",
        "    if DEBUG:\n",
        "      EPOCHS = 100\n",
        "    test_preds = []\n",
        "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=2021)\n",
        "    last_test_idx = []\n",
        "    \n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
        "        last_test_idx = test_idx\n",
        "        if len(test_preds) > fold:\n",
        "          print(f'Ignore corrent fold {fold+1} as we already predicted test data')\n",
        "\n",
        "        if DEBUG and fold > 0:\n",
        "          continue\n",
        "        X_train, X_valid = train[train_idx], train[test_idx]\n",
        "        y_train, y_valid = targets[train_idx], targets[test_idx]\n",
        "        \n",
        "        model = dnn_model_GaussianNoise()\n",
        "        model.compile(optimizer=\"adam\", loss=\"mae\")\n",
        "\n",
        "        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.75, \n",
        "                               patience=10, verbose=VERBOSE)\n",
        "        checkpoint_name = f'./model-gpu/Bidirect_LSTM_model_{fold+1}C.h5'\n",
        "        chk_point = ModelCheckpoint(checkpoint_name,\n",
        "                                    monitor='val_loss', verbose=VERBOSE, \n",
        "                                    save_best_only=True, mode='min')\n",
        "\n",
        "        es = EarlyStopping(monitor=\"val_loss\", patience=50,\n",
        "                           verbose=VERBOSE, mode=\"min\",\n",
        "                           restore_best_weights=True)\n",
        "        \n",
        "        if fold not in skip_folds:\n",
        "          model.fit(X_train, y_train, \n",
        "                    validation_data=(X_valid, y_valid), \n",
        "                    epochs=EPOCHS,\n",
        "                    verbose=VERBOSE,\n",
        "                    batch_size=BATCH_SIZE, \n",
        "                    callbacks=[lr, chk_point, es])\n",
        "        else:\n",
        "          print('Load pretrain weight from ', checkpoint_name)\n",
        "          load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "          try:\n",
        "            # At loading time, register the custom objects with a `custom_object_scope`:\n",
        "            custom_objects = {\"ScaleLayer\": ScaleLayer}\n",
        "            with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "                model = load_model(checkpoint_name, options=load_locally)\n",
        "          except (OSError):\n",
        "            print('Cannot load pretrain weight from ', checkpoint_name)\n",
        "            break\n",
        "                \n",
        "        y_true = y_valid.squeeze().reshape(-1, 1)\n",
        "        y_pred = model.predict(X_valid, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1)\n",
        "        train_pred[test_idx] = y_pred.reshape(-1, 80)\n",
        "        score = mean_absolute_error(y_true, y_pred)\n",
        "        test_preds.append(model.predict(test, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1).squeeze())\n",
        "\n",
        "        print(f\"Fold-{fold+1} | OOF Score: {score}\")\n",
        "        \n",
        "        del X_train, X_valid, y_train, y_valid, model\n",
        "        reset_keras()\n",
        "        gc.collect()\n",
        "\n",
        "        analy_predict(test_idx, train_pred)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INtJvC7xfKdV"
      },
      "source": [
        "skip_folds = []#np.arange(1)\n",
        "train_pred = np.zeros(targets.shape)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgZKi4Q-2aoC"
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "    \n",
        "    VERBOSE = 2\n",
        "    EPOCHS = 200\n",
        "    FOLDS = 7\n",
        "    if DEBUG:\n",
        "      EPOCHS = 100\n",
        "    test_preds = []\n",
        "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=2021)\n",
        "    last_test_idx = []\n",
        "    \n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
        "        last_test_idx = test_idx\n",
        "        if len(test_preds) > fold:\n",
        "          print(f'Ignore corrent fold {fold+1} as we already predicted test data')\n",
        "\n",
        "        if DEBUG and fold > 0:\n",
        "          continue\n",
        "        X_train, X_valid = train[train_idx], train[test_idx]\n",
        "        y_train, y_valid = targets[train_idx], targets[test_idx]\n",
        "        \n",
        "        model = dnn_model_aen()\n",
        "        model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3), \n",
        "              # loss=\"mae\"\n",
        "              loss = {\n",
        "                  'decoder': tf.keras.losses.MeanSquaredError(), \n",
        "                  #'decoder': tf.keras.losses.MeanAbsoluteError(), \n",
        "                  'ae_action': tf.keras.losses.MeanAbsoluteError(),\n",
        "                  'output': tf.keras.losses.MeanAbsoluteError(), \n",
        "                  },\n",
        "              loss_weights = [1, 1, 1]\n",
        "              )\n",
        "\n",
        "        lr = ReduceLROnPlateau(monitor=\"val_output_loss\", factor=0.75, \n",
        "                               patience=10, verbose=VERBOSE)\n",
        "        checkpoint_name = f'./model-gpu/Bidirect_LSTM_model_{fold+1}C.h5'\n",
        "        chk_point = ModelCheckpoint(checkpoint_name,\n",
        "                                    monitor='val_output_loss', \n",
        "                                    verbose=VERBOSE, \n",
        "                                    save_best_only=True, mode='min')\n",
        "\n",
        "        es = EarlyStopping(monitor=\"val_output_loss\", patience=50,\n",
        "                           verbose=VERBOSE, mode=\"min\",\n",
        "                           restore_best_weights=True)\n",
        "        \n",
        "        if fold not in skip_folds:\n",
        "          model.fit(X_train, [X_train, y_train, y_train], \n",
        "                    validation_data=(X_valid, [X_valid, y_valid, y_valid]), \n",
        "                    epochs=EPOCHS,\n",
        "                    verbose=VERBOSE,\n",
        "                    batch_size=BATCH_SIZE, \n",
        "                    callbacks=[lr, chk_point, es])\n",
        "        else:\n",
        "          print('Load pretrain weight from ', checkpoint_name)\n",
        "          load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "          try:\n",
        "            # At loading time, register the custom objects with a `custom_object_scope`:\n",
        "            custom_objects = {\"ScaleLayer\": ScaleLayer}\n",
        "            with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "                model = load_model(checkpoint_name, options=load_locally)\n",
        "          except (OSError):\n",
        "            print('Cannot load pretrain weight from ', checkpoint_name)\n",
        "            break\n",
        "                \n",
        "        y_true = y_valid.squeeze().reshape(-1, 1)\n",
        "        y_pred = model.predict(X_valid, batch_size=BATCH_SIZE)[-1].squeeze().reshape(-1, 1)\n",
        "        train_pred[test_idx] = y_pred.reshape(-1, 80)\n",
        "        score = mean_absolute_error(y_true, y_pred)\n",
        "        test_preds.append(model.predict(test, batch_size=BATCH_SIZE)[-1].squeeze().reshape(-1, 1).squeeze())\n",
        "\n",
        "        print(f\"Fold-{fold+1} | OOF Score: {score}\")\n",
        "        \n",
        "        del X_train, X_valid, y_train, y_valid, model\n",
        "        reset_keras()\n",
        "        gc.collect()\n",
        "\n",
        "        analy_predict(test_idx, train_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOju5NCSivUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a7e90b1-ca18-4116-ab10-dd1eb4057753"
      },
      "source": [
        "train.shape, targets.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((75450, 80, 66), (75450, 80))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RI6-1k8RCws"
      },
      "source": [
        "TRANSFORMER_START = -2\n",
        "TRANSFORMER_BLANK = -3\n",
        "TRANSFORMER_LAST_IDX = 38\n",
        "\n",
        "def make_transformer_targets(np_data, np_features):\n",
        "  #insert start\n",
        "  np_data_with_start = np.insert(np_data, 0, TRANSFORMER_START, axis=1)\n",
        "  #add blank in the end\n",
        "  np_data_with_start[:, TRANSFORMER_LAST_IDX:] = TRANSFORMER_BLANK\n",
        "  return np_data_with_start\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYPmE2GVeNCk"
      },
      "source": [
        "# Filter only RC==5050 or 5020\n",
        "retrain_idx = np.logical_or((train[:, 0, -4] == 1), (train[:, 0, -5] == 1))\n",
        "train = train[retrain_idx]\n",
        "targets = targets[retrain_idx]\n",
        "train.shape, targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx4IqaDowlN2"
      },
      "source": [
        "def RC_loss(y_true, y_pred, weight=50, cols = 80):\n",
        "    rc = y_true[:, cols: ]\n",
        "    y = y_true[:, :cols ]\n",
        "\n",
        "    w = 1 + rc*50\n",
        "    mae = w * tf.abs(y - y_pred)\n",
        "    return tf.reduce_sum(mae, axis=-1) / 80"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Px5fe-WszKu"
      },
      "source": [
        "#Sometime TF creats model with same weight name and it causes failure of save.\n",
        "def show_weight_names(model):\n",
        "  for i in range(len(model.weights)):\n",
        "    print(i, model.weights[i].name)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S_JFVeDg7Gz"
      },
      "source": [
        "'''\n",
        "Index(['R', 'C', 'time_step', 'u_in', 'u_out', 'cross', 'cross2', 'area',\n",
        "       'time_step_cumsum', 'u_in_cumsum', 'u_in_lag1', 'u_out_lag1',\n",
        "       'u_in_lag_back1', 'u_out_lag_back1', 'u_in_lag2', 'u_out_lag2',\n",
        "       'u_in_lag_back2', 'u_out_lag_back2', 'u_in_lag3', 'u_out_lag3',\n",
        "       'u_in_lag_back3', 'u_out_lag_back3', 'u_in_lag4', 'u_out_lag4',\n",
        "       'u_in_lag_back4', 'u_out_lag_back4', 'breath_id__u_in__max',\n",
        "       'breath_id__u_out__max', 'breath_id__u_in__diffmax',\n",
        "       'breath_id__u_in__diffmean', 'u_in_diff1', 'u_out_diff1', 'u_in_diff2',\n",
        "       'u_out_diff2', 'u_in_diff3', 'u_out_diff3', 'u_in_diff4', 'u_out_diff4',\n",
        "       'u_in_cummean', 'breath_id__u_in_lag', 'breath_id__u_in_lag2',\n",
        "       'time_step_diff', 'ewm_u_in_mean', '15_in_sum', '15_in_min',\n",
        "       '15_in_max', '15_in_mean', 'u_in_lagback_diff1', 'u_out_lagback_diff1',\n",
        "       'u_in_lagback_diff2', 'u_out_lagback_diff2', 'R_cat_20', 'R_cat_5',\n",
        "       'R_cat_50', 'C_cat_10', 'C_cat_20', 'C_cat_50', 'R__C_20__10',\n",
        "       'R__C_20__20', 'R__C_20__50', 'R__C_50__10', 'R__C_50__20',\n",
        "       'R__C_50__50', 'R__C_5__10', 'R__C_5__20', 'R__C_5__50'],\n",
        "      dtype='object')\n",
        "      \n",
        "u_out: index -4, 0->-1, 1->0\n",
        "'''\n",
        "TRANSFORMER_START = -2\n",
        "TRANSFORMER_BLANK = -3\n",
        "TRANSFORMER_LAST_IDX = 80\n",
        "U_OUT_INDEX = -4\n",
        "# np_data: nx80, np_features: nx80x66\n",
        "def make_transformer_targets(np_data):\n",
        "    #insert start\n",
        "    np_data_with_start = np.insert(np_data, 0, TRANSFORMER_START, axis=1)\n",
        "    #add blank in the end\n",
        "    np_data_with_start[:, TRANSFORMER_LAST_IDX:] = TRANSFORMER_BLANK\n",
        "    return np_data_with_start[:, :-1], np_data_with_start[:, 1:]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY1_23VYhSCj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0448b739-72a2-4380-9af8-cf6aca5280d3"
      },
      "source": [
        "skip_folds = np.arange(6)\n",
        "train_pred = np.zeros(targets.shape)\n",
        "\n",
        "\n",
        "BATCH_SIZE=256\n",
        "IS_WAVENET=False\n",
        "USE_RC_LOSS=False\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "    \n",
        "    VERBOSE = 2\n",
        "    EPOCHS = 300\n",
        "    FOLDS = 7\n",
        "    if DEBUG:\n",
        "      EPOCHS = 100\n",
        "    test_preds = []\n",
        "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=2021)\n",
        "    last_test_idx = []\n",
        "    \n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
        "        last_test_idx = test_idx\n",
        "        if len(test_preds) > fold:\n",
        "          print(f'Ignore corrent fold {fold+1} as we already predicted test data')\n",
        "\n",
        "        if DEBUG and fold > 0:\n",
        "          continue\n",
        "\n",
        "        X_train, X_valid = train[train_idx], train[test_idx]\n",
        "        y_train, y_valid = targets[train_idx], targets[test_idx]\n",
        "        #Align data to batch size and drop remainder. We only need it for TPU not GPU\n",
        "        if IS_TPU and IS_WAVENET:\n",
        "          fit_batch_size = X_train.shape[0] - (X_train.shape[0]%BATCH_SIZE)\n",
        "          print(X_train.shape[0], fit_batch_size)\n",
        "          X_train, y_train = X_train[:fit_batch_size, :, :], y_train[:fit_batch_size, :]\n",
        "          \n",
        "          fit_batch_size = X_valid.shape[0] - (X_valid.shape[0]%BATCH_SIZE)\n",
        "          print(X_valid.shape[0], fit_batch_size)\n",
        "          X_valid, y_valid = X_valid[:fit_batch_size, :, :], y_valid[:fit_batch_size, :]\n",
        "        \n",
        "        checkpoint_name = f'./model-gpu/transform_encoder_lstm_gpu_{fold+1}C.h5'\n",
        "\n",
        "        if fold not in skip_folds:\n",
        "          # model = load_dnn_model_v4_add_wavenet(-3, True)\n",
        "          # model = lstm_wavenet_model()\n",
        "          # model = dnn_TransformerEncoder_model()\n",
        "          model = Transformer_LSTM_model(ff_dim=256,num_heads=8,num_layers=2,)\n",
        "          #show_weight_names(model)\n",
        "          model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
        "                        # loss=RC_loss\n",
        "                        loss=tf.keras.losses.MeanAbsoluteError()\n",
        "                        )\n",
        "          \n",
        "          \n",
        "\n",
        "          lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.75, \n",
        "                                patience=10, verbose=VERBOSE)\n",
        "          \n",
        "          # scheduler = tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 5, 0.9)\n",
        "          # lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "\n",
        "          chk_point = ModelCheckpoint(checkpoint_name,\n",
        "                                      monitor='val_loss', verbose=VERBOSE, \n",
        "                                      save_best_only=True, mode='min')\n",
        "\n",
        "          es = EarlyStopping(monitor=\"val_loss\", patience=50,\n",
        "                            verbose=VERBOSE, mode=\"min\",\n",
        "                            restore_best_weights=True)\n",
        "\n",
        "\n",
        "          if os.path.isfile(checkpoint_name):\n",
        "            model.load_weights(checkpoint_name)\n",
        "            print('Resumed previous weight from ', checkpoint_name)\n",
        "\n",
        "          if USE_RC_LOSS:\n",
        "            model.fit(X_train, np.append(y_train, X_train[:, :, -4]+X_train[:, :, -5], axis =1), \n",
        "                      validation_data=(X_valid, np.append(y_valid, X_valid[:, :, -4]+X_valid[:, :, -5], axis =1)), \n",
        "                      epochs=EPOCHS,\n",
        "                      verbose=VERBOSE,\n",
        "                      batch_size=BATCH_SIZE, \n",
        "                      callbacks=[lr, chk_point, es])\n",
        "          else:\n",
        "            train_decoder_inputs, train_decoder_outputs = make_transformer_targets(y_train)\n",
        "            valid_decoder_inputs, valid_decoder_outputs = make_transformer_targets(y_valid)\n",
        "\n",
        "            print(train_decoder_inputs[0], train_decoder_outputs[0], valid_decoder_inputs[0], valid_decoder_outputs[0])\n",
        "\n",
        "            model.fit([X_train, train_decoder_inputs], train_decoder_outputs, \n",
        "                      validation_data=([X_valid, valid_decoder_inputs], valid_decoder_outputs), \n",
        "                      epochs=EPOCHS,\n",
        "                      verbose=VERBOSE,\n",
        "                      batch_size=BATCH_SIZE, \n",
        "                      callbacks=[lr, chk_point, es])\n",
        "        else:\n",
        "          print('Load pretrain weight from ', checkpoint_name)\n",
        "          load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "          try:\n",
        "            # At loading time, register the custom objects with a `custom_object_scope`:\n",
        "            custom_objects = {\"ScaleLayer\": ScaleLayer, \"RC_loss\": RC_loss, \n",
        "                              \"Time2Vec\": Time2Vec, \n",
        "                              \"TransformerEncoder\": TransformerEncoder,\n",
        "                              'TransformerDecoder': TransformerDecoder}\n",
        "            with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "                model = load_model(checkpoint_name, options=load_locally)\n",
        "                print('Loaded pretrain weight from ', checkpoint_name)\n",
        "          except (OSError):\n",
        "            print('Cannot load pretrain weight from ', checkpoint_name)\n",
        "            break\n",
        "                \n",
        "        # y_true = y_valid.squeeze().reshape(-1, 1)\n",
        "        # y_pred = model.predict(X_valid, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1)\n",
        "        # score = mean_absolute_error(y_true, y_pred)\n",
        "        # print(f\"Fold-{fold+1} | OOF Score: {score}\")\n",
        "\n",
        "        # #Save CV predict\n",
        "        # if not (IS_TPU and IS_WAVENET):\n",
        "        #   train_pred[test_idx] = y_pred.reshape(-1, 80)\n",
        "        #   test_preds.append(model.predict(test, batch_size=1).squeeze().reshape(-1, 1).squeeze())\n",
        "\n",
        "        \n",
        "        del X_train, X_valid, y_train, y_valid, model\n",
        "        reset_keras()\n",
        "        gc.collect()\n",
        "\n",
        "        # if not (IS_TPU and IS_WAVENET):\n",
        "        #   analy_predict(test_idx, train_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load pretrain weight from  ./model-gpu/transform_encoder_lstm_gpu_1C.h5\n",
            "Loaded pretrain weight from  ./model-gpu/transform_encoder_lstm_gpu_1C.h5\n",
            "Load pretrain weight from  ./model-gpu/transform_encoder_lstm_gpu_2C.h5\n",
            "Loaded pretrain weight from  ./model-gpu/transform_encoder_lstm_gpu_2C.h5\n",
            "Load pretrain weight from  ./model-gpu/transform_encoder_lstm_gpu_3C.h5\n",
            "Loaded pretrain weight from  ./model-gpu/transform_encoder_lstm_gpu_3C.h5\n",
            "Load pretrain weight from  ./model-gpu/transform_encoder_lstm_gpu_4C.h5\n",
            "Loaded pretrain weight from  ./model-gpu/transform_encoder_lstm_gpu_4C.h5\n",
            "Load pretrain weight from  ./model-gpu/transform_encoder_lstm_gpu_5C.h5\n",
            "Loaded pretrain weight from  ./model-gpu/transform_encoder_lstm_gpu_5C.h5\n",
            "Load pretrain weight from  ./model-gpu/transform_encoder_lstm_gpu_6C.h5\n",
            "Loaded pretrain weight from  ./model-gpu/transform_encoder_lstm_gpu_6C.h5\n",
            "Model: \"decoder_model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "decoder_inputs (InputLayer)     [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims (TFOpLambda)     (None, 80, 1)        0           decoder_inputs[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 80, 2)        4           tf.expand_dims[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 80, 128)      384         time_distributed_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decoder_state_inputs (InputLaye [(None, 80, 128)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "transformer_decoder (Transforme (None, 80, 128)      1319040     dense_1[0][0]                    \n",
            "                                                                 decoder_state_inputs[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 80, 128)      16512       transformer_decoder[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 80, 1)        129         dense_4[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 1,336,069\n",
            "Trainable params: 1,336,069\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Resumed previous weight from  ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "[-2.          5.83749171  5.90779385  7.87625392 11.74287192 12.23498694\n",
            " 12.86770625 14.69556203 15.8906985  15.53918778 15.75009421 17.29674141\n",
            " 17.22643927 16.17190709 17.36704356 18.07006501 17.15613712 18.28097145\n",
            " 18.77308647 17.85915858 19.1245972  19.33550363 18.49187789 18.56218003\n",
            " 18.63248218 18.84338861 19.05429505 19.26520149 19.33550363 19.33550363\n",
            " 19.47610792 19.54641007 17.01553283  9.56350541  7.87625392  8.64957752\n",
            "  7.59504534  7.66534749  8.2980668   7.24353461  7.94655607  7.59504534\n",
            "  7.45444105  8.1574625   6.82172174  7.17323247  7.5247432   6.96232603\n",
            "  7.45444105  7.59504534  7.17323247  7.73564963  7.24353461  7.59504534\n",
            "  7.3841389   7.24353461  7.73564963  7.5247432   6.96232603  7.10293032\n",
            "  7.31383676  6.11870029  7.03262818  6.82172174  6.54051316  6.96232603\n",
            "  6.82172174  6.54051316  6.82172174  6.82172174  6.39990887  6.7514196\n",
            "  6.54051316  6.39990887  6.7514196   6.47021101  6.39990887  6.61081531\n",
            "  6.32960672  6.54051316] [ 5.83749171  5.90779385  7.87625392 11.74287192 12.23498694 12.86770625\n",
            " 14.69556203 15.8906985  15.53918778 15.75009421 17.29674141 17.22643927\n",
            " 16.17190709 17.36704356 18.07006501 17.15613712 18.28097145 18.77308647\n",
            " 17.85915858 19.1245972  19.33550363 18.49187789 18.56218003 18.63248218\n",
            " 18.84338861 19.05429505 19.26520149 19.33550363 19.33550363 19.47610792\n",
            " 19.54641007 17.01553283  9.56350541  7.87625392  8.64957752  7.59504534\n",
            "  7.66534749  8.2980668   7.24353461  7.94655607  7.59504534  7.45444105\n",
            "  8.1574625   6.82172174  7.17323247  7.5247432   6.96232603  7.45444105\n",
            "  7.59504534  7.17323247  7.73564963  7.24353461  7.59504534  7.3841389\n",
            "  7.24353461  7.73564963  7.5247432   6.96232603  7.10293032  7.31383676\n",
            "  6.11870029  7.03262818  6.82172174  6.54051316  6.96232603  6.82172174\n",
            "  6.54051316  6.82172174  6.82172174  6.39990887  6.7514196   6.54051316\n",
            "  6.39990887  6.7514196   6.47021101  6.39990887  6.61081531  6.32960672\n",
            "  6.54051316 -3.        ] [-2.          6.32960672  7.3841389   8.36836894  9.00108825  9.77441185\n",
            " 10.12592258 10.82894403 11.2507569  11.95377836 11.74287192 12.65679981\n",
            " 12.72710196 13.50042556 13.711332   14.41435345 14.62525989 14.76586418\n",
            " 15.82039636 16.52341781 17.08583498 17.71855429 17.50764785 17.57795\n",
            " 17.85915858 17.99976287 18.42157574 18.63248218 18.84338861 19.26520149\n",
            " 19.40580578 18.3512736  12.23498694 11.11015261 10.89924618  9.63380756\n",
            " 11.2507569   8.79018181  8.43867109  9.56350541  7.87625392  7.5247432\n",
            "  8.43867109  7.10293032  6.54051316  7.17323247  6.54051316  7.17323247\n",
            "  6.25930458  6.39990887  6.68111745  5.978096    6.54051316  5.76718956\n",
            "  6.18900243  5.978096    5.48598098  6.18900243  5.2047724   6.11870029\n",
            "  4.85326167  5.34537669  5.48598098  5.13447025  5.34537669  5.2047724\n",
            "  5.34537669  5.34537669  5.27507454  5.34537669  5.34537669  5.55628312\n",
            "  5.62658527  5.69688741  5.76718956  5.76718956  5.83749171  5.83749171\n",
            "  5.83749171  6.04839814] [ 6.32960672  7.3841389   8.36836894  9.00108825  9.77441185 10.12592258\n",
            " 10.82894403 11.2507569  11.95377836 11.74287192 12.65679981 12.72710196\n",
            " 13.50042556 13.711332   14.41435345 14.62525989 14.76586418 15.82039636\n",
            " 16.52341781 17.08583498 17.71855429 17.50764785 17.57795    17.85915858\n",
            " 17.99976287 18.42157574 18.63248218 18.84338861 19.26520149 19.40580578\n",
            " 18.3512736  12.23498694 11.11015261 10.89924618  9.63380756 11.2507569\n",
            "  8.79018181  8.43867109  9.56350541  7.87625392  7.5247432   8.43867109\n",
            "  7.10293032  6.54051316  7.17323247  6.54051316  7.17323247  6.25930458\n",
            "  6.39990887  6.68111745  5.978096    6.54051316  5.76718956  6.18900243\n",
            "  5.978096    5.48598098  6.18900243  5.2047724   6.11870029  4.85326167\n",
            "  5.34537669  5.48598098  5.13447025  5.34537669  5.2047724   5.34537669\n",
            "  5.34537669  5.27507454  5.34537669  5.34537669  5.55628312  5.62658527\n",
            "  5.69688741  5.76718956  5.76718956  5.83749171  5.83749171  5.83749171\n",
            "  6.04839814 -3.        ]\n",
            "Epoch 1/300\n",
            "253/253 - 94s - loss: 0.3520 - val_loss: 0.3005\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.30054, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 2/300\n",
            "253/253 - 69s - loss: 0.3303 - val_loss: 0.3479\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.30054\n",
            "Epoch 3/300\n",
            "253/253 - 69s - loss: 0.3393 - val_loss: 0.3682\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.30054\n",
            "Epoch 4/300\n",
            "253/253 - 69s - loss: 0.3263 - val_loss: 0.3276\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.30054\n",
            "Epoch 5/300\n",
            "253/253 - 69s - loss: 0.3356 - val_loss: 0.3390\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.30054\n",
            "Epoch 6/300\n",
            "253/253 - 69s - loss: 0.3217 - val_loss: 0.3311\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.30054\n",
            "Epoch 7/300\n",
            "253/253 - 69s - loss: 0.3308 - val_loss: 0.3279\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.30054\n",
            "Epoch 8/300\n",
            "253/253 - 69s - loss: 0.3236 - val_loss: 0.3165\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.30054\n",
            "Epoch 9/300\n",
            "253/253 - 69s - loss: 0.3199 - val_loss: 0.3384\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.30054\n",
            "Epoch 10/300\n",
            "253/253 - 69s - loss: 0.3320 - val_loss: 0.3413\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.30054\n",
            "Epoch 11/300\n",
            "253/253 - 69s - loss: 0.3392 - val_loss: 0.3274\n",
            "\n",
            "Epoch 00011: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.30054\n",
            "Epoch 12/300\n",
            "253/253 - 69s - loss: 0.2932 - val_loss: 0.3130\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.30054\n",
            "Epoch 13/300\n",
            "253/253 - 69s - loss: 0.2851 - val_loss: 0.3121\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.30054\n",
            "Epoch 14/300\n",
            "253/253 - 69s - loss: 0.2873 - val_loss: 0.3078\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.30054\n",
            "Epoch 15/300\n",
            "253/253 - 68s - loss: 0.2992 - val_loss: 0.3234\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.30054\n",
            "Epoch 16/300\n",
            "253/253 - 68s - loss: 0.2984 - val_loss: 0.3159\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.30054\n",
            "Epoch 17/300\n",
            "253/253 - 68s - loss: 0.2879 - val_loss: 0.3060\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.30054\n",
            "Epoch 18/300\n",
            "253/253 - 69s - loss: 0.2944 - val_loss: 0.2951\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.30054 to 0.29505, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 19/300\n",
            "253/253 - 69s - loss: 0.2997 - val_loss: 0.3407\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.29505\n",
            "Epoch 20/300\n",
            "253/253 - 69s - loss: 0.3130 - val_loss: 0.3077\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.29505\n",
            "Epoch 21/300\n",
            "253/253 - 69s - loss: 0.2950 - val_loss: 0.3511\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.29505\n",
            "Epoch 22/300\n",
            "253/253 - 69s - loss: 0.2949 - val_loss: 0.2901\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.29505 to 0.29006, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 23/300\n",
            "253/253 - 69s - loss: 0.2951 - val_loss: 0.3049\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.29006\n",
            "Epoch 24/300\n",
            "253/253 - 68s - loss: 0.2980 - val_loss: 0.3405\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.29006\n",
            "Epoch 25/300\n",
            "253/253 - 69s - loss: 0.2964 - val_loss: 0.3296\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.29006\n",
            "Epoch 26/300\n",
            "253/253 - 69s - loss: 0.2961 - val_loss: 0.3168\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.29006\n",
            "Epoch 27/300\n",
            "253/253 - 69s - loss: 0.2986 - val_loss: 0.3206\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.29006\n",
            "Epoch 28/300\n",
            "253/253 - 69s - loss: 0.2970 - val_loss: 0.2966\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.29006\n",
            "Epoch 29/300\n",
            "253/253 - 69s - loss: 0.2913 - val_loss: 0.3328\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.29006\n",
            "Epoch 30/300\n",
            "253/253 - 69s - loss: 0.2862 - val_loss: 0.3206\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.29006\n",
            "Epoch 31/300\n",
            "253/253 - 68s - loss: 0.2869 - val_loss: 0.2964\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.29006\n",
            "Epoch 32/300\n",
            "253/253 - 69s - loss: 0.2880 - val_loss: 0.3062\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0005625000048894435.\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.29006\n",
            "Epoch 33/300\n",
            "253/253 - 69s - loss: 0.2714 - val_loss: 0.3052\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.29006\n",
            "Epoch 34/300\n",
            "253/253 - 68s - loss: 0.2768 - val_loss: 0.2822\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.29006 to 0.28225, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 35/300\n",
            "253/253 - 69s - loss: 0.2743 - val_loss: 0.3035\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.28225\n",
            "Epoch 36/300\n",
            "253/253 - 69s - loss: 0.2728 - val_loss: 0.2788\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.28225 to 0.27883, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 37/300\n",
            "253/253 - 69s - loss: 0.2687 - val_loss: 0.2939\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.27883\n",
            "Epoch 38/300\n",
            "253/253 - 69s - loss: 0.2773 - val_loss: 0.3618\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.27883\n",
            "Epoch 39/300\n",
            "253/253 - 69s - loss: 0.2666 - val_loss: 0.2949\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.27883\n",
            "Epoch 40/300\n",
            "253/253 - 69s - loss: 0.2760 - val_loss: 0.2787\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.27883 to 0.27866, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 41/300\n",
            "253/253 - 69s - loss: 0.2774 - val_loss: 0.3116\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.27866\n",
            "Epoch 42/300\n",
            "253/253 - 69s - loss: 0.2809 - val_loss: 0.2780\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.27866 to 0.27797, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 43/300\n",
            "253/253 - 69s - loss: 0.2703 - val_loss: 0.3046\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.27797\n",
            "Epoch 44/300\n",
            "253/253 - 69s - loss: 0.2732 - val_loss: 0.3381\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.27797\n",
            "Epoch 45/300\n",
            "253/253 - 69s - loss: 0.2777 - val_loss: 0.2874\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.27797\n",
            "Epoch 46/300\n",
            "253/253 - 69s - loss: 0.2691 - val_loss: 0.2907\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.27797\n",
            "Epoch 47/300\n",
            "253/253 - 69s - loss: 0.2660 - val_loss: 0.2825\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.27797\n",
            "Epoch 48/300\n",
            "253/253 - 69s - loss: 0.2741 - val_loss: 0.2808\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.27797\n",
            "Epoch 49/300\n",
            "253/253 - 69s - loss: 0.2674 - val_loss: 0.2860\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.27797\n",
            "Epoch 50/300\n",
            "253/253 - 69s - loss: 0.2728 - val_loss: 0.2781\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.27797\n",
            "Epoch 51/300\n",
            "253/253 - 69s - loss: 0.2799 - val_loss: 0.2908\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.27797\n",
            "Epoch 52/300\n",
            "253/253 - 69s - loss: 0.2719 - val_loss: 0.3089\n",
            "\n",
            "Epoch 00052: ReduceLROnPlateau reducing learning rate to 0.0004218749818392098.\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.27797\n",
            "Epoch 53/300\n",
            "253/253 - 69s - loss: 0.2531 - val_loss: 0.2859\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.27797\n",
            "Epoch 54/300\n",
            "253/253 - 68s - loss: 0.2529 - val_loss: 0.2793\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.27797\n",
            "Epoch 55/300\n",
            "253/253 - 68s - loss: 0.2541 - val_loss: 0.2869\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.27797\n",
            "Epoch 56/300\n",
            "253/253 - 68s - loss: 0.2559 - val_loss: 0.2808\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.27797\n",
            "Epoch 57/300\n",
            "253/253 - 68s - loss: 0.2527 - val_loss: 0.2674\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.27797 to 0.26738, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 58/300\n",
            "253/253 - 68s - loss: 0.2534 - val_loss: 0.2721\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.26738\n",
            "Epoch 59/300\n",
            "253/253 - 68s - loss: 0.2506 - val_loss: 0.2531\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.26738 to 0.25312, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 60/300\n",
            "253/253 - 68s - loss: 0.2503 - val_loss: 0.2982\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.25312\n",
            "Epoch 61/300\n",
            "253/253 - 68s - loss: 0.2486 - val_loss: 0.3084\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.25312\n",
            "Epoch 62/300\n",
            "253/253 - 68s - loss: 0.2588 - val_loss: 0.2987\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.25312\n",
            "Epoch 63/300\n",
            "253/253 - 69s - loss: 0.2584 - val_loss: 0.2750\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.25312\n",
            "Epoch 64/300\n",
            "253/253 - 68s - loss: 0.2387 - val_loss: 0.2499\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.25312 to 0.24988, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 65/300\n",
            "253/253 - 68s - loss: 0.2477 - val_loss: 0.2479\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.24988 to 0.24788, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 66/300\n",
            "253/253 - 68s - loss: 0.2563 - val_loss: 0.2812\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.24788\n",
            "Epoch 67/300\n",
            "253/253 - 68s - loss: 0.2466 - val_loss: 0.2794\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.24788\n",
            "Epoch 68/300\n",
            "253/253 - 69s - loss: 0.2577 - val_loss: 0.2769\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.24788\n",
            "Epoch 69/300\n",
            "253/253 - 68s - loss: 0.2550 - val_loss: 0.2815\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.24788\n",
            "Epoch 70/300\n",
            "253/253 - 68s - loss: 0.2554 - val_loss: 0.2466\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.24788 to 0.24659, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 71/300\n",
            "253/253 - 68s - loss: 0.2492 - val_loss: 0.2771\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.24659\n",
            "Epoch 72/300\n",
            "253/253 - 69s - loss: 0.2481 - val_loss: 0.2466\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.24659\n",
            "Epoch 73/300\n",
            "253/253 - 69s - loss: 0.2534 - val_loss: 0.2974\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.24659\n",
            "Epoch 74/300\n",
            "253/253 - 68s - loss: 0.2458 - val_loss: 0.2995\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.24659\n",
            "Epoch 75/300\n",
            "253/253 - 68s - loss: 0.2475 - val_loss: 0.3134\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.24659\n",
            "Epoch 76/300\n",
            "253/253 - 68s - loss: 0.2504 - val_loss: 0.2507\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.24659\n",
            "Epoch 77/300\n",
            "253/253 - 68s - loss: 0.2420 - val_loss: 0.2801\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.24659\n",
            "Epoch 78/300\n",
            "253/253 - 68s - loss: 0.2355 - val_loss: 0.2515\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.24659\n",
            "Epoch 79/300\n",
            "253/253 - 68s - loss: 0.2569 - val_loss: 0.2759\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.24659\n",
            "Epoch 80/300\n",
            "253/253 - 68s - loss: 0.2511 - val_loss: 0.2773\n",
            "\n",
            "Epoch 00080: ReduceLROnPlateau reducing learning rate to 0.00031640623637940735.\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.24659\n",
            "Epoch 81/300\n",
            "253/253 - 68s - loss: 0.2499 - val_loss: 0.2801\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.24659\n",
            "Epoch 82/300\n",
            "253/253 - 68s - loss: 0.2539 - val_loss: 0.2760\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.24659\n",
            "Epoch 83/300\n",
            "253/253 - 68s - loss: 0.2452 - val_loss: 0.2744\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.24659\n",
            "Epoch 84/300\n",
            "253/253 - 68s - loss: 0.2432 - val_loss: 0.3095\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.24659\n",
            "Epoch 85/300\n",
            "253/253 - 68s - loss: 0.2264 - val_loss: 0.2813\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.24659\n",
            "Epoch 86/300\n",
            "253/253 - 68s - loss: 0.2180 - val_loss: 0.2179\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.24659 to 0.21787, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 87/300\n",
            "253/253 - 68s - loss: 0.2219 - val_loss: 0.2128\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.21787 to 0.21283, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 88/300\n",
            "253/253 - 68s - loss: 0.2363 - val_loss: 0.2249\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.21283\n",
            "Epoch 89/300\n",
            "253/253 - 68s - loss: 0.2205 - val_loss: 0.2725\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.21283\n",
            "Epoch 90/300\n",
            "253/253 - 68s - loss: 0.2334 - val_loss: 0.2352\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.21283\n",
            "Epoch 91/300\n",
            "253/253 - 68s - loss: 0.2088 - val_loss: 0.2315\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.21283\n",
            "Epoch 92/300\n",
            "253/253 - 68s - loss: 0.1958 - val_loss: 0.2396\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.21283\n",
            "Epoch 93/300\n",
            "253/253 - 68s - loss: 0.2302 - val_loss: 0.2249\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.21283\n",
            "Epoch 94/300\n",
            "253/253 - 68s - loss: 0.2309 - val_loss: 0.2729\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.21283\n",
            "Epoch 95/300\n",
            "253/253 - 68s - loss: 0.2478 - val_loss: 0.2700\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.21283\n",
            "Epoch 96/300\n",
            "253/253 - 68s - loss: 0.2418 - val_loss: 0.2296\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.21283\n",
            "Epoch 97/300\n",
            "253/253 - 68s - loss: 0.2331 - val_loss: 0.2313\n",
            "\n",
            "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.00023730468819849193.\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.21283\n",
            "Epoch 98/300\n",
            "253/253 - 68s - loss: 0.1953 - val_loss: 0.2725\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.21283\n",
            "Epoch 99/300\n",
            "253/253 - 68s - loss: 0.2213 - val_loss: 0.2167\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.21283\n",
            "Epoch 100/300\n",
            "253/253 - 68s - loss: 0.1904 - val_loss: 0.2146\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.21283\n",
            "Epoch 101/300\n",
            "253/253 - 68s - loss: 0.2005 - val_loss: 0.2581\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.21283\n",
            "Epoch 102/300\n",
            "253/253 - 68s - loss: 0.1792 - val_loss: 0.1858\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.21283 to 0.18576, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 103/300\n",
            "253/253 - 68s - loss: 0.1716 - val_loss: 0.2047\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.18576\n",
            "Epoch 104/300\n",
            "253/253 - 68s - loss: 0.1765 - val_loss: 0.1949\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.18576\n",
            "Epoch 105/300\n",
            "253/253 - 68s - loss: 0.1652 - val_loss: 0.1998\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.18576\n",
            "Epoch 106/300\n",
            "253/253 - 68s - loss: 0.1659 - val_loss: 0.1950\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.18576\n",
            "Epoch 107/300\n",
            "253/253 - 68s - loss: 0.1978 - val_loss: 0.2708\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.18576\n",
            "Epoch 108/300\n",
            "253/253 - 68s - loss: 0.2407 - val_loss: 0.1947\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.18576\n",
            "Epoch 109/300\n",
            "253/253 - 68s - loss: 0.1849 - val_loss: 0.2711\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.18576\n",
            "Epoch 110/300\n",
            "253/253 - 68s - loss: 0.2467 - val_loss: 0.2691\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.18576\n",
            "Epoch 111/300\n",
            "253/253 - 68s - loss: 0.2161 - val_loss: 0.2644\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.18576\n",
            "Epoch 112/300\n",
            "253/253 - 68s - loss: 0.1828 - val_loss: 0.1867\n",
            "\n",
            "Epoch 00112: ReduceLROnPlateau reducing learning rate to 0.00017797851614886895.\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.18576\n",
            "Epoch 113/300\n",
            "253/253 - 68s - loss: 0.1577 - val_loss: 0.1765\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.18576 to 0.17653, saving model to ./model-gpu/transform_encoder_lstm_gpu_7C.h5\n",
            "Epoch 114/300\n",
            "253/253 - 68s - loss: 0.1515 - val_loss: 0.1845\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.17653\n",
            "Epoch 115/300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYJkCi1-9co3"
      },
      "source": [
        "BATCH_SIZE=512\n",
        "IS_WAVENET=False\n",
        "USE_RC_LOSS=False\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "    \n",
        "    VERBOSE = 2\n",
        "    EPOCHS = 300\n",
        "    FOLDS = 7\n",
        "    if DEBUG:\n",
        "      EPOCHS = 100\n",
        "    test_preds = []\n",
        "    kf = KFold(n_splits=FOLDS, shuffle=True, random_state=2021)\n",
        "    last_test_idx = []\n",
        "    \n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
        "        last_test_idx = test_idx\n",
        "        if len(test_preds) > fold:\n",
        "          print(f'Ignore corrent fold {fold+1} as we already predicted test data')\n",
        "\n",
        "        if DEBUG and fold > 0:\n",
        "          continue\n",
        "\n",
        "        X_train, X_valid = train[train_idx], train[test_idx]\n",
        "        y_train, y_valid = targets[train_idx], targets[test_idx]\n",
        "        #Align data to batch size and drop remainder. We only need it for TPU not GPU\n",
        "        if IS_TPU and IS_WAVENET:\n",
        "          fit_batch_size = X_train.shape[0] - (X_train.shape[0]%BATCH_SIZE)\n",
        "          print(X_train.shape[0], fit_batch_size)\n",
        "          X_train, y_train = X_train[:fit_batch_size, :, :], y_train[:fit_batch_size, :]\n",
        "          \n",
        "          fit_batch_size = X_valid.shape[0] - (X_valid.shape[0]%BATCH_SIZE)\n",
        "          print(X_valid.shape[0], fit_batch_size)\n",
        "          X_valid, y_valid = X_valid[:fit_batch_size, :, :], y_valid[:fit_batch_size, :]\n",
        "        \n",
        "        \n",
        "        # model = load_dnn_model_v4_add_wavenet(-3, True)\n",
        "        # model = lstm_wavenet_model()\n",
        "        # model = dnn_TransformerEncoder_model()\n",
        "        model = TransformerEncoder_LSTM_model(ff_dim=256,num_heads=8,num_layers=2,)\n",
        "        #show_weight_names(model)\n",
        "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-3),\n",
        "                      # loss=RC_loss\n",
        "                      loss=tf.keras.losses.MeanAbsoluteError()\n",
        "                      )\n",
        "        \n",
        "        \n",
        "\n",
        "        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.75, \n",
        "                               patience=10, verbose=VERBOSE)\n",
        "        \n",
        "        # scheduler = tf.keras.optimizers.schedules.ExponentialDecay(1e-3, 5, 0.9)\n",
        "        # lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "        checkpoint_name = f'./model-gpu/transform_encoder_lstm_gpu_{fold+1}C.h5'\n",
        "        chk_point = ModelCheckpoint(checkpoint_name,\n",
        "                                    monitor='val_loss', verbose=VERBOSE, \n",
        "                                    save_best_only=True, mode='min')\n",
        "\n",
        "        es = EarlyStopping(monitor=\"val_loss\", patience=50,\n",
        "                           verbose=VERBOSE, mode=\"min\",\n",
        "                           restore_best_weights=True)\n",
        "        \n",
        "        if fold not in skip_folds:\n",
        "          if USE_RC_LOSS:\n",
        "            model.fit(X_train, np.append(y_train, X_train[:, :, -4]+X_train[:, :, -5], axis =1), \n",
        "                      validation_data=(X_valid, np.append(y_valid, X_valid[:, :, -4]+X_valid[:, :, -5], axis =1)), \n",
        "                      epochs=EPOCHS,\n",
        "                      verbose=VERBOSE,\n",
        "                      batch_size=BATCH_SIZE, \n",
        "                      callbacks=[lr, chk_point, es])\n",
        "          else:\n",
        "            model.fit(X_train, y_train, \n",
        "                      validation_data=(X_valid, y_valid), \n",
        "                      epochs=EPOCHS,\n",
        "                      verbose=VERBOSE,\n",
        "                      batch_size=BATCH_SIZE, \n",
        "                      callbacks=[lr, chk_point, es])\n",
        "        else:\n",
        "          print('Load pretrain weight from ', checkpoint_name)\n",
        "          load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "          try:\n",
        "            # At loading time, register the custom objects with a `custom_object_scope`:\n",
        "            custom_objects = {\"ScaleLayer\": ScaleLayer, \"RC_loss\": RC_loss, \n",
        "                              \"Time2Vec\": Time2Vec, \n",
        "                              \"TransformerEncoder\": TransformerEncoder}\n",
        "            with tf.keras.utils.custom_object_scope(custom_objects):\n",
        "                model = load_model(checkpoint_name, options=load_locally)\n",
        "          except (OSError):\n",
        "            print('Cannot load pretrain weight from ', checkpoint_name)\n",
        "            break\n",
        "                \n",
        "        y_true = y_valid.squeeze().reshape(-1, 1)\n",
        "        y_pred = model.predict(X_valid, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1)\n",
        "        score = mean_absolute_error(y_true, y_pred)\n",
        "        print(f\"Fold-{fold+1} | OOF Score: {score}\")\n",
        "\n",
        "        #Save CV predict\n",
        "        if not (IS_TPU and IS_WAVENET):\n",
        "          train_pred[test_idx] = y_pred.reshape(-1, 80)\n",
        "          test_preds.append(model.predict(test, batch_size=1).squeeze().reshape(-1, 1).squeeze())\n",
        "\n",
        "        \n",
        "        del X_train, X_valid, y_train, y_valid, model\n",
        "        reset_keras()\n",
        "        gc.collect()\n",
        "\n",
        "        if not (IS_TPU and IS_WAVENET):\n",
        "          analy_predict(test_idx, train_pred)\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUanGPDq9co3"
      },
      "source": [
        "if not DEBUG:\n",
        "  train_pred = train_pred.flatten()\n",
        "  targets = targets.flatten()\n",
        "  score = mean_absolute_error(targets, train_pred)\n",
        "  print(f\"Train Score: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHx2oF849co4"
      },
      "source": [
        "## Create submission file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS9VjV7K9co4"
      },
      "source": [
        "if not DEBUG:\n",
        "  submission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n",
        "  submission[\"pressure\"] = sum(test_preds)/5\n",
        "  submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "  # ENSEMBLE FOLDS WITH MEDIAN\n",
        "  #取中位数\n",
        "  submission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\n",
        "  submission.to_csv('submission_median.csv', index=False)\n",
        "\n",
        "\n",
        "  # ENSEMBLE FOLDS WITH MEDIAN AND ROUND PREDICTIONS\n",
        "  submission[\"pressure\"] =\\\n",
        "      np.round( (submission.pressure - pressure_min)/pressure_step ) * pressure_step + pressure_min\n",
        "  submission.pressure = np.clip(submission.pressure, pressure_min, pressure_max)\n",
        "  submission.to_csv('submission_median_round.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NUXlVX70A_M"
      },
      "source": [
        "# Analyze predict data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7JStDQyQ5EJ"
      },
      "source": [
        "#analy_predict(last_test_idx, train_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TsMS3kv3xZM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}