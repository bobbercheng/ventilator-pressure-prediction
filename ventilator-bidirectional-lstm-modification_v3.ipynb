{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40a8566",
   "metadata": {
    "papermill": {
     "duration": 0.019649,
     "end_time": "2021-10-10T04:13:24.089000",
     "exception": false,
     "start_time": "2021-10-10T04:13:24.069351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Description\n",
    "Changes comparing V1:\n",
    "\n",
    "Adding new features:\n",
    "\n",
    "df['u_in_first'] = df.groupby(['breath_id'])['u_in'].transform('first')\n",
    "df['u_in_last'] = df.groupby(['breath_id'])['u_in'].transform('last')\n",
    "\n",
    "df.loc[train['time_step'] == 0, 'u_in_diff'] = 0\n",
    "df.loc[train['time_step'] == 0, 'u_out_diff'] = 0\n",
    "\n",
    "Using Rescaling laying for discrete output\n",
    "\n",
    "Ensemble folds with median\n",
    "Ensembble folds with median and round predictions\n",
    "\n",
    "Results:\n",
    "\n",
    "Median: 0.150\n",
    "Median and round predictions: 0.159\n",
    "\n",
    "V2:\n",
    "Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d90f884a",
   "metadata": {
    "_cell_guid": "3e5a0bd1-3e22-4b2c-a565-68985e55f95e",
    "_uuid": "e331dbcc-0346-4019-9ff6-b890154a878b",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-10T04:13:24.139689Z",
     "iopub.status.busy": "2021-10-10T04:13:24.138946Z",
     "iopub.status.idle": "2021-10-10T04:13:29.984217Z",
     "shell.execute_reply": "2021-10-10T04:13:29.983419Z",
     "shell.execute_reply.started": "2021-10-10T04:00:22.407719Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.87641,
     "end_time": "2021-10-10T04:13:29.984386",
     "exception": false,
     "start_time": "2021-10-10T04:13:24.107976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.preprocessing import RobustScaler, normalize\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, KFold\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ca48c38",
   "metadata": {
    "_cell_guid": "89eb257e-0ed2-4f8b-94d6-462a5e995eb1",
    "_uuid": "ca71d87d-6594-4f31-906d-0b53cd4c1374",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-10T04:13:30.026578Z",
     "iopub.status.busy": "2021-10-10T04:13:30.025963Z",
     "iopub.status.idle": "2021-10-10T04:13:45.150032Z",
     "shell.execute_reply": "2021-10-10T04:13:45.150540Z",
     "shell.execute_reply.started": "2021-10-10T04:00:29.903466Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 15.148133,
     "end_time": "2021-10-10T04:13:45.150742",
     "exception": false,
     "start_time": "2021-10-10T04:13:30.002609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "\n",
    "train = pd.read_csv('../input/ventilator-pressure-prediction/train.csv')\n",
    "test = pd.read_csv('../input/ventilator-pressure-prediction/test.csv')\n",
    "submission = pd.read_csv('../input/ventilator-pressure-prediction/sample_submission.csv')\n",
    "\n",
    "if DEBUG:\n",
    "    train = train[:80*1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b364401a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T04:13:45.199737Z",
     "iopub.status.busy": "2021-10-10T04:13:45.190291Z",
     "iopub.status.idle": "2021-10-10T04:13:45.269927Z",
     "shell.execute_reply": "2021-10-10T04:13:45.269394Z",
     "shell.execute_reply.started": "2021-10-10T04:00:46.174457Z"
    },
    "papermill": {
     "duration": 0.101016,
     "end_time": "2021-10-10T04:13:45.270072",
     "exception": false,
     "start_time": "2021-10-10T04:13:45.169056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_pressure = np.sort(train['pressure'].unique())\n",
    "pressure_min =  all_pressure[0].item()\n",
    "pressure_max = all_pressure[-1].item()\n",
    "pressure_step = (all_pressure[1] - all_pressure[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73fce48d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T04:13:45.315844Z",
     "iopub.status.busy": "2021-10-10T04:13:45.314815Z",
     "iopub.status.idle": "2021-10-10T04:13:45.321549Z",
     "shell.execute_reply": "2021-10-10T04:13:45.322015Z",
     "shell.execute_reply.started": "2021-10-10T04:00:46.266258Z"
    },
    "papermill": {
     "duration": 0.033264,
     "end_time": "2021-10-10T04:13:45.322202",
     "exception": false,
     "start_time": "2021-10-10T04:13:45.288938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.895744294564641"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "64.8209917386395"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.07030214545121005"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pressure_min)\n",
    "display(pressure_max)\n",
    "display(pressure_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed51998",
   "metadata": {
    "papermill": {
     "duration": 0.019428,
     "end_time": "2021-10-10T04:13:45.361568",
     "exception": false,
     "start_time": "2021-10-10T04:13:45.342140",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Engineer Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06a97f7",
   "metadata": {
    "papermill": {
     "duration": 0.019927,
     "end_time": "2021-10-10T04:13:45.401212",
     "exception": false,
     "start_time": "2021-10-10T04:13:45.381285",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## From [Ventilator: Feature engineering](https://www.kaggle.com/mistag/ventilator-feature-engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e88ba53",
   "metadata": {
    "_cell_guid": "13a36b46-7067-4b29-aad3-7e3b15e8415b",
    "_uuid": "dc41dbf2-f199-4b9d-bbd9-bf6084162b47",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-10T04:13:45.463526Z",
     "iopub.status.busy": "2021-10-10T04:13:45.458349Z",
     "iopub.status.idle": "2021-10-10T04:14:42.122918Z",
     "shell.execute_reply": "2021-10-10T04:14:42.123633Z",
     "shell.execute_reply.started": "2021-10-10T04:00:46.283342Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 56.703334,
     "end_time": "2021-10-10T04:14:42.123831",
     "exception": false,
     "start_time": "2021-10-10T04:13:45.420497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Feature engineering\n",
    "def add_features(df):\n",
    "    #time_step*u_in\n",
    "    df['area'] = df['time_step'] * df['u_in']\n",
    "    df['area'] = df.groupby('breath_id')['area'].cumsum()\n",
    "    \n",
    "    #sum of u_in\n",
    "    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
    "    \n",
    "    #shift +1 -1 +3 -3\n",
    "    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n",
    "    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n",
    "    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n",
    "    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n",
    "    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n",
    "    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n",
    "    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n",
    "    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n",
    "    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n",
    "    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n",
    "    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n",
    "    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n",
    "    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n",
    "    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n",
    "    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n",
    "    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n",
    "    df = df.fillna(0)\n",
    "         \n",
    "    df['u_in_first'] = df.groupby(['breath_id'])['u_in'].transform('first')\n",
    "    df['u_in_last'] = df.groupby(['breath_id'])['u_in'].transform('last')\n",
    "    \n",
    "    # max value of u_in and u_out for each breath\n",
    "    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n",
    "    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n",
    "   \n",
    "    # difference between consequitive values\n",
    "    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n",
    "    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n",
    "    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n",
    "    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n",
    "    \n",
    "    df.loc[df['time_step'] == 0, 'u_in_diff'] = 0\n",
    "    df.loc[df['time_step'] == 0, 'u_out_diff'] = 0\n",
    "    \n",
    "    # difference between the current value of u_in and the max value within the breath\n",
    "    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n",
    "    # difference between the current value of u_in and the mean value within the breath\n",
    "    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n",
    "    \n",
    "    # difference between consequitive values\n",
    "    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n",
    "    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n",
    "    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n",
    "    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n",
    "    \n",
    "    #u_in*u_out\n",
    "    df['cross']= df['u_in']*df['u_out']\n",
    "    \n",
    "    #time_step*u_out\n",
    "    df['cross2']= df['time_step']*df['u_out']\n",
    "    \n",
    "    df['R'] = df['R'].astype(str)\n",
    "    df['C'] = df['C'].astype(str)\n",
    "    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n",
    "    #one hot encoding\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    df = df.fillna(0)\n",
    "    return df\n",
    "\n",
    "train = add_features(train)\n",
    "test = add_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2d9ccb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T04:14:42.168738Z",
     "iopub.status.busy": "2021-10-10T04:14:42.166635Z",
     "iopub.status.idle": "2021-10-10T04:14:42.171108Z",
     "shell.execute_reply": "2021-10-10T04:14:42.171607Z",
     "shell.execute_reply.started": "2021-10-10T04:02:02.232207Z"
    },
    "papermill": {
     "duration": 0.027522,
     "end_time": "2021-10-10T04:14:42.171769",
     "exception": false,
     "start_time": "2021-10-10T04:14:42.144247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4024000, 56)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c52e465",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T04:14:42.214280Z",
     "iopub.status.busy": "2021-10-10T04:14:42.213652Z",
     "iopub.status.idle": "2021-10-10T04:14:43.255409Z",
     "shell.execute_reply": "2021-10-10T04:14:43.256340Z",
     "shell.execute_reply.started": "2021-10-10T04:02:02.244864Z"
    },
    "papermill": {
     "duration": 1.065005,
     "end_time": "2021-10-10T04:14:43.256573",
     "exception": false,
     "start_time": "2021-10-10T04:14:42.191568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "check_train_nan = train.isnull().sum().sum()\n",
    "check_test_nan = test.isnull().sum().sum()\n",
    "\n",
    "print(check_train_nan)\n",
    "print(check_test_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e9a13b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T04:14:43.300150Z",
     "iopub.status.busy": "2021-10-10T04:14:43.299234Z",
     "iopub.status.idle": "2021-10-10T04:14:43.323385Z",
     "shell.execute_reply": "2021-10-10T04:14:43.322839Z",
     "shell.execute_reply.started": "2021-10-10T04:02:03.339600Z"
    },
    "papermill": {
     "duration": 0.046596,
     "end_time": "2021-10-10T04:14:43.323523",
     "exception": false,
     "start_time": "2021-10-10T04:14:43.276927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "      <th>pressure</th>\n",
       "      <th>area</th>\n",
       "      <th>u_in_cumsum</th>\n",
       "      <th>u_in_lag1</th>\n",
       "      <th>u_out_lag1</th>\n",
       "      <th>...</th>\n",
       "      <th>C_50</th>\n",
       "      <th>R__C_20__10</th>\n",
       "      <th>R__C_20__20</th>\n",
       "      <th>R__C_20__50</th>\n",
       "      <th>R__C_50__10</th>\n",
       "      <th>R__C_50__20</th>\n",
       "      <th>R__C_50__50</th>\n",
       "      <th>R__C_5__10</th>\n",
       "      <th>R__C_5__20</th>\n",
       "      <th>R__C_5__50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0</td>\n",
       "      <td>5.837492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.033652</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>0</td>\n",
       "      <td>5.907794</td>\n",
       "      <td>0.618632</td>\n",
       "      <td>18.466375</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.067514</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>0</td>\n",
       "      <td>7.876254</td>\n",
       "      <td>2.138333</td>\n",
       "      <td>40.975653</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  breath_id  time_step       u_in  u_out  pressure      area  \\\n",
       "0   1          1   0.000000   0.083334      0  5.837492  0.000000   \n",
       "1   2          1   0.033652  18.383041      0  5.907794  0.618632   \n",
       "2   3          1   0.067514  22.509278      0  7.876254  2.138333   \n",
       "\n",
       "   u_in_cumsum  u_in_lag1  u_out_lag1  ...  C_50  R__C_20__10  R__C_20__20  \\\n",
       "0     0.083334   0.000000         0.0  ...     1            0            0   \n",
       "1    18.466375   0.083334         0.0  ...     1            0            0   \n",
       "2    40.975653  18.383041         0.0  ...     1            0            0   \n",
       "\n",
       "   R__C_20__50  R__C_50__10  R__C_50__20  R__C_50__50  R__C_5__10  R__C_5__20  \\\n",
       "0            1            0            0            0           0           0   \n",
       "1            1            0            0            0           0           0   \n",
       "2            1            0            0            0           0           0   \n",
       "\n",
       "   R__C_5__50  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "\n",
       "[3 rows x 57 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc7ec25",
   "metadata": {
    "_cell_guid": "01328860-fa2a-421c-9e5f-ea0048246f98",
    "_uuid": "346bf2c0-96d2-4da5-8837-c0f820294a85",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2021-10-10T04:14:43.389634Z",
     "iopub.status.busy": "2021-10-10T04:14:43.387478Z",
     "iopub.status.idle": "2021-10-10T04:14:44.583206Z",
     "shell.execute_reply": "2021-10-10T04:14:44.583738Z",
     "shell.execute_reply.started": "2021-10-10T04:02:03.373203Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.240015,
     "end_time": "2021-10-10T04:14:44.583908",
     "exception": false,
     "start_time": "2021-10-10T04:14:43.343893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets = train[['pressure']].to_numpy().reshape(-1, 80)\n",
    "train.drop(['pressure', 'id', 'breath_id'], axis=1, inplace=True)\n",
    "test = test.drop(['id', 'breath_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c92503c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T04:14:44.632295Z",
     "iopub.status.busy": "2021-10-10T04:14:44.631657Z",
     "iopub.status.idle": "2021-10-10T04:14:56.280291Z",
     "shell.execute_reply": "2021-10-10T04:14:56.279720Z",
     "shell.execute_reply.started": "2021-10-10T04:02:04.705116Z"
    },
    "papermill": {
     "duration": 11.675942,
     "end_time": "2021-10-10T04:14:56.280430",
     "exception": false,
     "start_time": "2021-10-10T04:14:44.604488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Normalise the dataset\n",
    "RS = RobustScaler()\n",
    "train = RS.fit_transform(train)\n",
    "test = RS.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dd29cdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T04:14:56.327485Z",
     "iopub.status.busy": "2021-10-10T04:14:56.326851Z",
     "iopub.status.idle": "2021-10-10T04:14:56.329957Z",
     "shell.execute_reply": "2021-10-10T04:14:56.329452Z",
     "shell.execute_reply.started": "2021-10-10T04:02:18.899336Z"
    },
    "papermill": {
     "duration": 0.028825,
     "end_time": "2021-10-10T04:14:56.330129",
     "exception": false,
     "start_time": "2021-10-10T04:14:56.301304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Reshape group to 80 timesteps for each breath ID\n",
    "train = train.reshape(-1, 80, train.shape[-1])\n",
    "test = test.reshape(-1, 80, train.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74163d1",
   "metadata": {
    "papermill": {
     "duration": 0.020355,
     "end_time": "2021-10-10T04:14:56.371346",
     "exception": false,
     "start_time": "2021-10-10T04:14:56.350991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f5d07",
   "metadata": {
    "papermill": {
     "duration": 0.020156,
     "end_time": "2021-10-10T04:14:56.412332",
     "exception": false,
     "start_time": "2021-10-10T04:14:56.392176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## From [Rescaling layer for discrete output in TensorFlow](https://www.kaggle.com/lucamassaron/rescaling-layer-for-discrete-output-in-tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06edce21",
   "metadata": {
    "papermill": {
     "duration": 0.020098,
     "end_time": "2021-10-10T04:14:56.452830",
     "exception": false,
     "start_time": "2021-10-10T04:14:56.432732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Please notice the custom rounding round_with_gradients function since tf.round has no gradients and it won't be differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abaebf69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T04:14:56.502913Z",
     "iopub.status.busy": "2021-10-10T04:14:56.502105Z",
     "iopub.status.idle": "2021-10-10T04:14:56.505161Z",
     "shell.execute_reply": "2021-10-10T04:14:56.504550Z",
     "shell.execute_reply.started": "2021-10-10T04:02:18.907997Z"
    },
    "papermill": {
     "duration": 0.032115,
     "end_time": "2021-10-10T04:14:56.505289",
     "exception": false,
     "start_time": "2021-10-10T04:14:56.473174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def round_with_gradients(x):\n",
    "    def grad(dy):\n",
    "        return dy\n",
    "    return tf.round(x), grad\n",
    "\n",
    "class ScaleLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ScaleLayer, self).__init__()\n",
    "        self.min = tf.constant(pressure_min, dtype=np.float32)\n",
    "        self.max = tf.constant(pressure_max, dtype=np.float32)\n",
    "        self.step = tf.constant(pressure_step, dtype=np.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        steps = tf.math.divide(tf.math.add(inputs, -self.min), self.step)\n",
    "        int_steps = round_with_gradients(steps)\n",
    "        rescaled_steps = tf.math.add(tf.math.multiply(int_steps, self.step), self.min)\n",
    "        clipped = tf.clip_by_value(rescaled_steps, self.min, self.max)\n",
    "        return clipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00627277",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T04:14:56.556169Z",
     "iopub.status.busy": "2021-10-10T04:14:56.555257Z",
     "iopub.status.idle": "2021-10-10T04:14:56.557620Z",
     "shell.execute_reply": "2021-10-10T04:14:56.558274Z",
     "shell.execute_reply.started": "2021-10-10T04:02:18.925612Z"
    },
    "papermill": {
     "duration": 0.032905,
     "end_time": "2021-10-10T04:14:56.558514",
     "exception": false,
     "start_time": "2021-10-10T04:14:56.525609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class Attention(Layer):\n",
    "    \n",
    "    def __init__(self, return_sequences=True):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(Attention,self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
    "                               initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
    "                               initializer=\"zeros\")\n",
    "        \n",
    "        super(Attention,self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "        \n",
    "        return K.sum(output, axis=1)\n",
    "\n",
    "    def get_config(self):\n",
    "        return super(Attention, self).get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e239ffe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T04:14:56.612505Z",
     "iopub.status.busy": "2021-10-10T04:14:56.611838Z",
     "iopub.status.idle": "2021-10-10T04:14:56.613957Z",
     "shell.execute_reply": "2021-10-10T04:14:56.614450Z",
     "shell.execute_reply.started": "2021-10-10T04:02:18.950506Z"
    },
    "papermill": {
     "duration": 0.03506,
     "end_time": "2021-10-10T04:14:56.614625",
     "exception": false,
     "start_time": "2021-10-10T04:14:56.579565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def BiLSTM_model():\n",
    "    inputs = keras.layers.Input(shape=train.shape[-2:])\n",
    "    x = inputs\n",
    "    #bidirectional LSTM 1024->512->256->128\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(1024, return_sequences=True))(x)\n",
    "    attend_x = Attention(return_sequences=True)(x)\n",
    "    x = keras.layers.Concatenate()([x, attend_x])\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(512, return_sequences=True))(x)\n",
    "    attend_x = Attention(return_sequences=True)(x)\n",
    "    x = keras.layers.Concatenate()([x, attend_x])\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(256, return_sequences=True))(x)\n",
    "    attend_x = Attention(return_sequences=True)(x)\n",
    "    x = keras.layers.Concatenate()([x, attend_x])\n",
    "    x = keras.layers.Bidirectional(keras.layers.LSTM(128, return_sequences=True))(x)\n",
    "    attend_x = Attention(return_sequences=True)(x)\n",
    "    x = keras.layers.Concatenate()([x, attend_x])\n",
    "    x = keras.layers.Dense(128, activation='selu')(x)\n",
    "    # keras.layers.Dropout(0.1)\n",
    "    outputs = keras.layers.Dense(1)(x)\n",
    "    outputs = ScaleLayer()(outputs)\n",
    "\n",
    "    model  = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.summary()\n",
    "    model.compile(optimizer=\"adam\", loss=\"mae\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa0bbc4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T04:14:56.660042Z",
     "iopub.status.busy": "2021-10-10T04:14:56.659383Z",
     "iopub.status.idle": "2021-10-10T12:35:32.493794Z",
     "shell.execute_reply": "2021-10-10T12:35:32.494743Z"
    },
    "papermill": {
     "duration": 30035.861681,
     "end_time": "2021-10-10T12:35:32.497288",
     "exception": false,
     "start_time": "2021-10-10T04:14:56.635607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- > Fold 1 < ---------------\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 80, 54)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 80, 2048)     8839168     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention (Attention)           (None, 80, 2048)     2128        bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 80, 4096)     0           bidirectional[0][0]              \n",
      "                                                                 attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 80, 1024)     18878464    concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 80, 1024)     1104        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 80, 2048)     0           bidirectional_1[0][0]            \n",
      "                                                                 attention_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 80, 512)      4720640     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 80, 512)      592         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 80, 1024)     0           bidirectional_2[0][0]            \n",
      "                                                                 attention_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 80, 256)      1180672     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_3 (Attention)         (None, 80, 256)      336         bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 80, 512)      0           bidirectional_3[0][0]            \n",
      "                                                                 attention_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 80, 128)      65664       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 80, 1)        129         dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "scale_layer (ScaleLayer)        (None, 80, 1)        0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 33,688,897\n",
      "Trainable params: 33,688,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(0.001, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 59s 486ms/step - loss: 4.2139 - val_loss: 1.2110\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21096, saving model to folds0.hdf5\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to tf.Tensor(0.000995129, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 1.0412 - val_loss: 0.7058\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.21096 to 0.70576, saving model to folds0.hdf5\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009902818, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.7227 - val_loss: 0.6750\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.70576 to 0.67503, saving model to folds0.hdf5\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009854581, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5832 - val_loss: 0.5127\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.67503 to 0.51272, saving model to folds0.hdf5\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.000980658, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.5050 - val_loss: 0.4662\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.51272 to 0.46624, saving model to folds0.hdf5\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097588124, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.4832 - val_loss: 0.4299\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.46624 to 0.42988, saving model to folds0.hdf5\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009711277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.4547 - val_loss: 0.4191\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.42988 to 0.41911, saving model to folds0.hdf5\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009663974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4301 - val_loss: 0.4543\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.41911\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009616901, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.4160 - val_loss: 0.3941\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.41911 to 0.39406, saving model to folds0.hdf5\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009570057, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.4002 - val_loss: 0.4333\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.39406\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095234415, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.4016 - val_loss: 0.3686\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.39406 to 0.36863, saving model to folds0.hdf5\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009477053, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.3624 - val_loss: 0.3610\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.36863 to 0.36095, saving model to folds0.hdf5\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009430891, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.3562 - val_loss: 0.3504\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.36095 to 0.35041, saving model to folds0.hdf5\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009384953, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.3558 - val_loss: 0.3749\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.35041\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009339239, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.3412 - val_loss: 0.3303\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.35041 to 0.33034, saving model to folds0.hdf5\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009293747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.3322 - val_loss: 0.3316\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.33034\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009248478, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.3215 - val_loss: 0.3354\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.33034\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(0.00092034286, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.3227 - val_loss: 0.3155\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.33034 to 0.31551, saving model to folds0.hdf5\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091585994, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.3295 - val_loss: 0.3366\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.31551\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091139873, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.3083 - val_loss: 0.2928\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.31551 to 0.29282, saving model to folds0.hdf5\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090695935, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2880 - val_loss: 0.3257\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.29282\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090254156, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2954 - val_loss: 0.3166\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.29282\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008981453, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2879 - val_loss: 0.2945\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.29282\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008937704, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2802 - val_loss: 0.2751\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.29282 to 0.27512, saving model to folds0.hdf5\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008894169, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2647 - val_loss: 0.2920\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.27512\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088508456, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2805 - val_loss: 0.2831\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.27512\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088077335, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2651 - val_loss: 0.2916\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.27512\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008764831, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2742 - val_loss: 0.2701\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.27512 to 0.27006, saving model to folds0.hdf5\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to tf.Tensor(0.00087221374, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2614 - val_loss: 0.2617\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.27006 to 0.26169, saving model to folds0.hdf5\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008679653, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2535 - val_loss: 0.2587\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.26169 to 0.25866, saving model to folds0.hdf5\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to tf.Tensor(0.00086373737, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2396 - val_loss: 0.2904\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.25866\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008595301, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2492 - val_loss: 0.2627\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.25866\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to tf.Tensor(0.00085534336, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2414 - val_loss: 0.2561\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.25866 to 0.25611, saving model to folds0.hdf5\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to tf.Tensor(0.000851177, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2347 - val_loss: 0.2440\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.25611 to 0.24398, saving model to folds0.hdf5\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to tf.Tensor(0.00084703095, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2306 - val_loss: 0.2499\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.24398\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008429051, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2334 - val_loss: 0.2423\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.24398 to 0.24229, saving model to folds0.hdf5\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to tf.Tensor(0.00083879934, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2337 - val_loss: 0.2418\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.24229 to 0.24182, saving model to folds0.hdf5\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008347136, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2258 - val_loss: 0.2360\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.24182 to 0.23604, saving model to folds0.hdf5\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008306477, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2280 - val_loss: 0.2493\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.23604\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008266016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2212 - val_loss: 0.2356\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.23604 to 0.23561, saving model to folds0.hdf5\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to tf.Tensor(0.00082257524, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2180 - val_loss: 0.2332\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.23561 to 0.23323, saving model to folds0.hdf5\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008185685, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2188 - val_loss: 0.2495\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.23323\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081458123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2206 - val_loss: 0.2388\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.23323\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081061345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2220 - val_loss: 0.2349\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.23323\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to tf.Tensor(0.000806665, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2445 - val_loss: 0.2955\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.23323\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to tf.Tensor(0.00080273574, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 14s 207ms/step - loss: 0.2334 - val_loss: 0.2352\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.23323\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007988256, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2154 - val_loss: 0.2322\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.23323 to 0.23217, saving model to folds0.hdf5\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007949346, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2093 - val_loss: 0.2477\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.23217\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007910624, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2227 - val_loss: 0.2220\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.23217 to 0.22201, saving model to folds0.hdf5\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007872092, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2080 - val_loss: 0.2254\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.22201\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007833747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2000 - val_loss: 0.2239\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.22201\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077955885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2017 - val_loss: 0.2196\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.22201 to 0.21956, saving model to folds0.hdf5\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077576167, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2060 - val_loss: 0.2280\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.21956\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077198294, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2029 - val_loss: 0.2182\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.21956 to 0.21817, saving model to folds0.hdf5\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007682226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1980 - val_loss: 0.2208\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.21817\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007644806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1933 - val_loss: 0.2085\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.21817 to 0.20848, saving model to folds0.hdf5\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007607569, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1888 - val_loss: 0.2081\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.20848 to 0.20807, saving model to folds0.hdf5\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007570512, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1848 - val_loss: 0.2262\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.20807\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to tf.Tensor(0.00075336365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2024 - val_loss: 0.2125\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.20807\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to tf.Tensor(0.000749694, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1881 - val_loss: 0.2090\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.20807\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007460423, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1885 - val_loss: 0.2087\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.20807\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to tf.Tensor(0.00074240833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1798 - val_loss: 0.2129\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.20807\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007387921, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1841 - val_loss: 0.2188\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.20807\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to tf.Tensor(0.00073519343, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1851 - val_loss: 0.2119\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.20807\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007316123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1837 - val_loss: 0.2062\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.20807 to 0.20623, saving model to folds0.hdf5\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007280487, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1931 - val_loss: 0.2211\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.20623\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007245024, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1947 - val_loss: 0.2090\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.20623\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007209733, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1788 - val_loss: 0.1971\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.20623 to 0.19707, saving model to folds0.hdf5\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071746146, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1723 - val_loss: 0.2193\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.19707\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007139667, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1725 - val_loss: 0.2071\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.19707\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071048894, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1741 - val_loss: 0.2079\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.19707\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007070282, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1764 - val_loss: 0.2209\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.19707\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007035842, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1784 - val_loss: 0.2030\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.19707\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to tf.Tensor(0.00070015714, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1760 - val_loss: 0.2046\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.19707\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006967467, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1700 - val_loss: 0.1976\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.19707\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to tf.Tensor(0.00069335283, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1699 - val_loss: 0.2030\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.19707\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006899755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1691 - val_loss: 0.1992\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.19707\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068661466, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1654 - val_loss: 0.2090\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.19707\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068327016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1723 - val_loss: 0.1936\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.19707 to 0.19365, saving model to folds0.hdf5\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067994196, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1585 - val_loss: 0.1938\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.19365\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067663, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1618 - val_loss: 0.1954\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.19365\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006733342, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1638 - val_loss: 0.1968\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.19365\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067005435, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1635 - val_loss: 0.1927\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.19365 to 0.19267, saving model to folds0.hdf5\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006667905, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1588 - val_loss: 0.1993\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.19267\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006635426, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1586 - val_loss: 0.1955\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.19267\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006603105, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1581 - val_loss: 0.1939\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.19267\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006570942, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1581 - val_loss: 0.2064\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.19267\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065389345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1644 - val_loss: 0.1940\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.19267\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065070833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1689 - val_loss: 0.1999\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.19267\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to tf.Tensor(0.00064753875, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1656 - val_loss: 0.1986\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.19267\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006443846, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1602 - val_loss: 0.2027\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.19267\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006412458, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1556 - val_loss: 0.1898\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.19267 to 0.18978, saving model to folds0.hdf5\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063812226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1527 - val_loss: 0.1875\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.18978 to 0.18747, saving model to folds0.hdf5\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063501403, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1538 - val_loss: 0.1944\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.18747\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006319209, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1524 - val_loss: 0.1864\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.18747 to 0.18644, saving model to folds0.hdf5\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006288428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1456 - val_loss: 0.1882\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.18644\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to tf.Tensor(0.00062577974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1461 - val_loss: 0.1885\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.18644\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006227316, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1429 - val_loss: 0.1867\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.18644\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061969826, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1439 - val_loss: 0.1914\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.18644\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006166797, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1477 - val_loss: 0.1856\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.18644 to 0.18556, saving model to folds0.hdf5\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061367586, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1402 - val_loss: 0.1921\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.18556\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061068666, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1393 - val_loss: 0.1867\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.18556\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006077121, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1529 - val_loss: 0.1950\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.18556\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060475187, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1459 - val_loss: 0.1921\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.18556\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060180615, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1435 - val_loss: 0.1941\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.18556\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059887476, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 14s 203ms/step - loss: 0.1442 - val_loss: 0.1839\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.18556 to 0.18388, saving model to folds0.hdf5\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005959577, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1404 - val_loss: 0.1855\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.18388\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059305475, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1397 - val_loss: 0.1858\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.18388\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to tf.Tensor(0.000590166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1363 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.18388 to 0.18235, saving model to folds0.hdf5\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to tf.Tensor(0.00058729126, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1391 - val_loss: 0.1866\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.18235\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005844306, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1396 - val_loss: 0.1812\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.18235 to 0.18121, saving model to folds0.hdf5\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005815838, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1343 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.18121\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to tf.Tensor(0.000578751, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1364 - val_loss: 0.1904\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.18121\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057593186, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1308 - val_loss: 0.1872\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.18121\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057312654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1293 - val_loss: 0.1848\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.18121\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057033484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1350 - val_loss: 0.1816\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.18121\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to tf.Tensor(0.00056755677, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1264 - val_loss: 0.1836\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.18121\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005647922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1277 - val_loss: 0.1817\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.18121\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005620411, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1247 - val_loss: 0.1865\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.18121\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055930344, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1293 - val_loss: 0.1945\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.18121\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055657903, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1357 - val_loss: 0.1935\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.18121\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055386795, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1303 - val_loss: 0.1837\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.18121\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005511701, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1281 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.18121\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005484854, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1241 - val_loss: 0.1835\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.18121\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005458137, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1257 - val_loss: 0.1806\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.18121 to 0.18061, saving model to folds0.hdf5\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to tf.Tensor(0.00054315507, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1216 - val_loss: 0.1880\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.18061\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005405093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1232 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.18061\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005378765, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1213 - val_loss: 0.1820\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.18061\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005352565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1207 - val_loss: 0.1799\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.18061 to 0.17991, saving model to folds0.hdf5\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005326493, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1185 - val_loss: 0.1854\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.17991\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005300548, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1184 - val_loss: 0.1863\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.17991\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005274729, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1182 - val_loss: 0.1804\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.17991\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052490365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1187 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.17991 to 0.17967, saving model to folds0.hdf5\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052234676, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1194 - val_loss: 0.1809\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.17967\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005198025, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1144 - val_loss: 0.1830\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.17967\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005172705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1244 - val_loss: 0.1879\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.17967\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051475083, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1163 - val_loss: 0.1821\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.17967\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051224354, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1183 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.17967\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005097484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1125 - val_loss: 0.1806\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.17967\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005072654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1126 - val_loss: 0.1866\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.17967\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005047946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1136 - val_loss: 0.1878\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.17967\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005023357, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1116 - val_loss: 0.1837\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.17967\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049988885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1129 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.17967\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004974539, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1103 - val_loss: 0.1844\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.17967\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004950308, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1104 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.17967 to 0.17878, saving model to folds0.hdf5\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004926195, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1097 - val_loss: 0.1810\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.17878\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049022, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1115 - val_loss: 0.1830\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.17878\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048783212, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1071 - val_loss: 0.1810\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.17878\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048545588, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1068 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00149: val_loss improved from 0.17878 to 0.17829, saving model to folds0.hdf5\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048309125, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1076 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.17829\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048073815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1071 - val_loss: 0.1784\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.17829\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047839645, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1048 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.17829\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004760662, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1015 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.17829\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004737473, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1045 - val_loss: 0.1818\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.17829\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047143968, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1038 - val_loss: 0.1815\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.17829\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004691433, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 14s 203ms/step - loss: 0.1009 - val_loss: 0.1807\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.17829\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046685815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0993 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.17829\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046458407, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0987 - val_loss: 0.1806\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.17829\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046232107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0978 - val_loss: 0.1814\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.17829\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046006907, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0986 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.17829\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045782814, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0982 - val_loss: 0.1790\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.17829\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045559806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0975 - val_loss: 0.1809\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.17829\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045337883, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0968 - val_loss: 0.1839\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.17829\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045117046, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0999 - val_loss: 0.1836\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.17829\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044897277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0975 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.17829\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044678585, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0965 - val_loss: 0.1776\n",
      "\n",
      "Epoch 00166: val_loss improved from 0.17829 to 0.17759, saving model to folds0.hdf5\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004446096, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0942 - val_loss: 0.1798\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.17759\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004424439, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0989 - val_loss: 0.1859\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.17759\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044028874, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1022 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.17759\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043814414, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0984 - val_loss: 0.1859\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.17759\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043600993, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0966 - val_loss: 0.1806\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.17759\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043388613, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0936 - val_loss: 0.1796\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.17759\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004317727, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0914 - val_loss: 0.1800\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.17759\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004296695, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0947 - val_loss: 0.1818\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.17759\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004275766, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0932 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.17759\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042549393, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0925 - val_loss: 0.1821\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.17759\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042342133, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0892 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.17759\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042135885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0880 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.17759\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041930642, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0896 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.17759\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041726397, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0889 - val_loss: 0.1795\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.17759\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041523148, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0869 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.17759\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041320888, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0903 - val_loss: 0.1894\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.17759\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041119618, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0916 - val_loss: 0.1905\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.17759\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040919325, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0917 - val_loss: 0.1836\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.17759\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040720004, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0886 - val_loss: 0.1893\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.17759\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004052166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0873 - val_loss: 0.1785\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.17759\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004032428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0840 - val_loss: 0.1790\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.17759\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004012786, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0841 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.17759\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039932402, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0833 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.17759\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039737887, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0809 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.17759\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039544326, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0839 - val_loss: 0.1795\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.17759\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039351705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 14s 205ms/step - loss: 0.0807 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.17759\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039160028, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0818 - val_loss: 0.1840\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.17759\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038969278, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0831 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.17759\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003877946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0834 - val_loss: 0.1808\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.17759\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038590565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0803 - val_loss: 0.1812\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.17759\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003840259, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.0796 - val_loss: 0.1805\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.17759\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038215535, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0788 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.17759\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038029387, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0814 - val_loss: 0.1808\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.17759\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037844144, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0789 - val_loss: 0.1806\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.17759\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037659804, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.0786 - val_loss: 0.1805\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.17759\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037476368, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.0791 - val_loss: 0.1804\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.17759\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037293817, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0769 - val_loss: 0.1801\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.17759\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003711216, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0790 - val_loss: 0.1805\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.17759\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003693139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0785 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.17759\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to tf.Tensor(0.000367515, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0766 - val_loss: 0.1810\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.17759\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036572482, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.0755 - val_loss: 0.1869\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.17759\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003639434, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0784 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.17759\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036217063, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0731 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.17759\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036040647, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0740 - val_loss: 0.1848\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.17759\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to tf.Tensor(0.00035865093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0742 - val_loss: 0.1838\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.17759\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to tf.Tensor(0.000356904, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0731 - val_loss: 0.1856\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.17759\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003551655, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0739 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.17759\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003534355, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0719 - val_loss: 0.1789\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.17759\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003517139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0734 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.17759\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003500007, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0735 - val_loss: 0.1798\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.17759\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034829587, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0718 - val_loss: 0.1809\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.17759\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034659932, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0715 - val_loss: 0.1803\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.17759\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034491107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0699 - val_loss: 0.1799\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.17759\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034323102, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0694 - val_loss: 0.1807\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.17759\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034155912, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0707 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.17759\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033989537, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0704 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.17759\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033823974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0716 - val_loss: 0.1815\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.17759\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003365922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0699 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.17759\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033495267, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0681 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.17759\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003333211, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0677 - val_loss: 0.1806\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.17759\n",
      "Epoch 00226: early stopping\n",
      "--------------- > Fold 2 < ---------------\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 80, 54)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 80, 2048)     8839168     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_4 (Attention)         (None, 80, 2048)     2128        bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 80, 4096)     0           bidirectional_4[0][0]            \n",
      "                                                                 attention_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 80, 1024)     18878464    concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_5 (Attention)         (None, 80, 1024)     1104        bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 80, 2048)     0           bidirectional_5[0][0]            \n",
      "                                                                 attention_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 80, 512)      4720640     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_6 (Attention)         (None, 80, 512)      592         bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 80, 1024)     0           bidirectional_6[0][0]            \n",
      "                                                                 attention_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_7 (Bidirectional) (None, 80, 256)      1180672     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_7 (Attention)         (None, 80, 256)      336         bidirectional_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 80, 512)      0           bidirectional_7[0][0]            \n",
      "                                                                 attention_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 80, 128)      65664       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 80, 1)        129         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "scale_layer_1 (ScaleLayer)      (None, 80, 1)        0           dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 33,688,897\n",
      "Trainable params: 33,688,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(0.001, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 64s 507ms/step - loss: 4.0230 - val_loss: 1.0569\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.05687, saving model to folds1.hdf5\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to tf.Tensor(0.000995129, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.9862 - val_loss: 0.7413\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.05687 to 0.74131, saving model to folds1.hdf5\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009902818, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.7105 - val_loss: 0.6358\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.74131 to 0.63580, saving model to folds1.hdf5\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009854581, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.5847 - val_loss: 0.6304\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63580 to 0.63035, saving model to folds1.hdf5\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.000980658, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.5234 - val_loss: 0.4849\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.63035 to 0.48487, saving model to folds1.hdf5\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097588124, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.4678 - val_loss: 0.5188\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.48487\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009711277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.4491 - val_loss: 0.4558\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.48487 to 0.45584, saving model to folds1.hdf5\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009663974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.4178 - val_loss: 0.4358\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.45584 to 0.43578, saving model to folds1.hdf5\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009616901, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.4069 - val_loss: 0.4065\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.43578 to 0.40651, saving model to folds1.hdf5\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009570057, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.4038 - val_loss: 0.3731\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.40651 to 0.37311, saving model to folds1.hdf5\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095234415, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.3630 - val_loss: 0.3476\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.37311 to 0.34760, saving model to folds1.hdf5\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009477053, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.3664 - val_loss: 0.3811\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.34760\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009430891, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.3798 - val_loss: 0.3643\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.34760\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009384953, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.3404 - val_loss: 0.3167\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.34760 to 0.31669, saving model to folds1.hdf5\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009339239, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.3236 - val_loss: 0.3131\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.31669 to 0.31310, saving model to folds1.hdf5\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009293747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.3224 - val_loss: 0.3212\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.31310\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009248478, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.3140 - val_loss: 0.3226\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31310\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(0.00092034286, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.3048 - val_loss: 0.3234\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.31310\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091585994, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.3024 - val_loss: 0.3178\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.31310\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091139873, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.2925 - val_loss: 0.3209\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.31310\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090695935, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2901 - val_loss: 0.3006\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.31310 to 0.30063, saving model to folds1.hdf5\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090254156, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2819 - val_loss: 0.3421\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.30063\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008981453, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.2903 - val_loss: 0.2778\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.30063 to 0.27783, saving model to folds1.hdf5\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008937704, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2656 - val_loss: 0.2623\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.27783 to 0.26230, saving model to folds1.hdf5\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008894169, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2634 - val_loss: 0.2691\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.26230\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088508456, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.2680 - val_loss: 0.2561\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.26230 to 0.25610, saving model to folds1.hdf5\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088077335, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2558 - val_loss: 0.2670\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.25610\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008764831, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.2582 - val_loss: 0.2878\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.25610\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to tf.Tensor(0.00087221374, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2607 - val_loss: 0.2958\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.25610\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008679653, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.2583 - val_loss: 0.2632\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.25610\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to tf.Tensor(0.00086373737, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.2435 - val_loss: 0.2476\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.25610 to 0.24759, saving model to folds1.hdf5\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008595301, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2396 - val_loss: 0.2696\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.24759\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to tf.Tensor(0.00085534336, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2356 - val_loss: 0.2272\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.24759 to 0.22725, saving model to folds1.hdf5\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to tf.Tensor(0.000851177, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2246 - val_loss: 0.2304\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.22725\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to tf.Tensor(0.00084703095, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2292 - val_loss: 0.2400\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.22725\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008429051, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2314 - val_loss: 0.2354\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.22725\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to tf.Tensor(0.00083879934, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2227 - val_loss: 0.2431\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.22725\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008347136, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.2246 - val_loss: 0.2214\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.22725 to 0.22145, saving model to folds1.hdf5\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008306477, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2131 - val_loss: 0.2445\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.22145\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008266016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2226 - val_loss: 0.2292\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.22145\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to tf.Tensor(0.00082257524, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2208 - val_loss: 0.2303\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.22145\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008185685, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.2183 - val_loss: 0.2355\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.22145\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081458123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 14s 204ms/step - loss: 0.2209 - val_loss: 0.2385\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.22145\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081061345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2190 - val_loss: 0.2290\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.22145\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to tf.Tensor(0.000806665, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2107 - val_loss: 0.2216\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.22145\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to tf.Tensor(0.00080273574, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2044 - val_loss: 0.2190\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.22145 to 0.21896, saving model to folds1.hdf5\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007988256, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2019 - val_loss: 0.2122\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.21896 to 0.21216, saving model to folds1.hdf5\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007949346, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2047 - val_loss: 0.2190\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.21216\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007910624, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.2040 - val_loss: 0.2369\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.21216\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007872092, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2182 - val_loss: 0.2235\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.21216\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007833747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2041 - val_loss: 0.2178\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.21216\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077955885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1986 - val_loss: 0.2241\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.21216\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077576167, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2035 - val_loss: 0.2136\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.21216\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077198294, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1951 - val_loss: 0.2200\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.21216\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007682226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2035 - val_loss: 0.2177\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.21216\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007644806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1996 - val_loss: 0.2016\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.21216 to 0.20161, saving model to folds1.hdf5\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007607569, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1857 - val_loss: 0.2161\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.20161\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007570512, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1915 - val_loss: 0.2181\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.20161\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to tf.Tensor(0.00075336365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1923 - val_loss: 0.1981\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.20161 to 0.19808, saving model to folds1.hdf5\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to tf.Tensor(0.000749694, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1824 - val_loss: 0.1975\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.19808 to 0.19753, saving model to folds1.hdf5\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007460423, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1843 - val_loss: 0.2073\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.19753\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to tf.Tensor(0.00074240833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1834 - val_loss: 0.1958\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.19753 to 0.19579, saving model to folds1.hdf5\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007387921, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1786 - val_loss: 0.1967\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.19579\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to tf.Tensor(0.00073519343, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1806 - val_loss: 0.2042\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.19579\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007316123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1832 - val_loss: 0.2146\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.19579\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007280487, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1787 - val_loss: 0.2076\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.19579\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007245024, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1806 - val_loss: 0.1992\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.19579\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007209733, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1756 - val_loss: 0.2099\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.19579\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071746146, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1772 - val_loss: 0.2057\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.19579\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007139667, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1798 - val_loss: 0.1933\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.19579 to 0.19333, saving model to folds1.hdf5\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071048894, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1701 - val_loss: 0.2230\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.19333\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007070282, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2037 - val_loss: 0.2110\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.19333\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007035842, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1782 - val_loss: 0.1949\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.19333\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to tf.Tensor(0.00070015714, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1692 - val_loss: 0.1963\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.19333\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006967467, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1738 - val_loss: 0.1954\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.19333\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to tf.Tensor(0.00069335283, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1709 - val_loss: 0.1968\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.19333\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006899755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1720 - val_loss: 0.2273\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.19333\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068661466, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1731 - val_loss: 0.1881\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.19333 to 0.18810, saving model to folds1.hdf5\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068327016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1691 - val_loss: 0.1940\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.18810\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067994196, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1606 - val_loss: 0.1859\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.18810 to 0.18591, saving model to folds1.hdf5\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067663, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1562 - val_loss: 0.1955\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.18591\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006733342, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1672 - val_loss: 0.2043\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.18591\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067005435, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1640 - val_loss: 0.1876\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.18591\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006667905, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1642 - val_loss: 0.1939\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.18591\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006635426, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1608 - val_loss: 0.1864\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.18591\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006603105, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1569 - val_loss: 0.1875\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.18591\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006570942, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1586 - val_loss: 0.1839\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.18591 to 0.18394, saving model to folds1.hdf5\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065389345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1554 - val_loss: 0.1887\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.18394\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065070833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1522 - val_loss: 0.1908\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.18394\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to tf.Tensor(0.00064753875, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 199ms/step - loss: 0.1520 - val_loss: 0.1921\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.18394\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006443846, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1495 - val_loss: 0.1877\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.18394\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006412458, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1506 - val_loss: 0.1795\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.18394 to 0.17950, saving model to folds1.hdf5\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063812226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1499 - val_loss: 0.1844\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.17950\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063501403, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1474 - val_loss: 0.1846\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.17950\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006319209, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1489 - val_loss: 0.1979\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.17950\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006288428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1508 - val_loss: 0.1999\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.17950\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to tf.Tensor(0.00062577974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1523 - val_loss: 0.1880\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.17950\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006227316, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1496 - val_loss: 0.1850\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.17950\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061969826, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1497 - val_loss: 0.1860\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.17950\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006166797, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1433 - val_loss: 0.1868\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.17950\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061367586, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.1437 - val_loss: 0.1991\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.17950\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061068666, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1519 - val_loss: 0.1830\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.17950\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006077121, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1462 - val_loss: 0.1789\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.17950 to 0.17891, saving model to folds1.hdf5\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060475187, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1452 - val_loss: 0.1916\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.17891\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060180615, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1499 - val_loss: 0.1874\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.17891\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059887476, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1435 - val_loss: 0.1834\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.17891\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005959577, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1407 - val_loss: 0.1844\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.17891\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059305475, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1413 - val_loss: 0.1926\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.17891\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to tf.Tensor(0.000590166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1431 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00109: val_loss improved from 0.17891 to 0.17860, saving model to folds1.hdf5\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to tf.Tensor(0.00058729126, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1357 - val_loss: 0.1810\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.17860\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005844306, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1331 - val_loss: 0.1770\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.17860 to 0.17699, saving model to folds1.hdf5\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005815838, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.1324 - val_loss: 0.1881\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.17699\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to tf.Tensor(0.000578751, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1381 - val_loss: 0.1944\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.17699\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057593186, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1491 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.17699\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057312654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1341 - val_loss: 0.1770\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.17699\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057033484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1303 - val_loss: 0.1763\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.17699 to 0.17626, saving model to folds1.hdf5\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to tf.Tensor(0.00056755677, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.1287 - val_loss: 0.1766\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.17626\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005647922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1283 - val_loss: 0.1767\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.17626\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005620411, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1264 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.17626\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055930344, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1265 - val_loss: 0.1868\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.17626\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055657903, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1257 - val_loss: 0.1739\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.17626 to 0.17391, saving model to folds1.hdf5\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055386795, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1244 - val_loss: 0.1819\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.17391\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005511701, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1252 - val_loss: 0.1798\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.17391\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005484854, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1312 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.17391\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005458137, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1261 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.17391\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to tf.Tensor(0.00054315507, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1257 - val_loss: 0.1820\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.17391\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005405093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1233 - val_loss: 0.1840\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.17391\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005378765, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1240 - val_loss: 0.1912\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.17391\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005352565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1272 - val_loss: 0.1835\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.17391\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005326493, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.1200 - val_loss: 0.1812\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.17391\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005300548, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1219 - val_loss: 0.1929\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.17391\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005274729, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1203 - val_loss: 0.1813\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.17391\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052490365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 14s 205ms/step - loss: 0.1166 - val_loss: 0.1816\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.17391\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052234676, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1310 - val_loss: 0.1897\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.17391\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005198025, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1249 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.17391\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005172705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1252 - val_loss: 0.1781\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.17391\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051475083, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1203 - val_loss: 0.1860\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.17391\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051224354, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1221 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.17391\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005097484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1142 - val_loss: 0.1731\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.17391 to 0.17308, saving model to folds1.hdf5\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005072654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1130 - val_loss: 0.1859\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.17308\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005047946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1148 - val_loss: 0.1735\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.17308\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005023357, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1133 - val_loss: 0.1725\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.17308 to 0.17248, saving model to folds1.hdf5\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049988885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1101 - val_loss: 0.1716\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.17248 to 0.17157, saving model to folds1.hdf5\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004974539, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1092 - val_loss: 0.1750\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.17157\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004950308, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1096 - val_loss: 0.1804\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.17157\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004926195, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1098 - val_loss: 0.1736\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.17157\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049022, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1069 - val_loss: 0.1802\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.17157\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048783212, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1088 - val_loss: 0.1736\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.17157\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048545588, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1065 - val_loss: 0.1739\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.17157\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048309125, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1064 - val_loss: 0.1819\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.17157\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048073815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1062 - val_loss: 0.1722\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.17157\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047839645, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1050 - val_loss: 0.1731\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.17157\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004760662, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1039 - val_loss: 0.1715\n",
      "\n",
      "Epoch 00153: val_loss improved from 0.17157 to 0.17149, saving model to folds1.hdf5\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004737473, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1044 - val_loss: 0.1737\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.17149\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047143968, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1033 - val_loss: 0.1756\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.17149\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004691433, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1009 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.17149\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046685815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1032 - val_loss: 0.1748\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.17149\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046458407, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1028 - val_loss: 0.1727\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.17149\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046232107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0996 - val_loss: 0.1745\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.17149\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046006907, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0979 - val_loss: 0.1744\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.17149\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045782814, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0992 - val_loss: 0.1717\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.17149\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045559806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0959 - val_loss: 0.1723\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.17149\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045337883, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0968 - val_loss: 0.1732\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.17149\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045117046, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0951 - val_loss: 0.1756\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.17149\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044897277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0962 - val_loss: 0.1720\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.17149\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044678585, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0948 - val_loss: 0.1716\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.17149\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004446096, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0948 - val_loss: 0.1782\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.17149\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004424439, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0946 - val_loss: 0.1715\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.17149\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044028874, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0933 - val_loss: 0.1711\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.17149 to 0.17105, saving model to folds1.hdf5\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043814414, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0934 - val_loss: 0.1712\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.17105\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043600993, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0955 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.17105\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043388613, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0992 - val_loss: 0.1821\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.17105\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004317727, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1006 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.17105\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004296695, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0957 - val_loss: 0.1756\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.17105\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004275766, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0925 - val_loss: 0.1724\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.17105\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042549393, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0914 - val_loss: 0.1719\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.17105\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042342133, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0902 - val_loss: 0.1718\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.17105\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042135885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0897 - val_loss: 0.1769\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.17105\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041930642, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 198ms/step - loss: 0.0900 - val_loss: 0.1723\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.17105\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041726397, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0875 - val_loss: 0.1727\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.17105\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041523148, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0886 - val_loss: 0.1726\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.17105\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041320888, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0883 - val_loss: 0.1728\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.17105\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041119618, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0870 - val_loss: 0.1728\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.17105\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040919325, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0866 - val_loss: 0.1732\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.17105\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040720004, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0858 - val_loss: 0.1731\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.17105\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004052166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0865 - val_loss: 0.1720\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.17105\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004032428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0838 - val_loss: 0.1717\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.17105\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004012786, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0853 - val_loss: 0.1742\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.17105\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039932402, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0835 - val_loss: 0.1836\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.17105\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039737887, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0930 - val_loss: 0.1753\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.17105\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039544326, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0856 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.17105\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039351705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0839 - val_loss: 0.1749\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.17105\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039160028, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0843 - val_loss: 0.1739\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.17105\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038969278, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0822 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.17105\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003877946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0829 - val_loss: 0.1752\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.17105\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038590565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0808 - val_loss: 0.1737\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.17105\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003840259, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0805 - val_loss: 0.1729\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.17105\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038215535, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0804 - val_loss: 0.1731\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.17105\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038029387, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0812 - val_loss: 0.1726\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.17105\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037844144, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0809 - val_loss: 0.1741\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.17105\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037659804, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0792 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.17105\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037476368, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0798 - val_loss: 0.1745\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.17105\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037293817, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0800 - val_loss: 0.1728\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.17105\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003711216, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0775 - val_loss: 0.1739\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.17105\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003693139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0787 - val_loss: 0.1747\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.17105\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to tf.Tensor(0.000367515, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0807 - val_loss: 0.1741\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.17105\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036572482, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0788 - val_loss: 0.1728\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.17105\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003639434, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0788 - val_loss: 0.1720\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.17105\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036217063, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0768 - val_loss: 0.1734\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.17105\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036040647, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0760 - val_loss: 0.1731\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.17105\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to tf.Tensor(0.00035865093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0755 - val_loss: 0.1720\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.17105\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to tf.Tensor(0.000356904, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0750 - val_loss: 0.1769\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.17105\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003551655, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0741 - val_loss: 0.1726\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.17105\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003534355, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0731 - val_loss: 0.1747\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.17105\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003517139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0746 - val_loss: 0.1751\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.17105\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003500007, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0745 - val_loss: 0.1758\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.17105\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034829587, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0720 - val_loss: 0.1734\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.17105\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034659932, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0722 - val_loss: 0.1727\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.17105\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034491107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0753 - val_loss: 0.1806\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.17105\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034323102, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0742 - val_loss: 0.1722\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.17105\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034155912, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 198ms/step - loss: 0.0703 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.17105\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033989537, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0690 - val_loss: 0.1752\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.17105\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033823974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0707 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.17105\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003365922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0702 - val_loss: 0.1755\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.17105\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033495267, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0697 - val_loss: 0.1715\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.17105\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003333211, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0685 - val_loss: 0.1770\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.17105\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033169755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0701 - val_loss: 0.1727\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.17105\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033008185, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0692 - val_loss: 0.1729\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.17105\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to tf.Tensor(0.000328474, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0680 - val_loss: 0.1739\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.17105\n",
      "Epoch 00229: early stopping\n",
      "--------------- > Fold 3 < ---------------\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 80, 54)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_8 (Bidirectional) (None, 80, 2048)     8839168     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 80, 2048)     2128        bidirectional_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 80, 4096)     0           bidirectional_8[0][0]            \n",
      "                                                                 attention_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_9 (Bidirectional) (None, 80, 1024)     18878464    concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 80, 1024)     1104        bidirectional_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 80, 2048)     0           bidirectional_9[0][0]            \n",
      "                                                                 attention_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_10 (Bidirectional (None, 80, 512)      4720640     concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 80, 512)      592         bidirectional_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 80, 1024)     0           bidirectional_10[0][0]           \n",
      "                                                                 attention_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_11 (Bidirectional (None, 80, 256)      1180672     concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 80, 256)      336         bidirectional_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 80, 512)      0           bidirectional_11[0][0]           \n",
      "                                                                 attention_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 80, 128)      65664       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 80, 1)        129         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "scale_layer_2 (ScaleLayer)      (None, 80, 1)        0           dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 33,688,897\n",
      "Trainable params: 33,688,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(0.001, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 61s 490ms/step - loss: 4.1462 - val_loss: 1.4984\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.49843, saving model to folds2.hdf5\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to tf.Tensor(0.000995129, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 1.1026 - val_loss: 0.8682\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.49843 to 0.86818, saving model to folds2.hdf5\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009902818, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.7539 - val_loss: 0.6930\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.86818 to 0.69305, saving model to folds2.hdf5\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009854581, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.6063 - val_loss: 0.5382\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.69305 to 0.53817, saving model to folds2.hdf5\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.000980658, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5325 - val_loss: 0.5008\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.53817 to 0.50076, saving model to folds2.hdf5\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097588124, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5404 - val_loss: 0.4521\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.50076 to 0.45209, saving model to folds2.hdf5\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009711277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4436 - val_loss: 0.4471\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.45209 to 0.44708, saving model to folds2.hdf5\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009663974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4399 - val_loss: 0.4076\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.44708 to 0.40761, saving model to folds2.hdf5\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009616901, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4368 - val_loss: 0.4334\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.40761\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009570057, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3980 - val_loss: 0.4052\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.40761 to 0.40519, saving model to folds2.hdf5\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095234415, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3958 - val_loss: 0.4553\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.40519\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009477053, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.3797 - val_loss: 0.3732\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.40519 to 0.37321, saving model to folds2.hdf5\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009430891, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3704 - val_loss: 0.3611\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.37321 to 0.36115, saving model to folds2.hdf5\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009384953, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3495 - val_loss: 0.3685\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.36115\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009339239, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3423 - val_loss: 0.3383\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.36115 to 0.33826, saving model to folds2.hdf5\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009293747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3377 - val_loss: 0.3351\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.33826 to 0.33509, saving model to folds2.hdf5\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009248478, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3313 - val_loss: 0.3199\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.33509 to 0.31993, saving model to folds2.hdf5\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(0.00092034286, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3177 - val_loss: 0.3293\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.31993\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091585994, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3103 - val_loss: 0.3326\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.31993\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091139873, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3119 - val_loss: 0.2977\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.31993 to 0.29769, saving model to folds2.hdf5\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090695935, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2973 - val_loss: 0.3075\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.29769\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090254156, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3049 - val_loss: 0.2892\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.29769 to 0.28919, saving model to folds2.hdf5\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008981453, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2933 - val_loss: 0.2898\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.28919\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008937704, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2814 - val_loss: 0.3027\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.28919\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008894169, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2834 - val_loss: 0.3090\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.28919\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088508456, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2750 - val_loss: 0.2756\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.28919 to 0.27559, saving model to folds2.hdf5\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088077335, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2762 - val_loss: 0.2812\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.27559\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008764831, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2765 - val_loss: 0.2709\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.27559 to 0.27087, saving model to folds2.hdf5\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to tf.Tensor(0.00087221374, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2597 - val_loss: 0.2711\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.27087\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008679653, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2581 - val_loss: 0.2921\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.27087\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to tf.Tensor(0.00086373737, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 196ms/step - loss: 0.2566 - val_loss: 0.2676\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.27087 to 0.26761, saving model to folds2.hdf5\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008595301, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2514 - val_loss: 0.2744\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.26761\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to tf.Tensor(0.00085534336, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2424 - val_loss: 0.2508\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.26761 to 0.25077, saving model to folds2.hdf5\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to tf.Tensor(0.000851177, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2371 - val_loss: 0.2575\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.25077\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to tf.Tensor(0.00084703095, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2305 - val_loss: 0.2673\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.25077\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008429051, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2349 - val_loss: 0.2484\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.25077 to 0.24842, saving model to folds2.hdf5\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to tf.Tensor(0.00083879934, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2307 - val_loss: 0.2399\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.24842 to 0.23991, saving model to folds2.hdf5\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008347136, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2363 - val_loss: 0.2378\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.23991 to 0.23781, saving model to folds2.hdf5\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008306477, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2251 - val_loss: 0.2532\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.23781\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008266016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2325 - val_loss: 0.2343\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.23781 to 0.23431, saving model to folds2.hdf5\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to tf.Tensor(0.00082257524, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2170 - val_loss: 0.2417\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.23431\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008185685, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2215 - val_loss: 0.2485\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.23431\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081458123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2251 - val_loss: 0.2593\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.23431\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081061345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2385 - val_loss: 0.2314\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.23431 to 0.23142, saving model to folds2.hdf5\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to tf.Tensor(0.000806665, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2198 - val_loss: 0.2341\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.23142\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to tf.Tensor(0.00080273574, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2203 - val_loss: 0.2483\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.23142\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007988256, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2207 - val_loss: 0.2443\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.23142\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007949346, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2112 - val_loss: 0.2198\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.23142 to 0.21979, saving model to folds2.hdf5\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007910624, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2084 - val_loss: 0.2274\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.21979\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007872092, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2072 - val_loss: 0.2306\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.21979\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007833747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2067 - val_loss: 0.2175\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.21979 to 0.21745, saving model to folds2.hdf5\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077955885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2064 - val_loss: 0.2269\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.21745\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077576167, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2035 - val_loss: 0.2338\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.21745\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077198294, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2026 - val_loss: 0.2157\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.21745 to 0.21571, saving model to folds2.hdf5\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007682226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1958 - val_loss: 0.2207\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.21571\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007644806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1981 - val_loss: 0.2165\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.21571\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007607569, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2081 - val_loss: 0.2301\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.21571\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007570512, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1978 - val_loss: 0.2230\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.21571\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to tf.Tensor(0.00075336365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1969 - val_loss: 0.2214\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.21571\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to tf.Tensor(0.000749694, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1927 - val_loss: 0.2090\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.21571 to 0.20899, saving model to folds2.hdf5\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007460423, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1976 - val_loss: 0.2297\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.20899\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to tf.Tensor(0.00074240833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1982 - val_loss: 0.2117\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.20899\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007387921, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1932 - val_loss: 0.2123\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.20899\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to tf.Tensor(0.00073519343, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1945 - val_loss: 0.2139\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.20899\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007316123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1861 - val_loss: 0.2055\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.20899 to 0.20552, saving model to folds2.hdf5\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007280487, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1909 - val_loss: 0.2195\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.20552\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007245024, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1968 - val_loss: 0.2194\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.20552\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007209733, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1876 - val_loss: 0.2037\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.20552 to 0.20372, saving model to folds2.hdf5\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071746146, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1883 - val_loss: 0.1983\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.20372 to 0.19833, saving model to folds2.hdf5\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007139667, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1910 - val_loss: 0.2121\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.19833\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071048894, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1916 - val_loss: 0.2203\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.19833\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007070282, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1842 - val_loss: 0.2072\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.19833\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007035842, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1882 - val_loss: 0.2166\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.19833\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to tf.Tensor(0.00070015714, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1885 - val_loss: 0.2127\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.19833\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006967467, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1781 - val_loss: 0.2054\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.19833\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to tf.Tensor(0.00069335283, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1750 - val_loss: 0.2267\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.19833\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006899755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1757 - val_loss: 0.1980\n",
      "\n",
      "Epoch 00077: val_loss improved from 0.19833 to 0.19796, saving model to folds2.hdf5\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068661466, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1732 - val_loss: 0.2020\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.19796\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068327016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1751 - val_loss: 0.1986\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.19796\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067994196, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1670 - val_loss: 0.1968\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.19796 to 0.19680, saving model to folds2.hdf5\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067663, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 198ms/step - loss: 0.1674 - val_loss: 0.2701\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.19680\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006733342, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2151 - val_loss: 0.2057\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.19680\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067005435, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1729 - val_loss: 0.2142\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.19680\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006667905, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1680 - val_loss: 0.2149\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.19680\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006635426, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1744 - val_loss: 0.2176\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.19680\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006603105, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1712 - val_loss: 0.2062\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.19680\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006570942, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1694 - val_loss: 0.1996\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.19680\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065389345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1667 - val_loss: 0.1970\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.19680\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065070833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1632 - val_loss: 0.1946\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.19680 to 0.19465, saving model to folds2.hdf5\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to tf.Tensor(0.00064753875, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1624 - val_loss: 0.1951\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.19465\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006443846, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1727 - val_loss: 0.2140\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.19465\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006412458, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1767 - val_loss: 0.2007\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.19465\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063812226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1701 - val_loss: 0.1968\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.19465\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063501403, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1573 - val_loss: 0.1982\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.19465\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006319209, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1650 - val_loss: 0.1956\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.19465\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006288428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1584 - val_loss: 0.1970\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.19465\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to tf.Tensor(0.00062577974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1615 - val_loss: 0.1863\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.19465 to 0.18635, saving model to folds2.hdf5\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006227316, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1509 - val_loss: 0.1911\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.18635\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061969826, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1496 - val_loss: 0.1937\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.18635\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006166797, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1524 - val_loss: 0.1965\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.18635\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061367586, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1507 - val_loss: 0.1948\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.18635\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061068666, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1644 - val_loss: 0.2056\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.18635\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006077121, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1559 - val_loss: 0.2196\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.18635\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060475187, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1574 - val_loss: 0.1930\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.18635\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060180615, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1545 - val_loss: 0.2097\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.18635\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059887476, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1581 - val_loss: 0.1906\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.18635\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005959577, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1595 - val_loss: 0.1890\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.18635\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059305475, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1516 - val_loss: 0.1998\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.18635\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to tf.Tensor(0.000590166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1564 - val_loss: 0.1877\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.18635\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to tf.Tensor(0.00058729126, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1460 - val_loss: 0.1852\n",
      "\n",
      "Epoch 00110: val_loss improved from 0.18635 to 0.18523, saving model to folds2.hdf5\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005844306, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1447 - val_loss: 0.1958\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.18523\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005815838, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1510 - val_loss: 0.1852\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.18523 to 0.18519, saving model to folds2.hdf5\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to tf.Tensor(0.000578751, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1389 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.18519 to 0.18311, saving model to folds2.hdf5\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057593186, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1408 - val_loss: 0.1879\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.18311\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057312654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1381 - val_loss: 0.1888\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.18311\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057033484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1354 - val_loss: 0.1815\n",
      "\n",
      "Epoch 00116: val_loss improved from 0.18311 to 0.18149, saving model to folds2.hdf5\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to tf.Tensor(0.00056755677, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 0.1360 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.18149\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005647922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1348 - val_loss: 0.1946\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.18149\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005620411, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1351 - val_loss: 0.1810\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.18149 to 0.18097, saving model to folds2.hdf5\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055930344, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1339 - val_loss: 0.1819\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.18097\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055657903, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1327 - val_loss: 0.1803\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.18097 to 0.18028, saving model to folds2.hdf5\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055386795, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1300 - val_loss: 0.1810\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.18028\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005511701, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1292 - val_loss: 0.1861\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.18028\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005484854, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1312 - val_loss: 0.1812\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.18028\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005458137, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1308 - val_loss: 0.1864\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.18028\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to tf.Tensor(0.00054315507, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1303 - val_loss: 0.1801\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.18028 to 0.18012, saving model to folds2.hdf5\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005405093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1279 - val_loss: 0.1819\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.18012\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005378765, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1263 - val_loss: 0.1807\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.18012\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005352565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1263 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.18012\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005326493, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1287 - val_loss: 0.1894\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.18012\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005300548, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1289 - val_loss: 0.1807\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.18012\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005274729, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1260 - val_loss: 0.1974\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.18012\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052490365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1271 - val_loss: 0.1810\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.18012\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052234676, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1259 - val_loss: 0.1800\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.18012 to 0.18004, saving model to folds2.hdf5\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005198025, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1216 - val_loss: 0.1818\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.18004\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005172705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1231 - val_loss: 0.1792\n",
      "\n",
      "Epoch 00136: val_loss improved from 0.18004 to 0.17924, saving model to folds2.hdf5\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051475083, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1209 - val_loss: 0.1822\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.17924\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051224354, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1231 - val_loss: 0.1869\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.17924\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005097484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1296 - val_loss: 0.1815\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.17924\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005072654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1180 - val_loss: 0.1926\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.17924\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005047946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1216 - val_loss: 0.1846\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.17924\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005023357, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1207 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.17924 to 0.17801, saving model to folds2.hdf5\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049988885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1197 - val_loss: 0.1812\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.17801\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004974539, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1177 - val_loss: 0.1807\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.17801\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004950308, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1193 - val_loss: 0.2006\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.17801\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004926195, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1289 - val_loss: 0.1873\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.17801\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049022, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1280 - val_loss: 0.1863\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.17801\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048783212, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1351 - val_loss: 0.1869\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.17801\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048545588, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1268 - val_loss: 0.1915\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.17801\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048309125, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1205 - val_loss: 0.1798\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.17801\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048073815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1150 - val_loss: 0.1991\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.17801\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047839645, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1181 - val_loss: 0.1802\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.17801\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004760662, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1122 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.17801\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004737473, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1114 - val_loss: 0.1808\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.17801\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047143968, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1163 - val_loss: 0.1789\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.17801\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004691433, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1141 - val_loss: 0.1800\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.17801\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046685815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1147 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.17801\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046458407, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1103 - val_loss: 0.1817\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.17801\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046232107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 198ms/step - loss: 0.1108 - val_loss: 0.1781\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.17801\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046006907, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1088 - val_loss: 0.1775\n",
      "\n",
      "Epoch 00160: val_loss improved from 0.17801 to 0.17755, saving model to folds2.hdf5\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045782814, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1116 - val_loss: 0.1859\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.17755\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045559806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1148 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.17755\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045337883, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1084 - val_loss: 0.1767\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.17755 to 0.17665, saving model to folds2.hdf5\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045117046, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1072 - val_loss: 0.1801\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.17665\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044897277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1060 - val_loss: 0.1809\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.17665\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044678585, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1070 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.17665\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004446096, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1038 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.17665\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004424439, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1033 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.17665\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044028874, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1022 - val_loss: 0.1761\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.17665 to 0.17610, saving model to folds2.hdf5\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043814414, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1044 - val_loss: 0.1772\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.17610\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043600993, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1063 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.17610\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043388613, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1011 - val_loss: 0.1796\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.17610\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004317727, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1001 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.17610\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004296695, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0984 - val_loss: 0.1790\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.17610\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004275766, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1015 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.17610\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042549393, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0983 - val_loss: 0.1771\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.17610\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042342133, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0985 - val_loss: 0.1760\n",
      "\n",
      "Epoch 00177: val_loss improved from 0.17610 to 0.17598, saving model to folds2.hdf5\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042135885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0960 - val_loss: 0.1768\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.17598\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041930642, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0978 - val_loss: 0.1775\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.17598\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041726397, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0958 - val_loss: 0.1795\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.17598\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041523148, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0960 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.17598\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041320888, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0963 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.17598\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041119618, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0943 - val_loss: 0.1785\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.17598\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040919325, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0953 - val_loss: 0.1816\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.17598\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040720004, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0925 - val_loss: 0.1825\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.17598\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004052166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0958 - val_loss: 0.1785\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.17598\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004032428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0937 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.17598\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004012786, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0965 - val_loss: 0.1785\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.17598\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039932402, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0934 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.17598\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039737887, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0921 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.17598\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039544326, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0906 - val_loss: 0.1760\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.17598 to 0.17596, saving model to folds2.hdf5\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039351705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0912 - val_loss: 0.1775\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.17596\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039160028, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0895 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.17596\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038969278, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0886 - val_loss: 0.1760\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.17596\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003877946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0902 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.17596\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038590565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0902 - val_loss: 0.1803\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.17596\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003840259, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0876 - val_loss: 0.1837\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.17596\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038215535, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0898 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.17596\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038029387, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0886 - val_loss: 0.1817\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.17596\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037844144, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0864 - val_loss: 0.1771\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.17596\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037659804, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0860 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.17596\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037476368, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0869 - val_loss: 0.1812\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.17596\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037293817, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0876 - val_loss: 0.1832\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.17596\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003711216, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0858 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.17596\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003693139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 0.0848 - val_loss: 0.1784\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.17596\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to tf.Tensor(0.000367515, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0866 - val_loss: 0.1770\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.17596\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036572482, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0830 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.17596\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003639434, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0828 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.17596\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036217063, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0832 - val_loss: 0.1806\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.17596\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036040647, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0820 - val_loss: 0.1800\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.17596\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to tf.Tensor(0.00035865093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0815 - val_loss: 0.1803\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.17596\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to tf.Tensor(0.000356904, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0815 - val_loss: 0.1790\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.17596\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003551655, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0815 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.17596\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003534355, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0831 - val_loss: 0.1800\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.17596\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003517139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0823 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.17596\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003500007, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0807 - val_loss: 0.1792\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.17596\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034829587, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0789 - val_loss: 0.1814\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.17596\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034659932, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0792 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.17596\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034491107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0786 - val_loss: 0.1847\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.17596\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034323102, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0793 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.17596\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034155912, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0789 - val_loss: 0.1776\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.17596\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033989537, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0787 - val_loss: 0.1781\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.17596\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033823974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0774 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.17596\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003365922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0775 - val_loss: 0.1766\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.17596\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033495267, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0759 - val_loss: 0.1782\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.17596\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003333211, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0775 - val_loss: 0.1775\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.17596\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033169755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0751 - val_loss: 0.1808\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.17596\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033008185, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0756 - val_loss: 0.1808\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.17596\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to tf.Tensor(0.000328474, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0743 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.17596\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to tf.Tensor(0.00032687403, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0766 - val_loss: 0.1801\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.17596\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003252818, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0749 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.17596\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to tf.Tensor(0.00032369737, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0751 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.17596\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to tf.Tensor(0.00032212064, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0732 - val_loss: 0.1770\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.17596\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to tf.Tensor(0.00032055163, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0726 - val_loss: 0.1808\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.17596\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to tf.Tensor(0.00031899018, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0751 - val_loss: 0.1802\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.17596\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003174364, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0742 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.17596\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003158902, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0738 - val_loss: 0.1790\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.17596\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003143515, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0737 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.17596\n",
      "Epoch 239/300\n",
      "\n",
      "Epoch 00239: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003128203, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0720 - val_loss: 0.1817\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.17596\n",
      "Epoch 240/300\n",
      "\n",
      "Epoch 00240: LearningRateScheduler reducing learning rate to tf.Tensor(0.00031129655, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0780 - val_loss: 0.1781\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.17596\n",
      "Epoch 241/300\n",
      "\n",
      "Epoch 00241: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003097802, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 0.0713 - val_loss: 0.1792\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.17596\n",
      "Epoch 242/300\n",
      "\n",
      "Epoch 00242: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003082713, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0708 - val_loss: 0.1789\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.17596\n",
      "Epoch 243/300\n",
      "\n",
      "Epoch 00243: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003067697, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0701 - val_loss: 0.1779\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.17596\n",
      "Epoch 244/300\n",
      "\n",
      "Epoch 00244: LearningRateScheduler reducing learning rate to tf.Tensor(0.00030527546, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0700 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.17596\n",
      "Epoch 245/300\n",
      "\n",
      "Epoch 00245: LearningRateScheduler reducing learning rate to tf.Tensor(0.00030378843, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0692 - val_loss: 0.1807\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.17596\n",
      "Epoch 246/300\n",
      "\n",
      "Epoch 00246: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003023087, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0701 - val_loss: 0.1802\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.17596\n",
      "Epoch 247/300\n",
      "\n",
      "Epoch 00247: LearningRateScheduler reducing learning rate to tf.Tensor(0.00030083617, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0695 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.17596\n",
      "Epoch 248/300\n",
      "\n",
      "Epoch 00248: LearningRateScheduler reducing learning rate to tf.Tensor(0.0002993708, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0696 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.17596\n",
      "Epoch 249/300\n",
      "\n",
      "Epoch 00249: LearningRateScheduler reducing learning rate to tf.Tensor(0.00029791257, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0686 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.17596\n",
      "Epoch 250/300\n",
      "\n",
      "Epoch 00250: LearningRateScheduler reducing learning rate to tf.Tensor(0.00029646145, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0672 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.17596\n",
      "Epoch 251/300\n",
      "\n",
      "Epoch 00251: LearningRateScheduler reducing learning rate to tf.Tensor(0.00029501738, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0670 - val_loss: 0.1788\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.17596\n",
      "Epoch 00251: early stopping\n",
      "--------------- > Fold 4 < ---------------\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 80, 54)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_12 (Bidirectional (None, 80, 2048)     8839168     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_12 (Attention)        (None, 80, 2048)     2128        bidirectional_12[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 80, 4096)     0           bidirectional_12[0][0]           \n",
      "                                                                 attention_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_13 (Bidirectional (None, 80, 1024)     18878464    concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_13 (Attention)        (None, 80, 1024)     1104        bidirectional_13[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 80, 2048)     0           bidirectional_13[0][0]           \n",
      "                                                                 attention_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_14 (Bidirectional (None, 80, 512)      4720640     concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_14 (Attention)        (None, 80, 512)      592         bidirectional_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 80, 1024)     0           bidirectional_14[0][0]           \n",
      "                                                                 attention_14[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_15 (Bidirectional (None, 80, 256)      1180672     concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_15 (Attention)        (None, 80, 256)      336         bidirectional_15[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 80, 512)      0           bidirectional_15[0][0]           \n",
      "                                                                 attention_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 80, 128)      65664       concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 80, 1)        129         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "scale_layer_3 (ScaleLayer)      (None, 80, 1)        0           dense_7[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 33,688,897\n",
      "Trainable params: 33,688,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(0.001, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 60s 486ms/step - loss: 4.2093 - val_loss: 1.0006\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.00059, saving model to folds3.hdf5\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to tf.Tensor(0.000995129, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.9318 - val_loss: 0.7643\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.00059 to 0.76430, saving model to folds3.hdf5\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009902818, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.6787 - val_loss: 0.6207\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.76430 to 0.62068, saving model to folds3.hdf5\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009854581, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.5535 - val_loss: 0.6692\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.62068\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.000980658, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5477 - val_loss: 0.4559\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.62068 to 0.45586, saving model to folds3.hdf5\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097588124, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4518 - val_loss: 0.4219\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.45586 to 0.42195, saving model to folds3.hdf5\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009711277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4210 - val_loss: 0.4037\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.42195 to 0.40365, saving model to folds3.hdf5\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009663974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.4277 - val_loss: 0.3921\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.40365 to 0.39207, saving model to folds3.hdf5\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009616901, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3897 - val_loss: 0.4028\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.39207\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009570057, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3788 - val_loss: 0.4008\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.39207\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095234415, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3692 - val_loss: 0.3588\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.39207 to 0.35882, saving model to folds3.hdf5\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009477053, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3483 - val_loss: 0.3842\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35882\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009430891, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3589 - val_loss: 0.3486\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.35882 to 0.34864, saving model to folds3.hdf5\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009384953, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3345 - val_loss: 0.3299\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.34864 to 0.32988, saving model to folds3.hdf5\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009339239, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.3192 - val_loss: 0.3523\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.32988\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009293747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.3217 - val_loss: 0.3156\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.32988 to 0.31559, saving model to folds3.hdf5\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009248478, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3150 - val_loss: 0.3083\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.31559 to 0.30826, saving model to folds3.hdf5\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(0.00092034286, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2999 - val_loss: 0.3092\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.30826\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091585994, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2906 - val_loss: 0.3236\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.30826\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091139873, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2932 - val_loss: 0.3169\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.30826\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090695935, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2860 - val_loss: 0.2896\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.30826 to 0.28960, saving model to folds3.hdf5\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090254156, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2716 - val_loss: 0.2926\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.28960\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008981453, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2744 - val_loss: 0.2984\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.28960\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008937704, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2574 - val_loss: 0.2898\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.28960\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008894169, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2612 - val_loss: 0.2723\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.28960 to 0.27233, saving model to folds3.hdf5\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088508456, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2583 - val_loss: 0.2594\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.27233 to 0.25940, saving model to folds3.hdf5\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088077335, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 195ms/step - loss: 0.2524 - val_loss: 0.2595\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.25940\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008764831, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2430 - val_loss: 0.2532\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.25940 to 0.25324, saving model to folds3.hdf5\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to tf.Tensor(0.00087221374, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2357 - val_loss: 0.2442\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.25324 to 0.24419, saving model to folds3.hdf5\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008679653, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2326 - val_loss: 0.2551\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.24419\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to tf.Tensor(0.00086373737, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2365 - val_loss: 0.2545\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.24419\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008595301, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2355 - val_loss: 0.2423\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.24419 to 0.24230, saving model to folds3.hdf5\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to tf.Tensor(0.00085534336, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2301 - val_loss: 0.2726\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.24230\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to tf.Tensor(0.000851177, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2321 - val_loss: 0.2451\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.24230\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to tf.Tensor(0.00084703095, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2276 - val_loss: 0.2673\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.24230\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008429051, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2349 - val_loss: 0.2420\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.24230 to 0.24201, saving model to folds3.hdf5\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to tf.Tensor(0.00083879934, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2217 - val_loss: 0.2280\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.24201 to 0.22801, saving model to folds3.hdf5\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008347136, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2152 - val_loss: 0.2534\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.22801\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008306477, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2139 - val_loss: 0.2317\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.22801\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008266016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2107 - val_loss: 0.2279\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.22801 to 0.22795, saving model to folds3.hdf5\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to tf.Tensor(0.00082257524, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2166 - val_loss: 0.2280\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.22795\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008185685, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2075 - val_loss: 0.2291\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.22795\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081458123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2058 - val_loss: 0.2549\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.22795\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081061345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2124 - val_loss: 0.2688\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.22795\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to tf.Tensor(0.000806665, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2089 - val_loss: 0.2480\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.22795\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to tf.Tensor(0.00080273574, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2126 - val_loss: 0.2384\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.22795\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007988256, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2071 - val_loss: 0.2150\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.22795 to 0.21498, saving model to folds3.hdf5\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007949346, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1977 - val_loss: 0.2266\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.21498\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007910624, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2001 - val_loss: 0.2329\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.21498\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007872092, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2027 - val_loss: 0.2356\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.21498\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007833747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2042 - val_loss: 0.2209\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.21498\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077955885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1891 - val_loss: 0.2249\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.21498\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077576167, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1875 - val_loss: 0.2205\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.21498\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077198294, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1991 - val_loss: 0.2171\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.21498\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007682226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1836 - val_loss: 0.2192\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.21498\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007644806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1962 - val_loss: 0.2071\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.21498 to 0.20707, saving model to folds3.hdf5\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007607569, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1844 - val_loss: 0.2098\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.20707\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007570512, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1873 - val_loss: 0.2071\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.20707 to 0.20707, saving model to folds3.hdf5\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to tf.Tensor(0.00075336365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1807 - val_loss: 0.2093\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.20707\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to tf.Tensor(0.000749694, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1834 - val_loss: 0.2168\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.20707\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007460423, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1828 - val_loss: 0.2045\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.20707 to 0.20450, saving model to folds3.hdf5\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to tf.Tensor(0.00074240833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1812 - val_loss: 0.2060\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.20450\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007387921, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1810 - val_loss: 0.2135\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.20450\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to tf.Tensor(0.00073519343, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1819 - val_loss: 0.1969\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.20450 to 0.19687, saving model to folds3.hdf5\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007316123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1721 - val_loss: 0.2021\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.19687\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007280487, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1800 - val_loss: 0.2115\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.19687\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007245024, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1754 - val_loss: 0.2007\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.19687\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007209733, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1798 - val_loss: 0.2130\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.19687\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071746146, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1845 - val_loss: 0.2177\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.19687\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007139667, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1768 - val_loss: 0.2108\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.19687\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071048894, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1822 - val_loss: 0.2089\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.19687\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007070282, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1668 - val_loss: 0.2011\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.19687\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007035842, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1715 - val_loss: 0.2078\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.19687\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to tf.Tensor(0.00070015714, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1690 - val_loss: 0.2052\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.19687\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006967467, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1651 - val_loss: 0.1977\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.19687\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to tf.Tensor(0.00069335283, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 198ms/step - loss: 0.1623 - val_loss: 0.1912\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.19687 to 0.19123, saving model to folds3.hdf5\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006899755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1576 - val_loss: 0.2312\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.19123\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068661466, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1898 - val_loss: 0.2063\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.19123\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068327016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1687 - val_loss: 0.1918\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.19123\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067994196, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1597 - val_loss: 0.1951\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.19123\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067663, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1661 - val_loss: 0.1901\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.19123 to 0.19014, saving model to folds3.hdf5\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006733342, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1512 - val_loss: 0.1909\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.19014\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067005435, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1565 - val_loss: 0.1891\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.19014 to 0.18910, saving model to folds3.hdf5\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006667905, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1540 - val_loss: 0.1952\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.18910\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006635426, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1587 - val_loss: 0.1960\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.18910\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006603105, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1631 - val_loss: 0.2067\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.18910\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006570942, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1617 - val_loss: 0.2046\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.18910\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065389345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1678 - val_loss: 0.1940\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.18910\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065070833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1560 - val_loss: 0.1950\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.18910\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to tf.Tensor(0.00064753875, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1509 - val_loss: 0.1926\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.18910\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006443846, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1458 - val_loss: 0.1906\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.18910\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006412458, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1464 - val_loss: 0.1878\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.18910 to 0.18784, saving model to folds3.hdf5\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063812226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1489 - val_loss: 0.1874\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.18784 to 0.18741, saving model to folds3.hdf5\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063501403, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1426 - val_loss: 0.1903\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.18741\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006319209, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1423 - val_loss: 0.1821\n",
      "\n",
      "Epoch 00095: val_loss improved from 0.18741 to 0.18208, saving model to folds3.hdf5\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006288428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1410 - val_loss: 0.1833\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.18208\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to tf.Tensor(0.00062577974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1358 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.18208\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006227316, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1356 - val_loss: 0.1854\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.18208\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061969826, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1409 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.18208\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006166797, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1378 - val_loss: 0.2038\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.18208\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061367586, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1396 - val_loss: 0.1818\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.18208 to 0.18179, saving model to folds3.hdf5\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061068666, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1350 - val_loss: 0.1904\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.18179\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006077121, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1415 - val_loss: 0.1874\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.18179\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060475187, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1357 - val_loss: 0.1825\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.18179\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060180615, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1319 - val_loss: 0.1862\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.18179\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059887476, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1323 - val_loss: 0.1964\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.18179\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005959577, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1361 - val_loss: 0.1855\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.18179\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059305475, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1324 - val_loss: 0.1796\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.18179 to 0.17961, saving model to folds3.hdf5\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to tf.Tensor(0.000590166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1254 - val_loss: 0.1908\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.17961\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to tf.Tensor(0.00058729126, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1280 - val_loss: 0.1817\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.17961\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005844306, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1258 - val_loss: 0.1932\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.17961\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005815838, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1284 - val_loss: 0.1842\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.17961\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to tf.Tensor(0.000578751, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1272 - val_loss: 0.1821\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.17961\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057593186, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1270 - val_loss: 0.1905\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.17961\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057312654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1280 - val_loss: 0.1968\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.17961\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057033484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1292 - val_loss: 0.1981\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.17961\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to tf.Tensor(0.00056755677, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1263 - val_loss: 0.1824\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.17961\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005647922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1307 - val_loss: 0.1877\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.17961\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005620411, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1242 - val_loss: 0.1834\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.17961\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055930344, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1242 - val_loss: 0.1824\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.17961\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055657903, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1242 - val_loss: 0.1879\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.17961\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055386795, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1254 - val_loss: 0.1870\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.17961\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005511701, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1266 - val_loss: 0.1868\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.17961\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005484854, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1198 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.17961\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005458137, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1193 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.17961\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to tf.Tensor(0.00054315507, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1184 - val_loss: 0.1798\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.17961\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005405093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1177 - val_loss: 0.1868\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.17961\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005378765, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1208 - val_loss: 0.1845\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.17961\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005352565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1184 - val_loss: 0.1841\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.17961\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005326493, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 0.1177 - val_loss: 0.1802\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.17961\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005300548, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1133 - val_loss: 0.1863\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.17961\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005274729, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1133 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00132: val_loss improved from 0.17961 to 0.17911, saving model to folds3.hdf5\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052490365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1117 - val_loss: 0.1847\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.17911\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052234676, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1120 - val_loss: 0.1810\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.17911\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005198025, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1095 - val_loss: 0.1818\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.17911\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005172705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1102 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.17911\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051475083, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1075 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.17911\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051224354, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1099 - val_loss: 0.1789\n",
      "\n",
      "Epoch 00138: val_loss improved from 0.17911 to 0.17894, saving model to folds3.hdf5\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005097484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1072 - val_loss: 0.1802\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.17894\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005072654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1124 - val_loss: 0.1843\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.17894\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005047946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1105 - val_loss: 0.1837\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.17894\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005023357, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1072 - val_loss: 0.1839\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.17894\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049988885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1102 - val_loss: 0.1785\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.17894 to 0.17854, saving model to folds3.hdf5\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004974539, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1032 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.17854\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004950308, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1033 - val_loss: 0.1805\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.17854\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004926195, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1030 - val_loss: 0.1834\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.17854\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049022, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1033 - val_loss: 0.1798\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.17854\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048783212, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1038 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.17854\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048545588, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1010 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.17854\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048309125, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0998 - val_loss: 0.1795\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.17854\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048073815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1005 - val_loss: 0.1792\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.17854\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047839645, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0972 - val_loss: 0.1785\n",
      "\n",
      "Epoch 00152: val_loss improved from 0.17854 to 0.17852, saving model to folds3.hdf5\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004760662, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0999 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.17852\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004737473, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0989 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.17852 to 0.17783, saving model to folds3.hdf5\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047143968, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0980 - val_loss: 0.1886\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.17783\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004691433, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1029 - val_loss: 0.1846\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.17783\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046685815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1015 - val_loss: 0.1809\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.17783\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046458407, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0980 - val_loss: 0.1806\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.17783\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046232107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0975 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.17783\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046006907, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0965 - val_loss: 0.1813\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.17783\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045782814, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0961 - val_loss: 0.1772\n",
      "\n",
      "Epoch 00161: val_loss improved from 0.17783 to 0.17718, saving model to folds3.hdf5\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045559806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0931 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.17718\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045337883, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0933 - val_loss: 0.1806\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.17718\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045117046, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0925 - val_loss: 0.1785\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.17718\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044897277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0921 - val_loss: 0.1789\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.17718\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044678585, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0900 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.17718\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004446096, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0895 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.17718\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004424439, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0918 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.17718\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044028874, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0881 - val_loss: 0.1763\n",
      "\n",
      "Epoch 00169: val_loss improved from 0.17718 to 0.17631, saving model to folds3.hdf5\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043814414, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0869 - val_loss: 0.1785\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.17631\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043600993, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0867 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.17631\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043388613, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0876 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.17631\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004317727, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0848 - val_loss: 0.1770\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.17631\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004296695, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0857 - val_loss: 0.1813\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.17631\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004275766, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0863 - val_loss: 0.1861\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.17631\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042549393, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0857 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.17631\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042342133, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0870 - val_loss: 0.1782\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.17631\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042135885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0826 - val_loss: 0.1836\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.17631\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041930642, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0856 - val_loss: 0.1824\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.17631\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041726397, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0851 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.17631\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041523148, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0826 - val_loss: 0.1775\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.17631\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041320888, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0829 - val_loss: 0.1833\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.17631\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041119618, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 198ms/step - loss: 0.0821 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.17631\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040919325, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0800 - val_loss: 0.1901\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.17631\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040720004, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0876 - val_loss: 0.1840\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.17631\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004052166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0949 - val_loss: 0.1839\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.17631\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004032428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0851 - val_loss: 0.1799\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.17631\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004012786, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0814 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.17631\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039932402, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0797 - val_loss: 0.1941\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.17631\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039737887, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0867 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.17631\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039544326, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0810 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.17631\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039351705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0800 - val_loss: 0.1784\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.17631\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039160028, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0791 - val_loss: 0.1779\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.17631\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038969278, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0789 - val_loss: 0.1796\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.17631\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003877946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0763 - val_loss: 0.1775\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.17631\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038590565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0762 - val_loss: 0.1835\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.17631\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003840259, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0766 - val_loss: 0.1768\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.17631\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038215535, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0759 - val_loss: 0.1816\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.17631\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038029387, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0757 - val_loss: 0.1776\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.17631\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037844144, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0748 - val_loss: 0.1812\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.17631\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037659804, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0741 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.17631\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037476368, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0737 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.17631\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037293817, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0720 - val_loss: 0.1792\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.17631\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003711216, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0726 - val_loss: 0.1776\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.17631\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003693139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0710 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.17631\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to tf.Tensor(0.000367515, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0724 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.17631\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036572482, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0698 - val_loss: 0.1816\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.17631\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003639434, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0710 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.17631\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036217063, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0697 - val_loss: 0.1784\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.17631\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036040647, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0694 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.17631\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to tf.Tensor(0.00035865093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0688 - val_loss: 0.1850\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.17631\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to tf.Tensor(0.000356904, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0706 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.17631\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003551655, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0704 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.17631\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003534355, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0675 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.17631\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003517139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0681 - val_loss: 0.1820\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.17631\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003500007, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0683 - val_loss: 0.1802\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.17631\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034829587, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0675 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.17631\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034659932, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0698 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.17631\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034491107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0687 - val_loss: 0.1840\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.17631\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034323102, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0688 - val_loss: 0.1803\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.17631\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034155912, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0678 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.17631\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033989537, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0656 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.17631\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033823974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0650 - val_loss: 0.1808\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.17631\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003365922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0656 - val_loss: 0.1779\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.17631\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033495267, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0655 - val_loss: 0.1792\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.17631\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003333211, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0652 - val_loss: 0.1821\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.17631\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033169755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0644 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.17631\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033008185, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0642 - val_loss: 0.1800\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.17631\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to tf.Tensor(0.000328474, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0634 - val_loss: 0.1790\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.17631\n",
      "Epoch 00229: early stopping\n",
      "--------------- > Fold 5 < ---------------\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 80, 54)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_16 (Bidirectional (None, 80, 2048)     8839168     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_16 (Attention)        (None, 80, 2048)     2128        bidirectional_16[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 80, 4096)     0           bidirectional_16[0][0]           \n",
      "                                                                 attention_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_17 (Bidirectional (None, 80, 1024)     18878464    concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_17 (Attention)        (None, 80, 1024)     1104        bidirectional_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 80, 2048)     0           bidirectional_17[0][0]           \n",
      "                                                                 attention_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_18 (Bidirectional (None, 80, 512)      4720640     concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_18 (Attention)        (None, 80, 512)      592         bidirectional_18[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 80, 1024)     0           bidirectional_18[0][0]           \n",
      "                                                                 attention_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_19 (Bidirectional (None, 80, 256)      1180672     concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_19 (Attention)        (None, 80, 256)      336         bidirectional_19[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 80, 512)      0           bidirectional_19[0][0]           \n",
      "                                                                 attention_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 80, 128)      65664       concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 80, 1)        129         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "scale_layer_4 (ScaleLayer)      (None, 80, 1)        0           dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 33,688,897\n",
      "Trainable params: 33,688,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(0.001, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 62s 500ms/step - loss: 4.1635 - val_loss: 1.3357\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.33568, saving model to folds4.hdf5\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to tf.Tensor(0.000995129, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 1.0841 - val_loss: 0.6884\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.33568 to 0.68836, saving model to folds4.hdf5\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009902818, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.7068 - val_loss: 0.6956\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.68836\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009854581, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5842 - val_loss: 0.5806\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.68836 to 0.58065, saving model to folds4.hdf5\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.000980658, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5231 - val_loss: 0.4729\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.58065 to 0.47291, saving model to folds4.hdf5\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097588124, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4786 - val_loss: 0.5040\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.47291\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009711277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4640 - val_loss: 0.4184\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.47291 to 0.41843, saving model to folds4.hdf5\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009663974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4157 - val_loss: 0.4278\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.41843\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009616901, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4063 - val_loss: 0.3807\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.41843 to 0.38066, saving model to folds4.hdf5\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009570057, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3996 - val_loss: 0.3828\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.38066\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095234415, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3711 - val_loss: 0.3571\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.38066 to 0.35711, saving model to folds4.hdf5\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009477053, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3582 - val_loss: 0.4134\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35711\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009430891, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3676 - val_loss: 0.3430\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.35711 to 0.34301, saving model to folds4.hdf5\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009384953, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3396 - val_loss: 0.3311\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.34301 to 0.33107, saving model to folds4.hdf5\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009339239, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 0.3266 - val_loss: 0.3208\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.33107 to 0.32080, saving model to folds4.hdf5\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009293747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3183 - val_loss: 0.3373\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.32080\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009248478, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3218 - val_loss: 0.3050\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.32080 to 0.30501, saving model to folds4.hdf5\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(0.00092034286, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3139 - val_loss: 0.2942\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.30501 to 0.29420, saving model to folds4.hdf5\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091585994, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2951 - val_loss: 0.3027\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.29420\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091139873, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2961 - val_loss: 0.2782\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.29420 to 0.27818, saving model to folds4.hdf5\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090695935, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2880 - val_loss: 0.3119\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.27818\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090254156, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2972 - val_loss: 0.2861\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.27818\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008981453, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2839 - val_loss: 0.2754\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.27818 to 0.27542, saving model to folds4.hdf5\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008937704, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2690 - val_loss: 0.2753\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.27542 to 0.27529, saving model to folds4.hdf5\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008894169, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2660 - val_loss: 0.2962\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.27529\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088508456, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2655 - val_loss: 0.3073\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.27529\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088077335, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2812 - val_loss: 0.2619\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.27529 to 0.26195, saving model to folds4.hdf5\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008764831, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2578 - val_loss: 0.2726\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.26195\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to tf.Tensor(0.00087221374, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2543 - val_loss: 0.2430\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.26195 to 0.24296, saving model to folds4.hdf5\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008679653, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2400 - val_loss: 0.2537\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.24296\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to tf.Tensor(0.00086373737, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2413 - val_loss: 0.2513\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.24296\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008595301, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2348 - val_loss: 0.2466\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.24296\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to tf.Tensor(0.00085534336, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2396 - val_loss: 0.2440\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.24296\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to tf.Tensor(0.000851177, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2327 - val_loss: 0.2578\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.24296\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to tf.Tensor(0.00084703095, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2336 - val_loss: 0.2563\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.24296\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008429051, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2371 - val_loss: 0.2417\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.24296 to 0.24167, saving model to folds4.hdf5\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to tf.Tensor(0.00083879934, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2447 - val_loss: 0.2388\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.24167 to 0.23878, saving model to folds4.hdf5\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008347136, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2250 - val_loss: 0.2396\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.23878\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008306477, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2243 - val_loss: 0.2260\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.23878 to 0.22595, saving model to folds4.hdf5\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008266016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2189 - val_loss: 0.2384\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.22595\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to tf.Tensor(0.00082257524, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2275 - val_loss: 0.2318\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.22595\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008185685, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2162 - val_loss: 0.2404\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.22595\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081458123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2222 - val_loss: 0.2354\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.22595\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081061345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2183 - val_loss: 0.2221\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.22595 to 0.22215, saving model to folds4.hdf5\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to tf.Tensor(0.000806665, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2097 - val_loss: 0.2256\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.22215\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to tf.Tensor(0.00080273574, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2149 - val_loss: 0.2126\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.22215 to 0.21263, saving model to folds4.hdf5\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007988256, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2073 - val_loss: 0.2287\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.21263\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007949346, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2059 - val_loss: 0.2096\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.21263 to 0.20958, saving model to folds4.hdf5\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007910624, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1998 - val_loss: 0.2172\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.20958\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007872092, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2018 - val_loss: 0.2273\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.20958\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007833747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2435 - val_loss: 0.2416\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.20958\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077955885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2193 - val_loss: 0.2315\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.20958\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077576167, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2091 - val_loss: 0.2114\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.20958\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077198294, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1990 - val_loss: 0.2209\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.20958\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007682226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2015 - val_loss: 0.2054\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.20958 to 0.20540, saving model to folds4.hdf5\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007644806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1920 - val_loss: 0.2088\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.20540\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007607569, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1943 - val_loss: 0.2174\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.20540\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007570512, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1941 - val_loss: 0.2065\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.20540\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to tf.Tensor(0.00075336365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1954 - val_loss: 0.2193\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.20540\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to tf.Tensor(0.000749694, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2014 - val_loss: 0.2167\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.20540\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007460423, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1902 - val_loss: 0.2068\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.20540\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to tf.Tensor(0.00074240833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 0.1918 - val_loss: 0.2043\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.20540 to 0.20432, saving model to folds4.hdf5\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007387921, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1863 - val_loss: 0.2233\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.20432\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to tf.Tensor(0.00073519343, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1884 - val_loss: 0.2001\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.20432 to 0.20013, saving model to folds4.hdf5\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007316123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1772 - val_loss: 0.1957\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.20013 to 0.19575, saving model to folds4.hdf5\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007280487, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1849 - val_loss: 0.2065\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.19575\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007245024, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1812 - val_loss: 0.1981\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.19575\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007209733, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1722 - val_loss: 0.1993\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.19575\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071746146, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1772 - val_loss: 0.1930\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.19575 to 0.19300, saving model to folds4.hdf5\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007139667, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1725 - val_loss: 0.2142\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.19300\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071048894, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1777 - val_loss: 0.2013\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.19300\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007070282, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1731 - val_loss: 0.2060\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.19300\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007035842, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1694 - val_loss: 0.1897\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.19300 to 0.18972, saving model to folds4.hdf5\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to tf.Tensor(0.00070015714, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1646 - val_loss: 0.1959\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.18972\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006967467, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1676 - val_loss: 0.1915\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.18972\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to tf.Tensor(0.00069335283, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1696 - val_loss: 0.1938\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.18972\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006899755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1652 - val_loss: 0.2005\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.18972\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068661466, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1771 - val_loss: 0.1916\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.18972\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068327016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1671 - val_loss: 0.2107\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.18972\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067994196, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1654 - val_loss: 0.1870\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.18972 to 0.18705, saving model to folds4.hdf5\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067663, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1605 - val_loss: 0.1957\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.18705\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006733342, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1676 - val_loss: 0.1911\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.18705\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067005435, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1649 - val_loss: 0.2000\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.18705\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006667905, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1693 - val_loss: 0.1857\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.18705 to 0.18570, saving model to folds4.hdf5\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006635426, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1575 - val_loss: 0.1879\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.18570\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006603105, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1593 - val_loss: 0.1891\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.18570\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006570942, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1539 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.18570 to 0.18275, saving model to folds4.hdf5\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065389345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1583 - val_loss: 0.1884\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.18275\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065070833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1551 - val_loss: 0.1900\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.18275\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to tf.Tensor(0.00064753875, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1710 - val_loss: 0.1939\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.18275\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006443846, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1581 - val_loss: 0.2021\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.18275\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006412458, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1668 - val_loss: 0.2054\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.18275\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063812226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1618 - val_loss: 0.1886\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.18275\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063501403, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1562 - val_loss: 0.1959\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.18275\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006319209, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1966 - val_loss: 0.2077\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.18275\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006288428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1607 - val_loss: 0.2023\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.18275\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to tf.Tensor(0.00062577974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1539 - val_loss: 0.1882\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.18275\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006227316, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1495 - val_loss: 0.1838\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.18275\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061969826, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1512 - val_loss: 0.2006\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.18275\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006166797, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1481 - val_loss: 0.1993\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.18275\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061367586, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1454 - val_loss: 0.1801\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.18275 to 0.18006, saving model to folds4.hdf5\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061068666, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1437 - val_loss: 0.1856\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.18006\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006077121, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1432 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.18006 to 0.17861, saving model to folds4.hdf5\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060475187, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1402 - val_loss: 0.1799\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.17861\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060180615, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 0.1385 - val_loss: 0.1808\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.17861\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059887476, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1375 - val_loss: 0.1784\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.17861 to 0.17838, saving model to folds4.hdf5\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005959577, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1353 - val_loss: 0.1840\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.17838\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059305475, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1381 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.17838\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to tf.Tensor(0.000590166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1372 - val_loss: 0.1873\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.17838\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to tf.Tensor(0.00058729126, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1343 - val_loss: 0.1824\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.17838\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005844306, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1381 - val_loss: 0.1849\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.17838\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005815838, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1395 - val_loss: 0.1904\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.17838\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to tf.Tensor(0.000578751, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1459 - val_loss: 0.1864\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.17838\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057593186, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1477 - val_loss: 0.1816\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.17838\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057312654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1391 - val_loss: 0.1956\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.17838\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057033484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1408 - val_loss: 0.1785\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.17838\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to tf.Tensor(0.00056755677, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1319 - val_loss: 0.1935\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.17838\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005647922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1420 - val_loss: 0.1821\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.17838\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005620411, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1334 - val_loss: 0.1806\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.17838\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055930344, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1340 - val_loss: 0.1808\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.17838\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055657903, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1311 - val_loss: 0.1883\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.17838\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055386795, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1312 - val_loss: 0.2069\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.17838\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005511701, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1508 - val_loss: 0.1833\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.17838\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005484854, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1400 - val_loss: 0.1939\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.17838\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005458137, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1485 - val_loss: 0.1945\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.17838\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to tf.Tensor(0.00054315507, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1477 - val_loss: 0.1922\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.17838\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005405093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1370 - val_loss: 0.1958\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.17838\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005378765, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1297 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.17838\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005352565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1298 - val_loss: 0.1759\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.17838 to 0.17586, saving model to folds4.hdf5\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005326493, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1259 - val_loss: 0.1834\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.17586\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005300548, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1274 - val_loss: 0.1762\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.17586\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005274729, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1238 - val_loss: 0.1768\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.17586\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052490365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1207 - val_loss: 0.1729\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.17586 to 0.17288, saving model to folds4.hdf5\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052234676, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1189 - val_loss: 0.1781\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.17288\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005198025, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1177 - val_loss: 0.1748\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.17288\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005172705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1201 - val_loss: 0.1757\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.17288\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051475083, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1156 - val_loss: 0.1738\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.17288\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051224354, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1187 - val_loss: 0.1735\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.17288\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005097484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1150 - val_loss: 0.1749\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.17288\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005072654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1155 - val_loss: 0.1782\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.17288\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005047946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1175 - val_loss: 0.1734\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.17288\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005023357, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1114 - val_loss: 0.1759\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.17288\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049988885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1114 - val_loss: 0.1802\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.17288\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004974539, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1130 - val_loss: 0.1746\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.17288\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004950308, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1114 - val_loss: 0.1853\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.17288\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004926195, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 200ms/step - loss: 0.1197 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.17288\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049022, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1132 - val_loss: 0.1769\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.17288\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048783212, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.1096 - val_loss: 0.1815\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.17288\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048545588, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1092 - val_loss: 0.1754\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.17288\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048309125, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1114 - val_loss: 0.1750\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.17288\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048073815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1077 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.17288\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047839645, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1079 - val_loss: 0.1733\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.17288\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004760662, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1067 - val_loss: 0.1735\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.17288\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004737473, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1073 - val_loss: 0.1723\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.17288 to 0.17232, saving model to folds4.hdf5\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047143968, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1046 - val_loss: 0.1832\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.17232\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004691433, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1087 - val_loss: 0.1711\n",
      "\n",
      "Epoch 00156: val_loss improved from 0.17232 to 0.17114, saving model to folds4.hdf5\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046685815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1025 - val_loss: 0.1801\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.17114\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046458407, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1047 - val_loss: 0.1746\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.17114\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046232107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1049 - val_loss: 0.1769\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.17114\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046006907, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1024 - val_loss: 0.1751\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.17114\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045782814, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1052 - val_loss: 0.1738\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.17114\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045559806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1002 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.17114\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045337883, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1046 - val_loss: 0.1728\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.17114\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045117046, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1002 - val_loss: 0.1740\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.17114\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044897277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1004 - val_loss: 0.1775\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.17114\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044678585, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0992 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.17114\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004446096, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1016 - val_loss: 0.1814\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.17114\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004424439, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0996 - val_loss: 0.1742\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.17114\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044028874, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0979 - val_loss: 0.1751\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.17114\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043814414, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0969 - val_loss: 0.1763\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.17114\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043600993, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0972 - val_loss: 0.1740\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.17114\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043388613, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0961 - val_loss: 0.1836\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.17114\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004317727, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0978 - val_loss: 0.1740\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.17114\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004296695, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0954 - val_loss: 0.1743\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.17114\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004275766, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0941 - val_loss: 0.1726\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.17114\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042549393, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0926 - val_loss: 0.1730\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.17114\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042342133, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0933 - val_loss: 0.1745\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.17114\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042135885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0920 - val_loss: 0.1729\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.17114\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041930642, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0925 - val_loss: 0.1819\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.17114\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041726397, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0925 - val_loss: 0.1776\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.17114\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041523148, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0922 - val_loss: 0.1737\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.17114\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041320888, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0920 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.17114\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041119618, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0912 - val_loss: 0.1729\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.17114\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040919325, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0892 - val_loss: 0.1757\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.17114\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040720004, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0883 - val_loss: 0.1730\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.17114\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004052166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0885 - val_loss: 0.1729\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.17114\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004032428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0884 - val_loss: 0.1812\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.17114\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004012786, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0893 - val_loss: 0.1722\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.17114\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039932402, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0872 - val_loss: 0.1721\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.17114\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039737887, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0878 - val_loss: 0.1744\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.17114\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039544326, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0867 - val_loss: 0.1731\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.17114\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039351705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0851 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.17114\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039160028, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0861 - val_loss: 0.1771\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.17114\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038969278, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0884 - val_loss: 0.1743\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.17114\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003877946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0849 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.17114\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038590565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0847 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.17114\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003840259, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0855 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.17114\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038215535, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0859 - val_loss: 0.1770\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.17114\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038029387, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0849 - val_loss: 0.1772\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.17114\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037844144, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0842 - val_loss: 0.1724\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.17114\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037659804, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0821 - val_loss: 0.1748\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.17114\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037476368, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0828 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.17114\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037293817, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0824 - val_loss: 0.1749\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.17114\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003711216, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0815 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.17114\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003693139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0815 - val_loss: 0.1719\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.17114\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to tf.Tensor(0.000367515, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0796 - val_loss: 0.1738\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.17114\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036572482, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0800 - val_loss: 0.1726\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.17114\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003639434, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0786 - val_loss: 0.1770\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.17114\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036217063, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0802 - val_loss: 0.1790\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.17114\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036040647, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0788 - val_loss: 0.1762\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.17114\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to tf.Tensor(0.00035865093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 0.0775 - val_loss: 0.1750\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.17114\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to tf.Tensor(0.000356904, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0769 - val_loss: 0.1749\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.17114\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003551655, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0776 - val_loss: 0.1744\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.17114\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003534355, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0773 - val_loss: 0.1740\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.17114\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003517139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0762 - val_loss: 0.1750\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.17114\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003500007, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0760 - val_loss: 0.1728\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.17114\n",
      "Epoch 00216: early stopping\n",
      "--------------- > Fold 6 < ---------------\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 80, 54)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_20 (Bidirectional (None, 80, 2048)     8839168     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_20 (Attention)        (None, 80, 2048)     2128        bidirectional_20[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 80, 4096)     0           bidirectional_20[0][0]           \n",
      "                                                                 attention_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_21 (Bidirectional (None, 80, 1024)     18878464    concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_21 (Attention)        (None, 80, 1024)     1104        bidirectional_21[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 80, 2048)     0           bidirectional_21[0][0]           \n",
      "                                                                 attention_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_22 (Bidirectional (None, 80, 512)      4720640     concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_22 (Attention)        (None, 80, 512)      592         bidirectional_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 80, 1024)     0           bidirectional_22[0][0]           \n",
      "                                                                 attention_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_23 (Bidirectional (None, 80, 256)      1180672     concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_23 (Attention)        (None, 80, 256)      336         bidirectional_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 80, 512)      0           bidirectional_23[0][0]           \n",
      "                                                                 attention_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 80, 128)      65664       concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 80, 1)        129         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "scale_layer_5 (ScaleLayer)      (None, 80, 1)        0           dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 33,688,897\n",
      "Trainable params: 33,688,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(0.001, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 62s 493ms/step - loss: 4.3051 - val_loss: 1.2291\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.22907, saving model to folds5.hdf5\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to tf.Tensor(0.000995129, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 1.0104 - val_loss: 0.6973\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.22907 to 0.69728, saving model to folds5.hdf5\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009902818, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.6947 - val_loss: 0.5826\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.69728 to 0.58264, saving model to folds5.hdf5\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009854581, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5623 - val_loss: 0.6533\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.58264\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.000980658, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5330 - val_loss: 0.4939\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.58264 to 0.49386, saving model to folds5.hdf5\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097588124, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.4723 - val_loss: 0.4740\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.49386 to 0.47404, saving model to folds5.hdf5\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009711277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4473 - val_loss: 0.4275\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.47404 to 0.42754, saving model to folds5.hdf5\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009663974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.4325 - val_loss: 0.4363\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.42754\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009616901, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4132 - val_loss: 0.4076\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.42754 to 0.40757, saving model to folds5.hdf5\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009570057, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3842 - val_loss: 0.3867\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.40757 to 0.38674, saving model to folds5.hdf5\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095234415, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3761 - val_loss: 0.3563\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.38674 to 0.35632, saving model to folds5.hdf5\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009477053, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.3588 - val_loss: 0.3698\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35632\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009430891, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3431 - val_loss: 0.3477\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.35632 to 0.34768, saving model to folds5.hdf5\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009384953, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3402 - val_loss: 0.3484\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.34768\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009339239, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3400 - val_loss: 0.3331\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.34768 to 0.33313, saving model to folds5.hdf5\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009293747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3204 - val_loss: 0.3144\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.33313 to 0.31440, saving model to folds5.hdf5\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009248478, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.3179 - val_loss: 0.3229\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31440\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(0.00092034286, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3075 - val_loss: 0.3088\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.31440 to 0.30879, saving model to folds5.hdf5\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091585994, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3142 - val_loss: 0.3055\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.30879 to 0.30555, saving model to folds5.hdf5\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091139873, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2961 - val_loss: 0.2873\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.30555 to 0.28726, saving model to folds5.hdf5\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090695935, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2867 - val_loss: 0.2820\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.28726 to 0.28203, saving model to folds5.hdf5\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090254156, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2786 - val_loss: 0.2806\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.28203 to 0.28061, saving model to folds5.hdf5\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008981453, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2811 - val_loss: 0.3037\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.28061\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008937704, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2690 - val_loss: 0.2751\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.28061 to 0.27506, saving model to folds5.hdf5\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008894169, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2741 - val_loss: 0.3331\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.27506\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088508456, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2875 - val_loss: 0.2774\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.27506\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088077335, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 196ms/step - loss: 0.2742 - val_loss: 0.2534\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.27506 to 0.25336, saving model to folds5.hdf5\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008764831, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2416 - val_loss: 0.2498\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.25336 to 0.24985, saving model to folds5.hdf5\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to tf.Tensor(0.00087221374, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2422 - val_loss: 0.2642\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.24985\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008679653, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2479 - val_loss: 0.2638\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.24985\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to tf.Tensor(0.00086373737, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2613 - val_loss: 0.2544\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.24985\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008595301, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2567 - val_loss: 0.2447\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.24985 to 0.24474, saving model to folds5.hdf5\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to tf.Tensor(0.00085534336, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2381 - val_loss: 0.2468\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.24474\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to tf.Tensor(0.000851177, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2353 - val_loss: 0.2370\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.24474 to 0.23698, saving model to folds5.hdf5\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to tf.Tensor(0.00084703095, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2341 - val_loss: 0.2413\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.23698\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008429051, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2281 - val_loss: 0.2441\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.23698\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to tf.Tensor(0.00083879934, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2312 - val_loss: 0.2439\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.23698\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008347136, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2245 - val_loss: 0.2441\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.23698\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008306477, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2291 - val_loss: 0.2283\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.23698 to 0.22832, saving model to folds5.hdf5\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008266016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2196 - val_loss: 0.2316\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.22832\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to tf.Tensor(0.00082257524, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2198 - val_loss: 0.2334\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.22832\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008185685, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2147 - val_loss: 0.2765\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.22832\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081458123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2393 - val_loss: 0.2474\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.22832\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081061345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2245 - val_loss: 0.2273\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.22832 to 0.22729, saving model to folds5.hdf5\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to tf.Tensor(0.000806665, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2127 - val_loss: 0.2176\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.22729 to 0.21760, saving model to folds5.hdf5\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to tf.Tensor(0.00080273574, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2066 - val_loss: 0.2262\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.21760\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007988256, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2167 - val_loss: 0.2252\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.21760\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007949346, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2035 - val_loss: 0.2128\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.21760 to 0.21282, saving model to folds5.hdf5\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007910624, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2076 - val_loss: 0.2305\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.21282\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007872092, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2068 - val_loss: 0.2190\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.21282\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007833747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1989 - val_loss: 0.2360\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.21282\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077955885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2076 - val_loss: 0.2128\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.21282 to 0.21276, saving model to folds5.hdf5\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077576167, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2032 - val_loss: 0.2076\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.21276 to 0.20765, saving model to folds5.hdf5\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077198294, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1992 - val_loss: 0.2260\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.20765\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007682226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1960 - val_loss: 0.2293\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.20765\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007644806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2023 - val_loss: 0.2107\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.20765\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007607569, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1900 - val_loss: 0.2092\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.20765\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007570512, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1932 - val_loss: 0.2081\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.20765\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to tf.Tensor(0.00075336365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1906 - val_loss: 0.2305\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.20765\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to tf.Tensor(0.000749694, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1896 - val_loss: 0.2232\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.20765\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007460423, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1939 - val_loss: 0.2355\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.20765\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to tf.Tensor(0.00074240833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2050 - val_loss: 0.2075\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.20765 to 0.20751, saving model to folds5.hdf5\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007387921, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1819 - val_loss: 0.2112\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.20751\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to tf.Tensor(0.00073519343, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1811 - val_loss: 0.1991\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.20751 to 0.19913, saving model to folds5.hdf5\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007316123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1789 - val_loss: 0.2064\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.19913\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007280487, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1798 - val_loss: 0.1998\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.19913\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007245024, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1795 - val_loss: 0.2071\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.19913\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007209733, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1830 - val_loss: 0.2117\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.19913\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071746146, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1817 - val_loss: 0.2124\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.19913\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007139667, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 14s 203ms/step - loss: 0.1775 - val_loss: 0.2068\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.19913\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071048894, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1756 - val_loss: 0.1983\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.19913 to 0.19828, saving model to folds5.hdf5\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007070282, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1706 - val_loss: 0.2006\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.19828\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007035842, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1750 - val_loss: 0.2070\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.19828\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to tf.Tensor(0.00070015714, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1787 - val_loss: 0.1943\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.19828 to 0.19426, saving model to folds5.hdf5\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006967467, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1682 - val_loss: 0.1983\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.19426\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to tf.Tensor(0.00069335283, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1712 - val_loss: 0.1966\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.19426\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006899755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1728 - val_loss: 0.2113\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.19426\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068661466, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1684 - val_loss: 0.2078\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.19426\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068327016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1651 - val_loss: 0.2072\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.19426\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067994196, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1628 - val_loss: 0.2021\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.19426\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067663, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1650 - val_loss: 0.1986\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.19426\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006733342, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1636 - val_loss: 0.2133\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.19426\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067005435, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1682 - val_loss: 0.1957\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.19426\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006667905, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1553 - val_loss: 0.2009\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.19426\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006635426, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1661 - val_loss: 0.2013\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.19426\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006603105, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1568 - val_loss: 0.1987\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.19426\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006570942, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1621 - val_loss: 0.1985\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.19426\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065389345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1645 - val_loss: 0.1946\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.19426\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065070833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1564 - val_loss: 0.1885\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.19426 to 0.18854, saving model to folds5.hdf5\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to tf.Tensor(0.00064753875, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1519 - val_loss: 0.1893\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.18854\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006443846, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1521 - val_loss: 0.1904\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.18854\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006412458, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1494 - val_loss: 0.1860\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.18854 to 0.18599, saving model to folds5.hdf5\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063812226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1473 - val_loss: 0.1866\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.18599\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063501403, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1444 - val_loss: 0.1869\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.18599\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006319209, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1463 - val_loss: 0.1892\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.18599\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006288428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1424 - val_loss: 0.1845\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.18599 to 0.18452, saving model to folds5.hdf5\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to tf.Tensor(0.00062577974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1416 - val_loss: 0.1924\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.18452\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006227316, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1458 - val_loss: 0.1846\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.18452\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061969826, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1379 - val_loss: 0.1816\n",
      "\n",
      "Epoch 00099: val_loss improved from 0.18452 to 0.18160, saving model to folds5.hdf5\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006166797, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1381 - val_loss: 0.1935\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.18160\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061367586, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1457 - val_loss: 0.1919\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.18160\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061068666, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1422 - val_loss: 0.1842\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.18160\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006077121, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1372 - val_loss: 0.1812\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.18160 to 0.18116, saving model to folds5.hdf5\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060475187, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1367 - val_loss: 0.1894\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.18116\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060180615, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1392 - val_loss: 0.1865\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.18116\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059887476, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1429 - val_loss: 0.1824\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.18116\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005959577, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1363 - val_loss: 0.1809\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.18116 to 0.18088, saving model to folds5.hdf5\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059305475, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1365 - val_loss: 0.1845\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.18088\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to tf.Tensor(0.000590166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1351 - val_loss: 0.1835\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.18088\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to tf.Tensor(0.00058729126, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1357 - val_loss: 0.1856\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.18088\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005844306, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1361 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.18088 to 0.17974, saving model to folds5.hdf5\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005815838, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1331 - val_loss: 0.1889\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.17974\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to tf.Tensor(0.000578751, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1322 - val_loss: 0.1860\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.17974\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057593186, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1297 - val_loss: 0.1795\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.17974 to 0.17946, saving model to folds5.hdf5\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057312654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1276 - val_loss: 0.1975\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.17946\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057033484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 199ms/step - loss: 0.1321 - val_loss: 0.1795\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.17946\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to tf.Tensor(0.00056755677, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1248 - val_loss: 0.1852\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.17946\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005647922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1265 - val_loss: 0.1798\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.17946\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005620411, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1239 - val_loss: 0.1789\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.17946 to 0.17894, saving model to folds5.hdf5\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055930344, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1250 - val_loss: 0.1803\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.17894\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055657903, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1251 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.17894 to 0.17869, saving model to folds5.hdf5\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055386795, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1227 - val_loss: 0.1810\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.17869\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005511701, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1256 - val_loss: 0.1888\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.17869\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005484854, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1235 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.17869\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005458137, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1220 - val_loss: 0.1813\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.17869\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to tf.Tensor(0.00054315507, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1215 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.17869\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005405093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1190 - val_loss: 0.1784\n",
      "\n",
      "Epoch 00127: val_loss improved from 0.17869 to 0.17844, saving model to folds5.hdf5\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005378765, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1172 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.17844 to 0.17828, saving model to folds5.hdf5\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005352565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1161 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.17828 to 0.17801, saving model to folds5.hdf5\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005326493, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.1165 - val_loss: 0.1803\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.17801\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005300548, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1181 - val_loss: 0.1852\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.17801\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005274729, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1205 - val_loss: 0.1878\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.17801\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052490365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1178 - val_loss: 0.1803\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.17801\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052234676, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1151 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.17801\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005198025, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1118 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.17801\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005172705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1123 - val_loss: 0.1854\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.17801\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051475083, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1152 - val_loss: 0.1795\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.17801\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051224354, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1108 - val_loss: 0.1814\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.17801\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005097484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1215 - val_loss: 0.1817\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.17801\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005072654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1093 - val_loss: 0.1810\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.17801\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005047946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1077 - val_loss: 0.1866\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.17801\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005023357, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1119 - val_loss: 0.1834\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.17801\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049988885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1070 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00143: val_loss improved from 0.17801 to 0.17652, saving model to folds5.hdf5\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004974539, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1056 - val_loss: 0.1834\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.17652\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004950308, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1086 - val_loss: 0.1747\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.17652 to 0.17473, saving model to folds5.hdf5\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004926195, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1069 - val_loss: 0.1842\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.17473\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049022, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1073 - val_loss: 0.1819\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.17473\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048783212, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1047 - val_loss: 0.1785\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.17473\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048545588, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1029 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.17473\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048309125, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1022 - val_loss: 0.1824\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.17473\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048073815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1052 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.17473\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047839645, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1001 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.17473\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004760662, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1006 - val_loss: 0.1764\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.17473\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004737473, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1005 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.17473\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047143968, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1003 - val_loss: 0.1785\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.17473\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004691433, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0975 - val_loss: 0.1779\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.17473\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046685815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0973 - val_loss: 0.1798\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.17473\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046458407, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0979 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.17473\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046232107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0991 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.17473\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046006907, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0966 - val_loss: 0.1812\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.17473\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045782814, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0959 - val_loss: 0.1760\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.17473\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045559806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0950 - val_loss: 0.1813\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.17473\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045337883, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0945 - val_loss: 0.1837\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.17473\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045117046, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0965 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.17473\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044897277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0940 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.17473\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044678585, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0929 - val_loss: 0.1751\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.17473\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004446096, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0910 - val_loss: 0.1799\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.17473\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004424439, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0910 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.17473\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044028874, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0914 - val_loss: 0.1766\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.17473\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043814414, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0904 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.17473\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043600993, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 198ms/step - loss: 0.0887 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.17473\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043388613, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0886 - val_loss: 0.1761\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.17473\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004317727, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0901 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.17473\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004296695, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0896 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.17473\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004275766, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0881 - val_loss: 0.1801\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.17473\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042549393, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0888 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.17473\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042342133, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1011 - val_loss: 0.1790\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.17473\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042135885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0924 - val_loss: 0.1810\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.17473\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041930642, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0907 - val_loss: 0.1772\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.17473\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041726397, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0889 - val_loss: 0.1782\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.17473\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041523148, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0886 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.17473\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041320888, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0870 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.17473\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041119618, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0876 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.17473\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040919325, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0843 - val_loss: 0.1758\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.17473\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040720004, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0847 - val_loss: 0.1805\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.17473\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004052166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0844 - val_loss: 0.1750\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.17473\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004032428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0821 - val_loss: 0.1752\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.17473\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004012786, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0825 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.17473\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039932402, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0839 - val_loss: 0.1761\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.17473\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039737887, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0804 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.17473\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039544326, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0835 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.17473\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039351705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0805 - val_loss: 0.1761\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.17473\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039160028, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0810 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.17473\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038969278, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0817 - val_loss: 0.1760\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.17473\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003877946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0789 - val_loss: 0.1762\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.17473\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038590565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0806 - val_loss: 0.1776\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.17473\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003840259, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0838 - val_loss: 0.1801\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.17473\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038215535, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0795 - val_loss: 0.1837\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.17473\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038029387, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0802 - val_loss: 0.1761\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.17473\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037844144, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0782 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.17473\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037659804, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0777 - val_loss: 0.1817\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.17473\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037476368, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0790 - val_loss: 0.1775\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.17473\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037293817, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0782 - val_loss: 0.1813\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.17473\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003711216, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0779 - val_loss: 0.1770\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.17473\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003693139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0767 - val_loss: 0.1822\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.17473\n",
      "Epoch 00205: early stopping\n",
      "--------------- > Fold 7 < ---------------\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 80, 54)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_24 (Bidirectional (None, 80, 2048)     8839168     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_24 (Attention)        (None, 80, 2048)     2128        bidirectional_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 80, 4096)     0           bidirectional_24[0][0]           \n",
      "                                                                 attention_24[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_25 (Bidirectional (None, 80, 1024)     18878464    concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_25 (Attention)        (None, 80, 1024)     1104        bidirectional_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 80, 2048)     0           bidirectional_25[0][0]           \n",
      "                                                                 attention_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_26 (Bidirectional (None, 80, 512)      4720640     concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_26 (Attention)        (None, 80, 512)      592         bidirectional_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 80, 1024)     0           bidirectional_26[0][0]           \n",
      "                                                                 attention_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_27 (Bidirectional (None, 80, 256)      1180672     concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_27 (Attention)        (None, 80, 256)      336         bidirectional_27[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 80, 512)      0           bidirectional_27[0][0]           \n",
      "                                                                 attention_27[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 80, 128)      65664       concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 80, 1)        129         dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "scale_layer_6 (ScaleLayer)      (None, 80, 1)        0           dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 33,688,897\n",
      "Trainable params: 33,688,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(0.001, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 62s 498ms/step - loss: 4.1385 - val_loss: 1.1079\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.10794, saving model to folds6.hdf5\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to tf.Tensor(0.000995129, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 1.0219 - val_loss: 0.9362\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.10794 to 0.93618, saving model to folds6.hdf5\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009902818, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.7440 - val_loss: 0.6259\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.93618 to 0.62594, saving model to folds6.hdf5\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009854581, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5769 - val_loss: 0.4868\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.62594 to 0.48683, saving model to folds6.hdf5\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.000980658, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5001 - val_loss: 0.4660\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48683 to 0.46598, saving model to folds6.hdf5\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097588124, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.4733 - val_loss: 0.4930\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.46598\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009711277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4637 - val_loss: 0.4495\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.46598 to 0.44951, saving model to folds6.hdf5\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009663974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4233 - val_loss: 0.4631\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.44951\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009616901, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4164 - val_loss: 0.3822\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.44951 to 0.38216, saving model to folds6.hdf5\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009570057, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3819 - val_loss: 0.3867\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.38216\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095234415, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3783 - val_loss: 0.4124\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.38216\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009477053, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3645 - val_loss: 0.3760\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.38216 to 0.37597, saving model to folds6.hdf5\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009430891, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3523 - val_loss: 0.3448\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.37597 to 0.34483, saving model to folds6.hdf5\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009384953, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3334 - val_loss: 0.3574\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.34483\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009339239, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3402 - val_loss: 0.3176\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.34483 to 0.31755, saving model to folds6.hdf5\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009293747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3319 - val_loss: 0.3469\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.31755\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009248478, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3262 - val_loss: 0.3805\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31755\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(0.00092034286, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.3205 - val_loss: 0.3058\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.31755 to 0.30582, saving model to folds6.hdf5\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091585994, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2972 - val_loss: 0.2933\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.30582 to 0.29328, saving model to folds6.hdf5\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091139873, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2934 - val_loss: 0.2830\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.29328 to 0.28301, saving model to folds6.hdf5\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090695935, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2802 - val_loss: 0.2873\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.28301\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090254156, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2883 - val_loss: 0.2943\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.28301\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008981453, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.2776 - val_loss: 0.2856\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.28301\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008937704, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2657 - val_loss: 0.3176\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.28301\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008894169, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2732 - val_loss: 0.2700\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.28301 to 0.26997, saving model to folds6.hdf5\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088508456, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2596 - val_loss: 0.2706\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.26997\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088077335, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2535 - val_loss: 0.3181\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.26997\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008764831, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2512 - val_loss: 0.2746\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.26997\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to tf.Tensor(0.00087221374, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2578 - val_loss: 0.2475\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.26997 to 0.24747, saving model to folds6.hdf5\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008679653, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2404 - val_loss: 0.2713\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.24747\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to tf.Tensor(0.00086373737, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2475 - val_loss: 0.2585\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.24747\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008595301, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2546 - val_loss: 0.2862\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.24747\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to tf.Tensor(0.00085534336, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2439 - val_loss: 0.2504\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.24747\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to tf.Tensor(0.000851177, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2342 - val_loss: 0.2416\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.24747 to 0.24158, saving model to folds6.hdf5\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to tf.Tensor(0.00084703095, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2405 - val_loss: 0.2453\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.24158\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008429051, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2323 - val_loss: 0.2473\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.24158\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to tf.Tensor(0.00083879934, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2268 - val_loss: 0.2318\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.24158 to 0.23181, saving model to folds6.hdf5\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008347136, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2210 - val_loss: 0.2327\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.23181\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008306477, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2201 - val_loss: 0.2340\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.23181\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008266016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2206 - val_loss: 0.2363\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.23181\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to tf.Tensor(0.00082257524, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2148 - val_loss: 0.2306\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.23181 to 0.23062, saving model to folds6.hdf5\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008185685, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2151 - val_loss: 0.2498\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.23062\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081458123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2267 - val_loss: 0.2344\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.23062\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081061345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2164 - val_loss: 0.2378\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.23062\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to tf.Tensor(0.000806665, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2151 - val_loss: 0.2394\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.23062\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to tf.Tensor(0.00080273574, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2173 - val_loss: 0.2251\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.23062 to 0.22510, saving model to folds6.hdf5\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007988256, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2138 - val_loss: 0.2239\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.22510 to 0.22387, saving model to folds6.hdf5\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007949346, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2086 - val_loss: 0.2319\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.22387\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007910624, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2103 - val_loss: 0.2364\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.22387\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007872092, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2107 - val_loss: 0.2441\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.22387\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007833747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2084 - val_loss: 0.2413\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.22387\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077955885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2094 - val_loss: 0.2213\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.22387 to 0.22135, saving model to folds6.hdf5\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077576167, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1995 - val_loss: 0.2321\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.22135\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077198294, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 14s 203ms/step - loss: 0.1980 - val_loss: 0.2122\n",
      "\n",
      "Epoch 00054: val_loss improved from 0.22135 to 0.21223, saving model to folds6.hdf5\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007682226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1959 - val_loss: 0.2294\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.21223\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007644806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1932 - val_loss: 0.2103\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.21223 to 0.21032, saving model to folds6.hdf5\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007607569, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1918 - val_loss: 0.2091\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.21032 to 0.20914, saving model to folds6.hdf5\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007570512, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1934 - val_loss: 0.2131\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.20914\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to tf.Tensor(0.00075336365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1928 - val_loss: 0.2131\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.20914\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to tf.Tensor(0.000749694, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1974 - val_loss: 0.2231\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.20914\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007460423, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1898 - val_loss: 0.2143\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.20914\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to tf.Tensor(0.00074240833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1851 - val_loss: 0.2150\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.20914\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007387921, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1844 - val_loss: 0.2136\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.20914\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to tf.Tensor(0.00073519343, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1789 - val_loss: 0.2046\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.20914 to 0.20461, saving model to folds6.hdf5\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007316123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1795 - val_loss: 0.2000\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.20461 to 0.20001, saving model to folds6.hdf5\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007280487, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1788 - val_loss: 0.2429\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.20001\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007245024, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1888 - val_loss: 0.2009\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.20001\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007209733, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1733 - val_loss: 0.1988\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.20001 to 0.19885, saving model to folds6.hdf5\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071746146, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1732 - val_loss: 0.2177\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.19885\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007139667, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1791 - val_loss: 0.1988\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.19885 to 0.19884, saving model to folds6.hdf5\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071048894, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1782 - val_loss: 0.2051\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.19884\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007070282, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1712 - val_loss: 0.2054\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.19884\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007035842, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1687 - val_loss: 0.2032\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.19884\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to tf.Tensor(0.00070015714, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1704 - val_loss: 0.2048\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.19884\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006967467, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1724 - val_loss: 0.2085\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.19884\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to tf.Tensor(0.00069335283, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1783 - val_loss: 0.2110\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.19884\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006899755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1732 - val_loss: 0.2009\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.19884\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068661466, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1677 - val_loss: 0.1956\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.19884 to 0.19559, saving model to folds6.hdf5\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068327016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1658 - val_loss: 0.2064\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.19559\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067994196, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1646 - val_loss: 0.1904\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.19559 to 0.19037, saving model to folds6.hdf5\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067663, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1697 - val_loss: 0.1949\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.19037\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006733342, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1677 - val_loss: 0.2080\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.19037\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067005435, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1720 - val_loss: 0.2084\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.19037\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006667905, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1738 - val_loss: 0.2254\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.19037\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006635426, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1757 - val_loss: 0.1896\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.19037 to 0.18958, saving model to folds6.hdf5\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006603105, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1594 - val_loss: 0.1972\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.18958\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006570942, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1616 - val_loss: 0.1925\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.18958\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065389345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1599 - val_loss: 0.2110\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.18958\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065070833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1635 - val_loss: 0.2065\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.18958\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to tf.Tensor(0.00064753875, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1704 - val_loss: 0.1992\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.18958\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006443846, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1591 - val_loss: 0.1970\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.18958\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006412458, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1569 - val_loss: 0.1969\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.18958\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063812226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1541 - val_loss: 0.1846\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.18958 to 0.18456, saving model to folds6.hdf5\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063501403, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1504 - val_loss: 0.1869\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.18456\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006319209, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1557 - val_loss: 0.1886\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.18456\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006288428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1526 - val_loss: 0.1941\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.18456\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to tf.Tensor(0.00062577974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1504 - val_loss: 0.1942\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.18456\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006227316, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1553 - val_loss: 0.1843\n",
      "\n",
      "Epoch 00098: val_loss improved from 0.18456 to 0.18425, saving model to folds6.hdf5\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061969826, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1475 - val_loss: 0.1876\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.18425\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006166797, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 14s 202ms/step - loss: 0.1438 - val_loss: 0.1840\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.18425 to 0.18405, saving model to folds6.hdf5\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061367586, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1487 - val_loss: 0.1834\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.18405 to 0.18344, saving model to folds6.hdf5\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061068666, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1505 - val_loss: 0.1995\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.18344\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006077121, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1472 - val_loss: 0.1875\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.18344\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060475187, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1472 - val_loss: 0.1926\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.18344\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060180615, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1457 - val_loss: 0.1842\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.18344\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059887476, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1409 - val_loss: 0.1817\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.18344 to 0.18174, saving model to folds6.hdf5\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005959577, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.1389 - val_loss: 0.3020\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.18174\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059305475, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.2061 - val_loss: 0.2040\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.18174\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to tf.Tensor(0.000590166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1549 - val_loss: 0.1866\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.18174\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to tf.Tensor(0.00058729126, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1427 - val_loss: 0.1876\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.18174\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005844306, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1476 - val_loss: 0.1980\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.18174\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005815838, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1431 - val_loss: 0.1877\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.18174\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to tf.Tensor(0.000578751, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1404 - val_loss: 0.1861\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.18174\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057593186, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1386 - val_loss: 0.1868\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.18174\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057312654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1417 - val_loss: 0.1938\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.18174\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057033484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1546 - val_loss: 0.1918\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.18174\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to tf.Tensor(0.00056755677, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1436 - val_loss: 0.1854\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.18174\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005647922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1451 - val_loss: 0.1934\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.18174\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005620411, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1418 - val_loss: 0.1902\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.18174\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055930344, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1355 - val_loss: 0.1936\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.18174\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055657903, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1396 - val_loss: 0.1813\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.18174 to 0.18129, saving model to folds6.hdf5\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055386795, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1340 - val_loss: 0.1970\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.18129\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005511701, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1373 - val_loss: 0.1872\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.18129\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005484854, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1448 - val_loss: 0.1921\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.18129\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005458137, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1333 - val_loss: 0.1847\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.18129\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to tf.Tensor(0.00054315507, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1266 - val_loss: 0.1848\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.18129\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005405093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1247 - val_loss: 0.1830\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.18129\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005378765, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1239 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.18129 to 0.17973, saving model to folds6.hdf5\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005352565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1228 - val_loss: 0.1847\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.17973\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005326493, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1220 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.17973\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005300548, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1218 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00131: val_loss improved from 0.17973 to 0.17774, saving model to folds6.hdf5\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005274729, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1207 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.17774\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052490365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1219 - val_loss: 0.1824\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.17774\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052234676, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1189 - val_loss: 0.1866\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.17774\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005198025, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1232 - val_loss: 0.1770\n",
      "\n",
      "Epoch 00135: val_loss improved from 0.17774 to 0.17699, saving model to folds6.hdf5\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005172705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1160 - val_loss: 0.1839\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.17699\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051475083, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1166 - val_loss: 0.1835\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.17699\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051224354, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1154 - val_loss: 0.1813\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.17699\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005097484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1151 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.17699\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005072654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1147 - val_loss: 0.1784\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.17699\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005047946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1148 - val_loss: 0.1754\n",
      "\n",
      "Epoch 00141: val_loss improved from 0.17699 to 0.17537, saving model to folds6.hdf5\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005023357, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1120 - val_loss: 0.1840\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.17537\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049988885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1121 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.17537\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004974539, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1111 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.17537\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004950308, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1140 - val_loss: 0.1821\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.17537\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004926195, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 198ms/step - loss: 0.1136 - val_loss: 0.1782\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.17537\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049022, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1140 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.17537\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048783212, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1077 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.17537\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048545588, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1076 - val_loss: 0.1809\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.17537\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048309125, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1071 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.17537\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048073815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1061 - val_loss: 0.1761\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.17537\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047839645, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1054 - val_loss: 0.1796\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.17537\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004760662, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1064 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.17537\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004737473, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1049 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.17537\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047143968, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1084 - val_loss: 0.1916\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.17537\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004691433, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1132 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.17537\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046685815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1072 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.17537\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046458407, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1034 - val_loss: 0.1800\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.17537\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046232107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1044 - val_loss: 0.1834\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.17537\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046006907, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1059 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.17537\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045782814, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1027 - val_loss: 0.1782\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.17537\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045559806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1025 - val_loss: 0.1824\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.17537\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045337883, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1004 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.17537\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045117046, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0999 - val_loss: 0.1796\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.17537\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044897277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 14s 202ms/step - loss: 0.0995 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.17537\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044678585, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1012 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.17537\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004446096, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0967 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.17537\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004424439, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0950 - val_loss: 0.1762\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.17537\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044028874, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0973 - val_loss: 0.1830\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.17537\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043814414, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1024 - val_loss: 0.1779\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.17537\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043600993, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0960 - val_loss: 0.1790\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.17537\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043388613, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0936 - val_loss: 0.1836\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.17537\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004317727, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0939 - val_loss: 0.1754\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.17537\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004296695, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0952 - val_loss: 0.1768\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.17537\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004275766, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0916 - val_loss: 0.1771\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.17537\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042549393, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0912 - val_loss: 0.1754\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.17537\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042342133, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0904 - val_loss: 0.1860\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.17537\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042135885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0924 - val_loss: 0.1751\n",
      "\n",
      "Epoch 00178: val_loss improved from 0.17537 to 0.17512, saving model to folds6.hdf5\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041930642, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0907 - val_loss: 0.1897\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.17512\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041726397, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0929 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.17512\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041523148, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0899 - val_loss: 0.1757\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.17512\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041320888, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0894 - val_loss: 0.1779\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.17512\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041119618, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0900 - val_loss: 0.1764\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.17512\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040919325, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0895 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.17512\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040720004, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0868 - val_loss: 0.1812\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.17512\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004052166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0885 - val_loss: 0.1757\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.17512\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004032428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0860 - val_loss: 0.1776\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.17512\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004012786, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0862 - val_loss: 0.1768\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.17512\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039932402, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0863 - val_loss: 0.1760\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.17512\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039737887, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0848 - val_loss: 0.1784\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.17512\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039544326, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0849 - val_loss: 0.1764\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.17512\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039351705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0833 - val_loss: 0.1761\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.17512\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039160028, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0831 - val_loss: 0.1800\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.17512\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038969278, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0832 - val_loss: 0.1759\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.17512\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003877946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0847 - val_loss: 0.1796\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.17512\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038590565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0825 - val_loss: 0.1795\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.17512\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003840259, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0819 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.17512\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038215535, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 0.0820 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.17512\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038029387, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0814 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.17512\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037844144, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0799 - val_loss: 0.1818\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.17512\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037659804, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0828 - val_loss: 0.1864\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.17512\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037476368, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0865 - val_loss: 0.1784\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.17512\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037293817, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0784 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.17512\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003711216, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0810 - val_loss: 0.1781\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.17512\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003693139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0793 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.17512\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to tf.Tensor(0.000367515, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0800 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.17512\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036572482, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0784 - val_loss: 0.1766\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.17512\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003639434, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0774 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.17512\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036217063, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0776 - val_loss: 0.1792\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.17512\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036040647, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0752 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.17512\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to tf.Tensor(0.00035865093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0757 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.17512\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to tf.Tensor(0.000356904, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0756 - val_loss: 0.1837\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.17512\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003551655, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0764 - val_loss: 0.1799\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.17512\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003534355, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0763 - val_loss: 0.1784\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.17512\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003517139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0747 - val_loss: 0.1775\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.17512\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003500007, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0755 - val_loss: 0.1776\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.17512\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034829587, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.0735 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.17512\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034659932, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0745 - val_loss: 0.1816\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.17512\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034491107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0731 - val_loss: 0.1779\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.17512\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034323102, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.0715 - val_loss: 0.1770\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.17512\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034155912, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0728 - val_loss: 0.1776\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.17512\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033989537, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0714 - val_loss: 0.1766\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.17512\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033823974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0712 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.17512\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003365922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0704 - val_loss: 0.1767\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.17512\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033495267, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0704 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.17512\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003333211, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0708 - val_loss: 0.1761\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.17512\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033169755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0703 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.17512\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033008185, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0723 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.17512\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to tf.Tensor(0.000328474, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0696 - val_loss: 0.1767\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.17512\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to tf.Tensor(0.00032687403, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0705 - val_loss: 0.1761\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.17512\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003252818, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0686 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.17512\n",
      "Epoch 232/300\n",
      "\n",
      "Epoch 00232: LearningRateScheduler reducing learning rate to tf.Tensor(0.00032369737, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0699 - val_loss: 0.1768\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.17512\n",
      "Epoch 233/300\n",
      "\n",
      "Epoch 00233: LearningRateScheduler reducing learning rate to tf.Tensor(0.00032212064, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0677 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.17512\n",
      "Epoch 234/300\n",
      "\n",
      "Epoch 00234: LearningRateScheduler reducing learning rate to tf.Tensor(0.00032055163, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0683 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.17512\n",
      "Epoch 235/300\n",
      "\n",
      "Epoch 00235: LearningRateScheduler reducing learning rate to tf.Tensor(0.00031899018, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0686 - val_loss: 0.1782\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.17512\n",
      "Epoch 236/300\n",
      "\n",
      "Epoch 00236: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003174364, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0664 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.17512\n",
      "Epoch 237/300\n",
      "\n",
      "Epoch 00237: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003158902, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0673 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.17512\n",
      "Epoch 238/300\n",
      "\n",
      "Epoch 00238: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003143515, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0661 - val_loss: 0.1812\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.17512\n",
      "Epoch 00238: early stopping\n",
      "--------------- > Fold 8 < ---------------\n",
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 80, 54)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_28 (Bidirectional (None, 80, 2048)     8839168     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_28 (Attention)        (None, 80, 2048)     2128        bidirectional_28[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 80, 4096)     0           bidirectional_28[0][0]           \n",
      "                                                                 attention_28[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_29 (Bidirectional (None, 80, 1024)     18878464    concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_29 (Attention)        (None, 80, 1024)     1104        bidirectional_29[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 80, 2048)     0           bidirectional_29[0][0]           \n",
      "                                                                 attention_29[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_30 (Bidirectional (None, 80, 512)      4720640     concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_30 (Attention)        (None, 80, 512)      592         bidirectional_30[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 80, 1024)     0           bidirectional_30[0][0]           \n",
      "                                                                 attention_30[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_31 (Bidirectional (None, 80, 256)      1180672     concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_31 (Attention)        (None, 80, 256)      336         bidirectional_31[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 80, 512)      0           bidirectional_31[0][0]           \n",
      "                                                                 attention_31[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 80, 128)      65664       concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 80, 1)        129         dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "scale_layer_7 (ScaleLayer)      (None, 80, 1)        0           dense_15[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 33,688,897\n",
      "Trainable params: 33,688,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(0.001, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 63s 513ms/step - loss: 4.1956 - val_loss: 1.2105\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.21047, saving model to folds7.hdf5\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to tf.Tensor(0.000995129, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.9971 - val_loss: 0.6737\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.21047 to 0.67370, saving model to folds7.hdf5\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009902818, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.6846 - val_loss: 0.6606\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.67370 to 0.66059, saving model to folds7.hdf5\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009854581, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5622 - val_loss: 0.6470\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.66059 to 0.64703, saving model to folds7.hdf5\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.000980658, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5804 - val_loss: 0.4540\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.64703 to 0.45395, saving model to folds7.hdf5\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097588124, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4717 - val_loss: 0.4720\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.45395\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009711277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4411 - val_loss: 0.4207\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.45395 to 0.42070, saving model to folds7.hdf5\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009663974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4095 - val_loss: 0.3870\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.42070 to 0.38698, saving model to folds7.hdf5\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009616901, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3943 - val_loss: 0.3910\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.38698\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009570057, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3949 - val_loss: 0.3770\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.38698 to 0.37700, saving model to folds7.hdf5\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095234415, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3790 - val_loss: 0.3880\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.37700\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009477053, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3474 - val_loss: 0.3357\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.37700 to 0.33566, saving model to folds7.hdf5\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009430891, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3461 - val_loss: 0.3315\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.33566 to 0.33153, saving model to folds7.hdf5\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009384953, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3380 - val_loss: 0.3401\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.33153\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009339239, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3206 - val_loss: 0.3124\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.33153 to 0.31238, saving model to folds7.hdf5\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009293747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3100 - val_loss: 0.3158\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.31238\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009248478, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3099 - val_loss: 0.3394\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31238\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(0.00092034286, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3106 - val_loss: 0.2976\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.31238 to 0.29762, saving model to folds7.hdf5\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091585994, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2896 - val_loss: 0.2987\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.29762\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091139873, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2872 - val_loss: 0.2905\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.29762 to 0.29045, saving model to folds7.hdf5\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090695935, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2876 - val_loss: 0.2778\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.29045 to 0.27783, saving model to folds7.hdf5\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090254156, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2771 - val_loss: 0.2957\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.27783\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008981453, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2839 - val_loss: 0.3111\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.27783\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008937704, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2683 - val_loss: 0.3091\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.27783\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008894169, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2708 - val_loss: 0.2554\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.27783 to 0.25539, saving model to folds7.hdf5\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088508456, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2482 - val_loss: 0.2564\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.25539\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088077335, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2503 - val_loss: 0.2560\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.25539\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008764831, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.2559 - val_loss: 0.2531\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.25539 to 0.25309, saving model to folds7.hdf5\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to tf.Tensor(0.00087221374, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2467 - val_loss: 0.2465\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.25309 to 0.24654, saving model to folds7.hdf5\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008679653, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2347 - val_loss: 0.2484\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.24654\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to tf.Tensor(0.00086373737, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2328 - val_loss: 0.2619\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.24654\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008595301, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2453 - val_loss: 0.2465\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.24654\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to tf.Tensor(0.00085534336, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2326 - val_loss: 0.2559\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.24654\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to tf.Tensor(0.000851177, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2304 - val_loss: 0.2563\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.24654\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to tf.Tensor(0.00084703095, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2344 - val_loss: 0.2470\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.24654\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008429051, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2217 - val_loss: 0.2400\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.24654 to 0.24000, saving model to folds7.hdf5\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to tf.Tensor(0.00083879934, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2154 - val_loss: 0.2323\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.24000 to 0.23229, saving model to folds7.hdf5\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008347136, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2213 - val_loss: 0.2260\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.23229 to 0.22603, saving model to folds7.hdf5\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008306477, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2112 - val_loss: 0.2249\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.22603 to 0.22492, saving model to folds7.hdf5\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008266016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2170 - val_loss: 0.2447\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.22492\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to tf.Tensor(0.00082257524, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2092 - val_loss: 0.2255\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.22492\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008185685, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2107 - val_loss: 0.2789\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.22492\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081458123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2224 - val_loss: 0.2202\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.22492 to 0.22024, saving model to folds7.hdf5\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081061345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2065 - val_loss: 0.2424\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.22024\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to tf.Tensor(0.000806665, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2137 - val_loss: 0.2390\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.22024\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to tf.Tensor(0.00080273574, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2166 - val_loss: 0.2181\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.22024 to 0.21812, saving model to folds7.hdf5\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007988256, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1957 - val_loss: 0.2318\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.21812\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007949346, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1990 - val_loss: 0.2238\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.21812\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007910624, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1919 - val_loss: 0.2131\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.21812 to 0.21309, saving model to folds7.hdf5\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007872092, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1943 - val_loss: 0.2077\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.21309 to 0.20768, saving model to folds7.hdf5\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007833747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1857 - val_loss: 0.2297\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.20768\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077955885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 200ms/step - loss: 0.2177 - val_loss: 0.2402\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.20768\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077576167, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2180 - val_loss: 0.2195\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.20768\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077198294, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1989 - val_loss: 0.2343\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.20768\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007682226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2101 - val_loss: 0.2129\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.20768\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007644806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1897 - val_loss: 0.2337\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.20768\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007607569, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1986 - val_loss: 0.2128\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.20768\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007570512, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1856 - val_loss: 0.2264\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.20768\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to tf.Tensor(0.00075336365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1913 - val_loss: 0.2129\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.20768\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to tf.Tensor(0.000749694, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1912 - val_loss: 0.2088\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.20768\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007460423, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1796 - val_loss: 0.1967\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.20768 to 0.19668, saving model to folds7.hdf5\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to tf.Tensor(0.00074240833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1744 - val_loss: 0.2020\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.19668\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007387921, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1754 - val_loss: 0.2049\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.19668\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to tf.Tensor(0.00073519343, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1712 - val_loss: 0.2011\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.19668\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007316123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1728 - val_loss: 0.1985\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.19668\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007280487, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1792 - val_loss: 0.2003\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.19668\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007245024, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1696 - val_loss: 0.2011\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.19668\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007209733, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1742 - val_loss: 0.2071\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.19668\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071746146, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1751 - val_loss: 0.2031\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.19668\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007139667, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1742 - val_loss: 0.2017\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.19668\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071048894, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1725 - val_loss: 0.1967\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.19668 to 0.19666, saving model to folds7.hdf5\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007070282, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1662 - val_loss: 0.2115\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.19666\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007035842, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1703 - val_loss: 0.1899\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.19666 to 0.18993, saving model to folds7.hdf5\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to tf.Tensor(0.00070015714, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1613 - val_loss: 0.2160\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.18993\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006967467, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1656 - val_loss: 0.1930\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.18993\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to tf.Tensor(0.00069335283, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1580 - val_loss: 0.1948\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.18993\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006899755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1652 - val_loss: 0.1901\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.18993\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068661466, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1596 - val_loss: 0.2007\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.18993\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068327016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1610 - val_loss: 0.2102\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.18993\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067994196, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1636 - val_loss: 0.1897\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.18993 to 0.18966, saving model to folds7.hdf5\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067663, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1559 - val_loss: 0.1878\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.18966 to 0.18779, saving model to folds7.hdf5\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006733342, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1570 - val_loss: 0.1979\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.18779\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067005435, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1567 - val_loss: 0.1920\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.18779\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006667905, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1614 - val_loss: 0.1946\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.18779\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006635426, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1552 - val_loss: 0.1875\n",
      "\n",
      "Epoch 00085: val_loss improved from 0.18779 to 0.18748, saving model to folds7.hdf5\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006603105, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1489 - val_loss: 0.1879\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.18748\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006570942, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1511 - val_loss: 0.1846\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.18748 to 0.18462, saving model to folds7.hdf5\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065389345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1472 - val_loss: 0.1932\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.18462\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065070833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1467 - val_loss: 0.1894\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.18462\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to tf.Tensor(0.00064753875, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1459 - val_loss: 0.1899\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.18462\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006443846, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1465 - val_loss: 0.1834\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.18462 to 0.18338, saving model to folds7.hdf5\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006412458, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1400 - val_loss: 0.1924\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.18338\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063812226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1433 - val_loss: 0.1804\n",
      "\n",
      "Epoch 00093: val_loss improved from 0.18338 to 0.18039, saving model to folds7.hdf5\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063501403, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1401 - val_loss: 0.1821\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.18039\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006319209, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1418 - val_loss: 0.1982\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.18039\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006288428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1442 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.18039 to 0.17938, saving model to folds7.hdf5\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to tf.Tensor(0.00062577974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1362 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.17938 to 0.17859, saving model to folds7.hdf5\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006227316, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1349 - val_loss: 0.1917\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.17859\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061969826, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1408 - val_loss: 0.1903\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.17859\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006166797, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1380 - val_loss: 0.1822\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.17859\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061367586, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1361 - val_loss: 0.1984\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.17859\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061068666, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 198ms/step - loss: 0.1447 - val_loss: 0.1894\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.17859\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006077121, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1399 - val_loss: 0.1905\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.17859\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060475187, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1334 - val_loss: 0.1816\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.17859\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060180615, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1322 - val_loss: 0.1930\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.17859\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059887476, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1324 - val_loss: 0.1813\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.17859\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005959577, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1320 - val_loss: 0.1781\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.17859 to 0.17815, saving model to folds7.hdf5\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059305475, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1282 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.17815\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to tf.Tensor(0.000590166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1262 - val_loss: 0.1814\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.17815\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to tf.Tensor(0.00058729126, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1254 - val_loss: 0.1844\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.17815\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005844306, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1235 - val_loss: 0.1781\n",
      "\n",
      "Epoch 00111: val_loss improved from 0.17815 to 0.17815, saving model to folds7.hdf5\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005815838, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1251 - val_loss: 0.1807\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.17815\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to tf.Tensor(0.000578751, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1248 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00113: val_loss improved from 0.17815 to 0.17730, saving model to folds7.hdf5\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057593186, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1249 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.17730\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057312654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1234 - val_loss: 0.1794\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.17730\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057033484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1210 - val_loss: 0.1795\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.17730\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to tf.Tensor(0.00056755677, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1203 - val_loss: 0.1789\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.17730\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005647922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1207 - val_loss: 0.1760\n",
      "\n",
      "Epoch 00118: val_loss improved from 0.17730 to 0.17598, saving model to folds7.hdf5\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005620411, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1175 - val_loss: 0.1856\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.17598\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055930344, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1214 - val_loss: 0.1900\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.17598\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055657903, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1203 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.17598\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055386795, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1184 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.17598\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005511701, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1146 - val_loss: 0.1797\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.17598\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005484854, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1137 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.17598\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005458137, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1197 - val_loss: 0.1816\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.17598\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to tf.Tensor(0.00054315507, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1150 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.17598\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005405093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1139 - val_loss: 0.1792\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.17598\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005378765, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1129 - val_loss: 0.1844\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.17598\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005352565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1172 - val_loss: 0.1789\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.17598\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005326493, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1116 - val_loss: 0.1898\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.17598\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005300548, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1126 - val_loss: 0.1919\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.17598\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005274729, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1145 - val_loss: 0.1809\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.17598\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052490365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1135 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.17598\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052234676, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1114 - val_loss: 0.1752\n",
      "\n",
      "Epoch 00134: val_loss improved from 0.17598 to 0.17521, saving model to folds7.hdf5\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005198025, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1051 - val_loss: 0.1784\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.17521\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005172705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1066 - val_loss: 0.1809\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.17521\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051475083, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1050 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.17521\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051224354, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1039 - val_loss: 0.1766\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.17521\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005097484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1048 - val_loss: 0.1782\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.17521\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005072654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1040 - val_loss: 0.1764\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.17521\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005047946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1028 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.17521\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005023357, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1030 - val_loss: 0.1803\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.17521\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049988885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1044 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.17521\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004974539, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1029 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.17521\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004950308, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1020 - val_loss: 0.1767\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.17521\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004926195, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0989 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.17521\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049022, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0994 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.17521\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048783212, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0976 - val_loss: 0.1790\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.17521\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048545588, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0994 - val_loss: 0.1771\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.17521\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048309125, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0964 - val_loss: 0.1784\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.17521\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048073815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0962 - val_loss: 0.1830\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.17521\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047839645, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0968 - val_loss: 0.1846\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.17521\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004760662, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0971 - val_loss: 0.1770\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.17521\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004737473, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0964 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.17521\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047143968, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0938 - val_loss: 0.1801\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.17521\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004691433, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0946 - val_loss: 0.1779\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.17521\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046685815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0937 - val_loss: 0.1790\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.17521\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046458407, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0933 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.17521\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046232107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 198ms/step - loss: 0.0947 - val_loss: 0.1833\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.17521\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046006907, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0941 - val_loss: 0.1776\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.17521\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045782814, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0909 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.17521\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045559806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0931 - val_loss: 0.1772\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.17521\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045337883, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0916 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.17521\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045117046, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0901 - val_loss: 0.1771\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.17521\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044897277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0913 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.17521\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044678585, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0908 - val_loss: 0.1847\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.17521\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004446096, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0892 - val_loss: 0.1768\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.17521\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004424439, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0862 - val_loss: 0.1769\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.17521\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044028874, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0866 - val_loss: 0.1768\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.17521\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043814414, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0859 - val_loss: 0.1759\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.17521\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043600993, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0867 - val_loss: 0.1802\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.17521\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043388613, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0858 - val_loss: 0.1790\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.17521\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004317727, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0836 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.17521\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004296695, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0843 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.17521\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004275766, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0841 - val_loss: 0.1810\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.17521\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042549393, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0821 - val_loss: 0.1839\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.17521\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042342133, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0836 - val_loss: 0.1782\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.17521\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042135885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0829 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.17521\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041930642, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0846 - val_loss: 0.1779\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.17521\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041726397, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0825 - val_loss: 0.1822\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.17521\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041523148, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0817 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.17521\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041320888, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0814 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.17521\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041119618, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1154 - val_loss: 0.1900\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.17521\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040919325, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1178 - val_loss: 0.1824\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.17521\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040720004, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0955 - val_loss: 0.1811\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.17521\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004052166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0898 - val_loss: 0.1819\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.17521\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004032428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0864 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.17521\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004012786, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0857 - val_loss: 0.1770\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.17521\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039932402, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0814 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.17521\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039737887, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0815 - val_loss: 0.1790\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.17521\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039544326, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0795 - val_loss: 0.1767\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.17521\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039351705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0786 - val_loss: 0.1767\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.17521\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039160028, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0762 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.17521\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038969278, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0781 - val_loss: 0.1824\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.17521\n",
      "Epoch 00194: early stopping\n",
      "--------------- > Fold 9 < ---------------\n",
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 80, 54)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_32 (Bidirectional (None, 80, 2048)     8839168     input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_32 (Attention)        (None, 80, 2048)     2128        bidirectional_32[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 80, 4096)     0           bidirectional_32[0][0]           \n",
      "                                                                 attention_32[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_33 (Bidirectional (None, 80, 1024)     18878464    concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_33 (Attention)        (None, 80, 1024)     1104        bidirectional_33[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 80, 2048)     0           bidirectional_33[0][0]           \n",
      "                                                                 attention_33[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_34 (Bidirectional (None, 80, 512)      4720640     concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_34 (Attention)        (None, 80, 512)      592         bidirectional_34[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 80, 1024)     0           bidirectional_34[0][0]           \n",
      "                                                                 attention_34[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_35 (Bidirectional (None, 80, 256)      1180672     concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_35 (Attention)        (None, 80, 256)      336         bidirectional_35[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 80, 512)      0           bidirectional_35[0][0]           \n",
      "                                                                 attention_35[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 80, 128)      65664       concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 80, 1)        129         dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "scale_layer_8 (ScaleLayer)      (None, 80, 1)        0           dense_17[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 33,688,897\n",
      "Trainable params: 33,688,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(0.001, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 62s 497ms/step - loss: 4.1832 - val_loss: 1.1094\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.10938, saving model to folds8.hdf5\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to tf.Tensor(0.000995129, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 1.0232 - val_loss: 0.7597\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.10938 to 0.75972, saving model to folds8.hdf5\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009902818, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 193ms/step - loss: 0.6974 - val_loss: 0.6061\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.75972 to 0.60605, saving model to folds8.hdf5\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009854581, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5627 - val_loss: 0.4857\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60605 to 0.48569, saving model to folds8.hdf5\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.000980658, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5074 - val_loss: 0.4712\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.48569 to 0.47119, saving model to folds8.hdf5\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097588124, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4750 - val_loss: 0.4746\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.47119\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009711277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4442 - val_loss: 0.4117\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.47119 to 0.41167, saving model to folds8.hdf5\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009663974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4189 - val_loss: 0.4244\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.41167\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009616901, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4134 - val_loss: 0.4275\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.41167\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009570057, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3902 - val_loss: 0.4029\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.41167 to 0.40292, saving model to folds8.hdf5\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095234415, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3718 - val_loss: 0.3575\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.40292 to 0.35753, saving model to folds8.hdf5\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009477053, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3603 - val_loss: 0.3657\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.35753\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009430891, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3461 - val_loss: 0.3821\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.35753\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009384953, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3448 - val_loss: 0.3279\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.35753 to 0.32785, saving model to folds8.hdf5\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009339239, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3272 - val_loss: 0.3275\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.32785 to 0.32748, saving model to folds8.hdf5\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009293747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3244 - val_loss: 0.3054\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.32748 to 0.30540, saving model to folds8.hdf5\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009248478, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3108 - val_loss: 0.3405\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.30540\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(0.00092034286, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3157 - val_loss: 0.3646\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.30540\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091585994, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3360 - val_loss: 0.3313\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.30540\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091139873, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3083 - val_loss: 0.3103\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.30540\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090695935, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2976 - val_loss: 0.3547\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.30540\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090254156, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3001 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.30540 to 0.27705, saving model to folds8.hdf5\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008981453, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2756 - val_loss: 0.3103\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.27705\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008937704, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2739 - val_loss: 0.3389\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.27705\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008894169, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2852 - val_loss: 0.2786\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.27705\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088508456, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2646 - val_loss: 0.2855\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.27705\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088077335, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2589 - val_loss: 0.2710\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.27705 to 0.27096, saving model to folds8.hdf5\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008764831, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2539 - val_loss: 0.2581\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.27096 to 0.25811, saving model to folds8.hdf5\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to tf.Tensor(0.00087221374, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2560 - val_loss: 0.2590\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.25811\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008679653, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2478 - val_loss: 0.2857\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.25811\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to tf.Tensor(0.00086373737, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2577 - val_loss: 0.2476\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.25811 to 0.24760, saving model to folds8.hdf5\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008595301, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2413 - val_loss: 0.2480\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.24760\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to tf.Tensor(0.00085534336, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2442 - val_loss: 0.2545\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.24760\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to tf.Tensor(0.000851177, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2418 - val_loss: 0.2744\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.24760\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to tf.Tensor(0.00084703095, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2333 - val_loss: 0.2412\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.24760 to 0.24123, saving model to folds8.hdf5\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008429051, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2298 - val_loss: 0.2336\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.24123 to 0.23356, saving model to folds8.hdf5\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to tf.Tensor(0.00083879934, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2211 - val_loss: 0.2536\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.23356\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008347136, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2302 - val_loss: 0.2449\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.23356\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008306477, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2339 - val_loss: 0.2408\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.23356\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008266016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2326 - val_loss: 0.2515\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.23356\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to tf.Tensor(0.00082257524, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2224 - val_loss: 0.2297\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.23356 to 0.22967, saving model to folds8.hdf5\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008185685, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2181 - val_loss: 0.2311\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.22967\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081458123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2187 - val_loss: 0.2247\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.22967 to 0.22469, saving model to folds8.hdf5\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081061345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2089 - val_loss: 0.2267\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.22469\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to tf.Tensor(0.000806665, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2097 - val_loss: 0.2271\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.22469\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to tf.Tensor(0.00080273574, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2151 - val_loss: 0.2276\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.22469\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007988256, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2159 - val_loss: 0.2509\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.22469\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007949346, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2149 - val_loss: 0.2466\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.22469\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007910624, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2159 - val_loss: 0.2378\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.22469\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007872092, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2079 - val_loss: 0.2276\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.22469\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007833747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 197ms/step - loss: 0.1993 - val_loss: 0.2105\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.22469 to 0.21054, saving model to folds8.hdf5\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077955885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1940 - val_loss: 0.2205\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.21054\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077576167, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2038 - val_loss: 0.2350\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.21054\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077198294, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2076 - val_loss: 0.2176\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.21054\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007682226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1941 - val_loss: 0.2171\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.21054\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007644806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1991 - val_loss: 0.2182\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.21054\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007607569, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1898 - val_loss: 0.2314\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.21054\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007570512, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2012 - val_loss: 0.2127\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.21054\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to tf.Tensor(0.00075336365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1914 - val_loss: 0.2136\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.21054\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to tf.Tensor(0.000749694, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1918 - val_loss: 0.2251\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.21054\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007460423, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1959 - val_loss: 0.2124\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.21054\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to tf.Tensor(0.00074240833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1920 - val_loss: 0.2111\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.21054\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007387921, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1857 - val_loss: 0.2072\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.21054 to 0.20720, saving model to folds8.hdf5\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to tf.Tensor(0.00073519343, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1819 - val_loss: 0.2018\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.20720 to 0.20179, saving model to folds8.hdf5\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007316123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1787 - val_loss: 0.2015\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.20179 to 0.20149, saving model to folds8.hdf5\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007280487, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1754 - val_loss: 0.1981\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.20149 to 0.19810, saving model to folds8.hdf5\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007245024, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1807 - val_loss: 0.1994\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.19810\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007209733, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 187ms/step - loss: 0.1807 - val_loss: 0.2007\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.19810\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071746146, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1810 - val_loss: 0.2005\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.19810\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007139667, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1751 - val_loss: 0.1924\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.19810 to 0.19235, saving model to folds8.hdf5\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071048894, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1760 - val_loss: 0.2024\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.19235\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007070282, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1749 - val_loss: 0.2059\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.19235\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007035842, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1767 - val_loss: 0.1957\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.19235\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to tf.Tensor(0.00070015714, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1688 - val_loss: 0.1907\n",
      "\n",
      "Epoch 00074: val_loss improved from 0.19235 to 0.19072, saving model to folds8.hdf5\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006967467, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1627 - val_loss: 0.2013\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.19072\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to tf.Tensor(0.00069335283, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1630 - val_loss: 0.2140\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.19072\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006899755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1681 - val_loss: 0.1980\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.19072\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068661466, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1593 - val_loss: 0.1887\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.19072 to 0.18870, saving model to folds8.hdf5\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068327016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1614 - val_loss: 0.2235\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.18870\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067994196, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1791 - val_loss: 0.1931\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.18870\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067663, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1662 - val_loss: 0.2007\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.18870\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006733342, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1705 - val_loss: 0.1910\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.18870\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067005435, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1637 - val_loss: 0.2006\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.18870\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006667905, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1635 - val_loss: 0.1960\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.18870\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006635426, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1621 - val_loss: 0.1921\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.18870\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006603105, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1565 - val_loss: 0.2006\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.18870\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006570942, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1659 - val_loss: 0.1991\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.18870\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065389345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1591 - val_loss: 0.1977\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.18870\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065070833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1552 - val_loss: 0.1886\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.18870 to 0.18857, saving model to folds8.hdf5\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to tf.Tensor(0.00064753875, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1531 - val_loss: 0.1915\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.18857\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006443846, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1520 - val_loss: 0.1893\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.18857\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006412458, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1625 - val_loss: 0.1928\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.18857\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063812226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1631 - val_loss: 0.1965\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.18857\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063501403, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 198ms/step - loss: 0.1569 - val_loss: 0.1990\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.18857\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006319209, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1517 - val_loss: 0.1976\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.18857\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006288428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1541 - val_loss: 0.1918\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.18857\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to tf.Tensor(0.00062577974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1503 - val_loss: 0.1821\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.18857 to 0.18207, saving model to folds8.hdf5\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006227316, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1476 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.18207\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061969826, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1438 - val_loss: 0.1839\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.18207\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006166797, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1417 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.18207\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061367586, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1411 - val_loss: 0.1887\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.18207\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061068666, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1433 - val_loss: 0.1791\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.18207 to 0.17914, saving model to folds8.hdf5\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006077121, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1381 - val_loss: 0.1829\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.17914\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060475187, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1410 - val_loss: 0.1920\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.17914\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060180615, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1418 - val_loss: 0.1800\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.17914\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059887476, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1404 - val_loss: 0.2006\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.17914\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005959577, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1491 - val_loss: 0.1822\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.17914\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059305475, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1412 - val_loss: 0.1860\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.17914\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to tf.Tensor(0.000590166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1382 - val_loss: 0.1812\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.17914\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to tf.Tensor(0.00058729126, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1337 - val_loss: 0.1821\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.17914\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005844306, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1322 - val_loss: 0.1844\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.17914\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005815838, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1339 - val_loss: 0.1863\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.17914\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to tf.Tensor(0.000578751, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1325 - val_loss: 0.1855\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.17914\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057593186, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1325 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.17914 to 0.17804, saving model to folds8.hdf5\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057312654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1319 - val_loss: 0.1815\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.17804\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057033484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1298 - val_loss: 0.1854\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.17804\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to tf.Tensor(0.00056755677, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1340 - val_loss: 0.1896\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.17804\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005647922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1462 - val_loss: 0.2060\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.17804\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005620411, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1543 - val_loss: 0.1888\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.17804\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055930344, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1360 - val_loss: 0.1807\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.17804\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055657903, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1357 - val_loss: 0.1870\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.17804\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055386795, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1353 - val_loss: 0.1856\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.17804\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005511701, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1323 - val_loss: 0.1816\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.17804\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005484854, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1325 - val_loss: 0.1838\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.17804\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005458137, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1314 - val_loss: 0.1981\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.17804\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to tf.Tensor(0.00054315507, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1279 - val_loss: 0.1743\n",
      "\n",
      "Epoch 00126: val_loss improved from 0.17804 to 0.17428, saving model to folds8.hdf5\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005405093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1249 - val_loss: 0.1757\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.17428\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005378765, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1222 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.17428\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005352565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1425 - val_loss: 0.1835\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.17428\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005326493, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1301 - val_loss: 0.1776\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.17428\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005300548, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1252 - val_loss: 0.1848\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.17428\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005274729, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1312 - val_loss: 0.1822\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.17428\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052490365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1251 - val_loss: 0.1819\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.17428\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052234676, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1265 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.17428\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005198025, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1207 - val_loss: 0.1758\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.17428\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005172705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1166 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.17428\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051475083, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 199ms/step - loss: 0.1187 - val_loss: 0.1837\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.17428\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051224354, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1198 - val_loss: 0.1758\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.17428\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005097484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1183 - val_loss: 0.1753\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.17428\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005072654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1179 - val_loss: 0.1744\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.17428\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005047946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1150 - val_loss: 0.1748\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.17428\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005023357, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1129 - val_loss: 0.1742\n",
      "\n",
      "Epoch 00142: val_loss improved from 0.17428 to 0.17419, saving model to folds8.hdf5\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049988885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1125 - val_loss: 0.1781\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.17419\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004974539, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1114 - val_loss: 0.1742\n",
      "\n",
      "Epoch 00144: val_loss improved from 0.17419 to 0.17415, saving model to folds8.hdf5\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004950308, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1107 - val_loss: 0.1728\n",
      "\n",
      "Epoch 00145: val_loss improved from 0.17415 to 0.17277, saving model to folds8.hdf5\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004926195, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1077 - val_loss: 0.1744\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.17277\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049022, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1091 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.17277\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048783212, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1147 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.17277\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048545588, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1089 - val_loss: 0.1771\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.17277\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048309125, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1092 - val_loss: 0.1926\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.17277\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048073815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1366 - val_loss: 0.1820\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.17277\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047839645, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1168 - val_loss: 0.1816\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.17277\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004760662, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1194 - val_loss: 0.1841\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.17277\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004737473, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1173 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.17277\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047143968, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1113 - val_loss: 0.1750\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.17277\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004691433, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1092 - val_loss: 0.1767\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.17277\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046685815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1066 - val_loss: 0.1757\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.17277\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046458407, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1032 - val_loss: 0.1743\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.17277\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046232107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1026 - val_loss: 0.1816\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.17277\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046006907, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1025 - val_loss: 0.1745\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.17277\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045782814, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1021 - val_loss: 0.1806\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.17277\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045559806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1035 - val_loss: 0.1736\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.17277\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045337883, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1004 - val_loss: 0.1726\n",
      "\n",
      "Epoch 00163: val_loss improved from 0.17277 to 0.17263, saving model to folds8.hdf5\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045117046, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1005 - val_loss: 0.1740\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.17263\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044897277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0991 - val_loss: 0.1745\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.17263\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044678585, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0967 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.17263\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004446096, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0992 - val_loss: 0.1741\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.17263\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004424439, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0963 - val_loss: 0.1727\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.17263\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044028874, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0957 - val_loss: 0.1726\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.17263\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043814414, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0958 - val_loss: 0.1749\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.17263\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043600993, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0966 - val_loss: 0.1720\n",
      "\n",
      "Epoch 00171: val_loss improved from 0.17263 to 0.17195, saving model to folds8.hdf5\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043388613, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0941 - val_loss: 0.1776\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.17195\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004317727, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0945 - val_loss: 0.1742\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.17195\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004296695, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0935 - val_loss: 0.1734\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.17195\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004275766, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0926 - val_loss: 0.1731\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.17195\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042549393, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0929 - val_loss: 0.1731\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.17195\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042342133, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0976 - val_loss: 0.1760\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.17195\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042135885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0940 - val_loss: 0.1816\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.17195\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041930642, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0959 - val_loss: 0.1740\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.17195\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041726397, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0934 - val_loss: 0.1789\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.17195\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041523148, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0933 - val_loss: 0.1755\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.17195\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041320888, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0892 - val_loss: 0.1731\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.17195\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041119618, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0887 - val_loss: 0.1742\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.17195\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040919325, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0894 - val_loss: 0.1747\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.17195\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040720004, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0886 - val_loss: 0.1746\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.17195\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004052166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0877 - val_loss: 0.1744\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.17195\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004032428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0867 - val_loss: 0.1723\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.17195\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004012786, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0874 - val_loss: 0.1751\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.17195\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039932402, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 198ms/step - loss: 0.0885 - val_loss: 0.1734\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.17195\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039737887, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0862 - val_loss: 0.1750\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.17195\n",
      "Epoch 191/300\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039544326, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0868 - val_loss: 0.1738\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 0.17195\n",
      "Epoch 192/300\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039351705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0852 - val_loss: 0.1751\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.17195\n",
      "Epoch 193/300\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039160028, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0842 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.17195\n",
      "Epoch 194/300\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038969278, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0861 - val_loss: 0.1748\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.17195\n",
      "Epoch 195/300\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003877946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0852 - val_loss: 0.1801\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.17195\n",
      "Epoch 196/300\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038590565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0863 - val_loss: 0.1759\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.17195\n",
      "Epoch 197/300\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003840259, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0879 - val_loss: 0.1781\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.17195\n",
      "Epoch 198/300\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038215535, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0851 - val_loss: 0.1785\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.17195\n",
      "Epoch 199/300\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to tf.Tensor(0.00038029387, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0829 - val_loss: 0.1769\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.17195\n",
      "Epoch 200/300\n",
      "\n",
      "Epoch 00200: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037844144, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0882 - val_loss: 0.1756\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.17195\n",
      "Epoch 201/300\n",
      "\n",
      "Epoch 00201: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037659804, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0808 - val_loss: 0.1785\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 0.17195\n",
      "Epoch 202/300\n",
      "\n",
      "Epoch 00202: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037476368, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0826 - val_loss: 0.1742\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 0.17195\n",
      "Epoch 203/300\n",
      "\n",
      "Epoch 00203: LearningRateScheduler reducing learning rate to tf.Tensor(0.00037293817, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0812 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 0.17195\n",
      "Epoch 204/300\n",
      "\n",
      "Epoch 00204: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003711216, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0811 - val_loss: 0.1778\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 0.17195\n",
      "Epoch 205/300\n",
      "\n",
      "Epoch 00205: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003693139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0804 - val_loss: 0.1754\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 0.17195\n",
      "Epoch 206/300\n",
      "\n",
      "Epoch 00206: LearningRateScheduler reducing learning rate to tf.Tensor(0.000367515, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0791 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 0.17195\n",
      "Epoch 207/300\n",
      "\n",
      "Epoch 00207: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036572482, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0792 - val_loss: 0.1793\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 0.17195\n",
      "Epoch 208/300\n",
      "\n",
      "Epoch 00208: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003639434, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0799 - val_loss: 0.1755\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 0.17195\n",
      "Epoch 209/300\n",
      "\n",
      "Epoch 00209: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036217063, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0798 - val_loss: 0.1732\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 0.17195\n",
      "Epoch 210/300\n",
      "\n",
      "Epoch 00210: LearningRateScheduler reducing learning rate to tf.Tensor(0.00036040647, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0762 - val_loss: 0.1751\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 0.17195\n",
      "Epoch 211/300\n",
      "\n",
      "Epoch 00211: LearningRateScheduler reducing learning rate to tf.Tensor(0.00035865093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0768 - val_loss: 0.1740\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 0.17195\n",
      "Epoch 212/300\n",
      "\n",
      "Epoch 00212: LearningRateScheduler reducing learning rate to tf.Tensor(0.000356904, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0765 - val_loss: 0.1860\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 0.17195\n",
      "Epoch 213/300\n",
      "\n",
      "Epoch 00213: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003551655, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0863 - val_loss: 0.1863\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 0.17195\n",
      "Epoch 214/300\n",
      "\n",
      "Epoch 00214: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003534355, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0870 - val_loss: 0.1747\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 0.17195\n",
      "Epoch 215/300\n",
      "\n",
      "Epoch 00215: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003517139, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0768 - val_loss: 0.1747\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 0.17195\n",
      "Epoch 216/300\n",
      "\n",
      "Epoch 00216: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003500007, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0751 - val_loss: 0.1754\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 0.17195\n",
      "Epoch 217/300\n",
      "\n",
      "Epoch 00217: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034829587, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0740 - val_loss: 0.1755\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 0.17195\n",
      "Epoch 218/300\n",
      "\n",
      "Epoch 00218: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034659932, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0741 - val_loss: 0.1761\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 0.17195\n",
      "Epoch 219/300\n",
      "\n",
      "Epoch 00219: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034491107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0749 - val_loss: 0.1772\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.17195\n",
      "Epoch 220/300\n",
      "\n",
      "Epoch 00220: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034323102, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0742 - val_loss: 0.1748\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.17195\n",
      "Epoch 221/300\n",
      "\n",
      "Epoch 00221: LearningRateScheduler reducing learning rate to tf.Tensor(0.00034155912, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0735 - val_loss: 0.1765\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.17195\n",
      "Epoch 222/300\n",
      "\n",
      "Epoch 00222: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033989537, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0723 - val_loss: 0.1752\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.17195\n",
      "Epoch 223/300\n",
      "\n",
      "Epoch 00223: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033823974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0734 - val_loss: 0.1739\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.17195\n",
      "Epoch 224/300\n",
      "\n",
      "Epoch 00224: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003365922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0711 - val_loss: 0.1755\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.17195\n",
      "Epoch 225/300\n",
      "\n",
      "Epoch 00225: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033495267, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0708 - val_loss: 0.1738\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 0.17195\n",
      "Epoch 226/300\n",
      "\n",
      "Epoch 00226: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003333211, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0713 - val_loss: 0.1745\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.17195\n",
      "Epoch 227/300\n",
      "\n",
      "Epoch 00227: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033169755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0708 - val_loss: 0.1753\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.17195\n",
      "Epoch 228/300\n",
      "\n",
      "Epoch 00228: LearningRateScheduler reducing learning rate to tf.Tensor(0.00033008185, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0707 - val_loss: 0.1738\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.17195\n",
      "Epoch 229/300\n",
      "\n",
      "Epoch 00229: LearningRateScheduler reducing learning rate to tf.Tensor(0.000328474, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0693 - val_loss: 0.1742\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.17195\n",
      "Epoch 230/300\n",
      "\n",
      "Epoch 00230: LearningRateScheduler reducing learning rate to tf.Tensor(0.00032687403, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0692 - val_loss: 0.1775\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.17195\n",
      "Epoch 231/300\n",
      "\n",
      "Epoch 00231: LearningRateScheduler reducing learning rate to tf.Tensor(0.0003252818, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0693 - val_loss: 0.1790\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.17195\n",
      "Epoch 00231: early stopping\n",
      "--------------- > Fold 10 < ---------------\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 80, 54)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_36 (Bidirectional (None, 80, 2048)     8839168     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_36 (Attention)        (None, 80, 2048)     2128        bidirectional_36[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_36 (Concatenate)    (None, 80, 4096)     0           bidirectional_36[0][0]           \n",
      "                                                                 attention_36[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_37 (Bidirectional (None, 80, 1024)     18878464    concatenate_36[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_37 (Attention)        (None, 80, 1024)     1104        bidirectional_37[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_37 (Concatenate)    (None, 80, 2048)     0           bidirectional_37[0][0]           \n",
      "                                                                 attention_37[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_38 (Bidirectional (None, 80, 512)      4720640     concatenate_37[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_38 (Attention)        (None, 80, 512)      592         bidirectional_38[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_38 (Concatenate)    (None, 80, 1024)     0           bidirectional_38[0][0]           \n",
      "                                                                 attention_38[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_39 (Bidirectional (None, 80, 256)      1180672     concatenate_38[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "attention_39 (Attention)        (None, 80, 256)      336         bidirectional_39[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_39 (Concatenate)    (None, 80, 512)      0           bidirectional_39[0][0]           \n",
      "                                                                 attention_39[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 80, 128)      65664       concatenate_39[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 80, 1)        129         dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "scale_layer_9 (ScaleLayer)      (None, 80, 1)        0           dense_19[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 33,688,897\n",
      "Trainable params: 33,688,897\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/300\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to tf.Tensor(0.001, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 62s 501ms/step - loss: 4.0922 - val_loss: 1.1620\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.16198, saving model to folds9.hdf5\n",
      "Epoch 2/300\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to tf.Tensor(0.000995129, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 1.0488 - val_loss: 0.8119\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.16198 to 0.81191, saving model to folds9.hdf5\n",
      "Epoch 3/300\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009902818, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.6935 - val_loss: 0.6092\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.81191 to 0.60916, saving model to folds9.hdf5\n",
      "Epoch 4/300\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009854581, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5854 - val_loss: 0.5020\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.60916 to 0.50199, saving model to folds9.hdf5\n",
      "Epoch 5/300\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to tf.Tensor(0.000980658, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.5174 - val_loss: 0.4493\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.50199 to 0.44930, saving model to folds9.hdf5\n",
      "Epoch 6/300\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to tf.Tensor(0.00097588124, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4644 - val_loss: 0.4219\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.44930 to 0.42191, saving model to folds9.hdf5\n",
      "Epoch 7/300\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009711277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4328 - val_loss: 0.4089\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.42191 to 0.40886, saving model to folds9.hdf5\n",
      "Epoch 8/300\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009663974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.4135 - val_loss: 0.4181\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.40886\n",
      "Epoch 9/300\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009616901, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.4135 - val_loss: 0.4302\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.40886\n",
      "Epoch 10/300\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009570057, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3893 - val_loss: 0.4363\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.40886\n",
      "Epoch 11/300\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to tf.Tensor(0.00095234415, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.3858 - val_loss: 0.4231\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.40886\n",
      "Epoch 12/300\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009477053, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3772 - val_loss: 0.3687\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.40886 to 0.36871, saving model to folds9.hdf5\n",
      "Epoch 13/300\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009430891, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3570 - val_loss: 0.3527\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.36871 to 0.35272, saving model to folds9.hdf5\n",
      "Epoch 14/300\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009384953, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3460 - val_loss: 0.3244\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.35272 to 0.32438, saving model to folds9.hdf5\n",
      "Epoch 15/300\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009339239, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3235 - val_loss: 0.3261\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.32438\n",
      "Epoch 16/300\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009293747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 194ms/step - loss: 0.3171 - val_loss: 0.3160\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.32438 to 0.31603, saving model to folds9.hdf5\n",
      "Epoch 17/300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to tf.Tensor(0.0009248478, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3098 - val_loss: 0.3106\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.31603 to 0.31063, saving model to folds9.hdf5\n",
      "Epoch 18/300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to tf.Tensor(0.00092034286, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.3166 - val_loss: 0.3196\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.31063\n",
      "Epoch 19/300\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091585994, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2970 - val_loss: 0.2977\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.31063 to 0.29774, saving model to folds9.hdf5\n",
      "Epoch 20/300\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to tf.Tensor(0.00091139873, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2986 - val_loss: 0.3089\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.29774\n",
      "Epoch 21/300\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090695935, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2892 - val_loss: 0.3046\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.29774\n",
      "Epoch 22/300\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to tf.Tensor(0.00090254156, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2827 - val_loss: 0.2843\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.29774 to 0.28426, saving model to folds9.hdf5\n",
      "Epoch 23/300\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008981453, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2667 - val_loss: 0.2665\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.28426 to 0.26652, saving model to folds9.hdf5\n",
      "Epoch 24/300\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008937704, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2595 - val_loss: 0.2732\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.26652\n",
      "Epoch 25/300\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008894169, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2583 - val_loss: 0.2959\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.26652\n",
      "Epoch 26/300\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088508456, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2624 - val_loss: 0.2584\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.26652 to 0.25840, saving model to folds9.hdf5\n",
      "Epoch 27/300\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to tf.Tensor(0.00088077335, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2505 - val_loss: 0.2623\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.25840\n",
      "Epoch 28/300\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008764831, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2515 - val_loss: 0.2526\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.25840 to 0.25255, saving model to folds9.hdf5\n",
      "Epoch 29/300\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to tf.Tensor(0.00087221374, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 192ms/step - loss: 0.2519 - val_loss: 0.2544\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.25255\n",
      "Epoch 30/300\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008679653, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2430 - val_loss: 0.2470\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.25255 to 0.24700, saving model to folds9.hdf5\n",
      "Epoch 31/300\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to tf.Tensor(0.00086373737, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2410 - val_loss: 0.2461\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.24700 to 0.24606, saving model to folds9.hdf5\n",
      "Epoch 32/300\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008595301, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2384 - val_loss: 0.2434\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.24606 to 0.24342, saving model to folds9.hdf5\n",
      "Epoch 33/300\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to tf.Tensor(0.00085534336, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2376 - val_loss: 0.2594\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.24342\n",
      "Epoch 34/300\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to tf.Tensor(0.000851177, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2346 - val_loss: 0.2421\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.24342 to 0.24215, saving model to folds9.hdf5\n",
      "Epoch 35/300\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to tf.Tensor(0.00084703095, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2336 - val_loss: 0.2415\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.24215 to 0.24153, saving model to folds9.hdf5\n",
      "Epoch 36/300\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008429051, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2405 - val_loss: 0.2538\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.24153\n",
      "Epoch 37/300\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to tf.Tensor(0.00083879934, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2203 - val_loss: 0.2406\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.24153 to 0.24056, saving model to folds9.hdf5\n",
      "Epoch 38/300\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008347136, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2185 - val_loss: 0.2577\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.24056\n",
      "Epoch 39/300\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008306477, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2242 - val_loss: 0.2406\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.24056\n",
      "Epoch 40/300\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008266016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2217 - val_loss: 0.2344\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.24056 to 0.23438, saving model to folds9.hdf5\n",
      "Epoch 41/300\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to tf.Tensor(0.00082257524, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2179 - val_loss: 0.2379\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.23438\n",
      "Epoch 42/300\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to tf.Tensor(0.0008185685, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2108 - val_loss: 0.2231\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.23438 to 0.22312, saving model to folds9.hdf5\n",
      "Epoch 43/300\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081458123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2158 - val_loss: 0.2229\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.22312 to 0.22294, saving model to folds9.hdf5\n",
      "Epoch 44/300\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to tf.Tensor(0.00081061345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2076 - val_loss: 0.2281\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.22294\n",
      "Epoch 45/300\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to tf.Tensor(0.000806665, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2034 - val_loss: 0.2175\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.22294 to 0.21747, saving model to folds9.hdf5\n",
      "Epoch 46/300\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to tf.Tensor(0.00080273574, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2049 - val_loss: 0.2231\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.21747\n",
      "Epoch 47/300\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007988256, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2032 - val_loss: 0.2247\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.21747\n",
      "Epoch 48/300\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007949346, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2085 - val_loss: 0.2336\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.21747\n",
      "Epoch 49/300\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007910624, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2006 - val_loss: 0.2232\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.21747\n",
      "Epoch 50/300\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007872092, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1971 - val_loss: 0.2434\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.21747\n",
      "Epoch 51/300\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007833747, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.2079 - val_loss: 0.2171\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.21747 to 0.21714, saving model to folds9.hdf5\n",
      "Epoch 52/300\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077955885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2169 - val_loss: 0.2255\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.21714\n",
      "Epoch 53/300\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077576167, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2073 - val_loss: 0.2132\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.21714 to 0.21316, saving model to folds9.hdf5\n",
      "Epoch 54/300\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to tf.Tensor(0.00077198294, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1956 - val_loss: 0.2202\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.21316\n",
      "Epoch 55/300\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007682226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1959 - val_loss: 0.2144\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.21316\n",
      "Epoch 56/300\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007644806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1981 - val_loss: 0.2246\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.21316\n",
      "Epoch 57/300\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007607569, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.2024 - val_loss: 0.2121\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.21316 to 0.21211, saving model to folds9.hdf5\n",
      "Epoch 58/300\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007570512, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1887 - val_loss: 0.2138\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.21211\n",
      "Epoch 59/300\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to tf.Tensor(0.00075336365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1915 - val_loss: 0.2156\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.21211\n",
      "Epoch 60/300\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to tf.Tensor(0.000749694, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1847 - val_loss: 0.2023\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.21211 to 0.20230, saving model to folds9.hdf5\n",
      "Epoch 61/300\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007460423, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1821 - val_loss: 0.2205\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.20230\n",
      "Epoch 62/300\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to tf.Tensor(0.00074240833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1826 - val_loss: 0.2123\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.20230\n",
      "Epoch 63/300\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007387921, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1859 - val_loss: 0.2077\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.20230\n",
      "Epoch 64/300\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to tf.Tensor(0.00073519343, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1814 - val_loss: 0.2184\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.20230\n",
      "Epoch 65/300\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007316123, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1852 - val_loss: 0.2172\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.20230\n",
      "Epoch 66/300\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007280487, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1856 - val_loss: 0.2086\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.20230\n",
      "Epoch 67/300\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007245024, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1763 - val_loss: 0.2103\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.20230\n",
      "Epoch 68/300\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007209733, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1769 - val_loss: 0.1952\n",
      "\n",
      "Epoch 00068: val_loss improved from 0.20230 to 0.19516, saving model to folds9.hdf5\n",
      "Epoch 69/300\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071746146, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 198ms/step - loss: 0.1714 - val_loss: 0.2166\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.19516\n",
      "Epoch 70/300\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007139667, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1750 - val_loss: 0.2070\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.19516\n",
      "Epoch 71/300\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to tf.Tensor(0.00071048894, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1731 - val_loss: 0.1943\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.19516 to 0.19430, saving model to folds9.hdf5\n",
      "Epoch 72/300\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007070282, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1707 - val_loss: 0.2139\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.19430\n",
      "Epoch 73/300\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to tf.Tensor(0.0007035842, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1702 - val_loss: 0.1889\n",
      "\n",
      "Epoch 00073: val_loss improved from 0.19430 to 0.18885, saving model to folds9.hdf5\n",
      "Epoch 74/300\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to tf.Tensor(0.00070015714, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1680 - val_loss: 0.1944\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.18885\n",
      "Epoch 75/300\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006967467, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1698 - val_loss: 0.1985\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.18885\n",
      "Epoch 76/300\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to tf.Tensor(0.00069335283, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1703 - val_loss: 0.2069\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.18885\n",
      "Epoch 77/300\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006899755, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1763 - val_loss: 0.2000\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.18885\n",
      "Epoch 78/300\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068661466, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1697 - val_loss: 0.1996\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.18885\n",
      "Epoch 79/300\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to tf.Tensor(0.00068327016, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1725 - val_loss: 0.2039\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.18885\n",
      "Epoch 80/300\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067994196, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1647 - val_loss: 0.1959\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.18885\n",
      "Epoch 81/300\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067663, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1600 - val_loss: 0.1869\n",
      "\n",
      "Epoch 00081: val_loss improved from 0.18885 to 0.18695, saving model to folds9.hdf5\n",
      "Epoch 82/300\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006733342, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1546 - val_loss: 0.1991\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.18695\n",
      "Epoch 83/300\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to tf.Tensor(0.00067005435, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1556 - val_loss: 0.1842\n",
      "\n",
      "Epoch 00083: val_loss improved from 0.18695 to 0.18419, saving model to folds9.hdf5\n",
      "Epoch 84/300\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006667905, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1552 - val_loss: 0.1886\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.18419\n",
      "Epoch 85/300\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006635426, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1544 - val_loss: 0.1872\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.18419\n",
      "Epoch 86/300\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006603105, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1576 - val_loss: 0.1899\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.18419\n",
      "Epoch 87/300\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006570942, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1611 - val_loss: 0.2010\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.18419\n",
      "Epoch 88/300\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065389345, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1586 - val_loss: 0.1906\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.18419\n",
      "Epoch 89/300\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to tf.Tensor(0.00065070833, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1516 - val_loss: 0.1881\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.18419\n",
      "Epoch 90/300\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to tf.Tensor(0.00064753875, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1481 - val_loss: 0.1886\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.18419\n",
      "Epoch 91/300\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006443846, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1539 - val_loss: 0.1898\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.18419\n",
      "Epoch 92/300\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006412458, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1523 - val_loss: 0.1916\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.18419\n",
      "Epoch 93/300\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063812226, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1514 - val_loss: 0.1865\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.18419\n",
      "Epoch 94/300\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to tf.Tensor(0.00063501403, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1507 - val_loss: 0.1861\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.18419\n",
      "Epoch 95/300\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006319209, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1483 - val_loss: 0.1889\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.18419\n",
      "Epoch 96/300\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006288428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1494 - val_loss: 0.1859\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.18419\n",
      "Epoch 97/300\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to tf.Tensor(0.00062577974, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 200ms/step - loss: 0.1481 - val_loss: 0.1877\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.18419\n",
      "Epoch 98/300\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006227316, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1457 - val_loss: 0.1881\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.18419\n",
      "Epoch 99/300\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061969826, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1437 - val_loss: 0.1972\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.18419\n",
      "Epoch 100/300\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006166797, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1457 - val_loss: 0.1840\n",
      "\n",
      "Epoch 00100: val_loss improved from 0.18419 to 0.18396, saving model to folds9.hdf5\n",
      "Epoch 101/300\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061367586, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1454 - val_loss: 0.1836\n",
      "\n",
      "Epoch 00101: val_loss improved from 0.18396 to 0.18357, saving model to folds9.hdf5\n",
      "Epoch 102/300\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to tf.Tensor(0.00061068666, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1381 - val_loss: 0.1910\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.18357\n",
      "Epoch 103/300\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to tf.Tensor(0.0006077121, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1420 - val_loss: 0.1846\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.18357\n",
      "Epoch 104/300\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060475187, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1388 - val_loss: 0.1862\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.18357\n",
      "Epoch 105/300\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to tf.Tensor(0.00060180615, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1418 - val_loss: 0.1849\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.18357\n",
      "Epoch 106/300\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059887476, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1332 - val_loss: 0.1861\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 0.18357\n",
      "Epoch 107/300\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005959577, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1399 - val_loss: 0.1876\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.18357\n",
      "Epoch 108/300\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to tf.Tensor(0.00059305475, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1372 - val_loss: 0.1825\n",
      "\n",
      "Epoch 00108: val_loss improved from 0.18357 to 0.18249, saving model to folds9.hdf5\n",
      "Epoch 109/300\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to tf.Tensor(0.000590166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1351 - val_loss: 0.1885\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.18249\n",
      "Epoch 110/300\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to tf.Tensor(0.00058729126, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1441 - val_loss: 0.1925\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.18249\n",
      "Epoch 111/300\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005844306, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1374 - val_loss: 0.1835\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.18249\n",
      "Epoch 112/300\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005815838, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 201ms/step - loss: 0.1379 - val_loss: 0.1871\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 0.18249\n",
      "Epoch 113/300\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to tf.Tensor(0.000578751, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1347 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.18249\n",
      "Epoch 114/300\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057593186, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1323 - val_loss: 0.1808\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.18249 to 0.18079, saving model to folds9.hdf5\n",
      "Epoch 115/300\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057312654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1321 - val_loss: 0.1817\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.18079\n",
      "Epoch 116/300\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to tf.Tensor(0.00057033484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1277 - val_loss: 0.1825\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.18079\n",
      "Epoch 117/300\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to tf.Tensor(0.00056755677, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1256 - val_loss: 0.1824\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.18079\n",
      "Epoch 118/300\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005647922, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1306 - val_loss: 0.2006\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.18079\n",
      "Epoch 119/300\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005620411, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1303 - val_loss: 0.1798\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.18079 to 0.17976, saving model to folds9.hdf5\n",
      "Epoch 120/300\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055930344, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1264 - val_loss: 0.1833\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.17976\n",
      "Epoch 121/300\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055657903, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1261 - val_loss: 0.1815\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.17976\n",
      "Epoch 122/300\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to tf.Tensor(0.00055386795, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1275 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00122: val_loss improved from 0.17976 to 0.17881, saving model to folds9.hdf5\n",
      "Epoch 123/300\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005511701, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1270 - val_loss: 0.1786\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.17881 to 0.17859, saving model to folds9.hdf5\n",
      "Epoch 124/300\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005484854, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1196 - val_loss: 0.1822\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.17859\n",
      "Epoch 125/300\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005458137, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1246 - val_loss: 0.1768\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.17859 to 0.17677, saving model to folds9.hdf5\n",
      "Epoch 126/300\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to tf.Tensor(0.00054315507, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1195 - val_loss: 0.1813\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.17677\n",
      "Epoch 127/300\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005405093, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1238 - val_loss: 0.1801\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.17677\n",
      "Epoch 128/300\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005378765, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1198 - val_loss: 0.1792\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 0.17677\n",
      "Epoch 129/300\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005352565, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1202 - val_loss: 0.1821\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 0.17677\n",
      "Epoch 130/300\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005326493, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1204 - val_loss: 0.1751\n",
      "\n",
      "Epoch 00130: val_loss improved from 0.17677 to 0.17508, saving model to folds9.hdf5\n",
      "Epoch 131/300\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005300548, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1180 - val_loss: 0.1769\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.17508\n",
      "Epoch 132/300\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005274729, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1160 - val_loss: 0.1772\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.17508\n",
      "Epoch 133/300\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052490365, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1243 - val_loss: 0.1852\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 0.17508\n",
      "Epoch 134/300\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to tf.Tensor(0.00052234676, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1245 - val_loss: 0.1837\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.17508\n",
      "Epoch 135/300\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005198025, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1214 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.17508\n",
      "Epoch 136/300\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005172705, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1329 - val_loss: 0.1891\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.17508\n",
      "Epoch 137/300\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051475083, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1300 - val_loss: 0.1810\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.17508\n",
      "Epoch 138/300\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to tf.Tensor(0.00051224354, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1207 - val_loss: 0.1846\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.17508\n",
      "Epoch 139/300\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005097484, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.1186 - val_loss: 0.1848\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 0.17508\n",
      "Epoch 140/300\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005072654, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1258 - val_loss: 0.1834\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.17508\n",
      "Epoch 141/300\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005047946, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1205 - val_loss: 0.1777\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.17508\n",
      "Epoch 142/300\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to tf.Tensor(0.0005023357, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1142 - val_loss: 0.1788\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.17508\n",
      "Epoch 143/300\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049988885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1111 - val_loss: 0.1796\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.17508\n",
      "Epoch 144/300\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004974539, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1101 - val_loss: 0.1924\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.17508\n",
      "Epoch 145/300\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004950308, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1173 - val_loss: 0.1826\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.17508\n",
      "Epoch 146/300\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004926195, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1130 - val_loss: 0.1818\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.17508\n",
      "Epoch 147/300\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to tf.Tensor(0.00049022, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1119 - val_loss: 0.1755\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.17508\n",
      "Epoch 148/300\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048783212, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1108 - val_loss: 0.1805\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.17508\n",
      "Epoch 149/300\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048545588, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1059 - val_loss: 0.1764\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.17508\n",
      "Epoch 150/300\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048309125, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1043 - val_loss: 0.1831\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.17508\n",
      "Epoch 151/300\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to tf.Tensor(0.00048073815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1050 - val_loss: 0.1756\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.17508\n",
      "Epoch 152/300\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047839645, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1041 - val_loss: 0.1758\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.17508\n",
      "Epoch 153/300\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004760662, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1015 - val_loss: 0.1789\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.17508\n",
      "Epoch 154/300\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004737473, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1011 - val_loss: 0.1758\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 0.17508\n",
      "Epoch 155/300\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to tf.Tensor(0.00047143968, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1025 - val_loss: 0.1754\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.17508\n",
      "Epoch 156/300\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004691433, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1009 - val_loss: 0.1869\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.17508\n",
      "Epoch 157/300\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046685815, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.1019 - val_loss: 0.1779\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.17508\n",
      "Epoch 158/300\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046458407, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 191ms/step - loss: 0.0989 - val_loss: 0.1823\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.17508\n",
      "Epoch 159/300\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046232107, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 199ms/step - loss: 0.1010 - val_loss: 0.1838\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.17508\n",
      "Epoch 160/300\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to tf.Tensor(0.00046006907, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.1000 - val_loss: 0.1783\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.17508\n",
      "Epoch 161/300\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045782814, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0999 - val_loss: 0.1805\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.17508\n",
      "Epoch 162/300\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045559806, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0986 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.17508\n",
      "Epoch 163/300\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045337883, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0966 - val_loss: 0.1862\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.17508\n",
      "Epoch 164/300\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to tf.Tensor(0.00045117046, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0977 - val_loss: 0.1771\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.17508\n",
      "Epoch 165/300\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044897277, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 190ms/step - loss: 0.0960 - val_loss: 0.1792\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.17508\n",
      "Epoch 166/300\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044678585, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0977 - val_loss: 0.1768\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.17508\n",
      "Epoch 167/300\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004446096, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0957 - val_loss: 0.1782\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.17508\n",
      "Epoch 168/300\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004424439, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0950 - val_loss: 0.1774\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 0.17508\n",
      "Epoch 169/300\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to tf.Tensor(0.00044028874, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1042 - val_loss: 0.1799\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.17508\n",
      "Epoch 170/300\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043814414, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0997 - val_loss: 0.1803\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.17508\n",
      "Epoch 171/300\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043600993, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0970 - val_loss: 0.1796\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.17508\n",
      "Epoch 172/300\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to tf.Tensor(0.00043388613, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0959 - val_loss: 0.1815\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.17508\n",
      "Epoch 173/300\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004317727, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0931 - val_loss: 0.1771\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.17508\n",
      "Epoch 174/300\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004296695, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0946 - val_loss: 0.1787\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.17508\n",
      "Epoch 175/300\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004275766, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0981 - val_loss: 0.1897\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.17508\n",
      "Epoch 176/300\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042549393, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1028 - val_loss: 0.1819\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.17508\n",
      "Epoch 177/300\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042342133, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0937 - val_loss: 0.1914\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.17508\n",
      "Epoch 178/300\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to tf.Tensor(0.00042135885, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.1019 - val_loss: 0.1810\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.17508\n",
      "Epoch 179/300\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041930642, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0929 - val_loss: 0.1773\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.17508\n",
      "Epoch 180/300\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041726397, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0902 - val_loss: 0.1828\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.17508\n",
      "Epoch 181/300\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041523148, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0890 - val_loss: 0.1798\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.17508\n",
      "Epoch 182/300\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041320888, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0890 - val_loss: 0.1769\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 0.17508\n",
      "Epoch 183/300\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to tf.Tensor(0.00041119618, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0886 - val_loss: 0.1770\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 0.17508\n",
      "Epoch 184/300\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040919325, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 189ms/step - loss: 0.0887 - val_loss: 0.1827\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.17508\n",
      "Epoch 185/300\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to tf.Tensor(0.00040720004, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0885 - val_loss: 0.1853\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.17508\n",
      "Epoch 186/300\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004052166, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0880 - val_loss: 0.1760\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.17508\n",
      "Epoch 187/300\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004032428, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0854 - val_loss: 0.1775\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.17508\n",
      "Epoch 188/300\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to tf.Tensor(0.0004012786, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0855 - val_loss: 0.1780\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.17508\n",
      "Epoch 189/300\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039932402, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0872 - val_loss: 0.1767\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.17508\n",
      "Epoch 190/300\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to tf.Tensor(0.00039737887, shape=(), dtype=float32).\n",
      "67/67 [==============================] - 13s 188ms/step - loss: 0.0850 - val_loss: 0.1816\n",
      "Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.17508\n",
      "Epoch 00190: early stopping\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 300\n",
    "BATCH_SIZE = 1024\n",
    "NUM_FOLDS = 10\n",
    "\n",
    "# detect and init the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "\n",
    "# instantiate a distribution strategy\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "#GPU init\n",
    "#gpu_strategy = tf.distribute.get_strategy()\n",
    "\n",
    "#with gpu_strategy.scope():\n",
    "with tpu_strategy.scope():\n",
    "    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=2021)\n",
    "    test_preds = []\n",
    "    \n",
    "        \n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n",
    "        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n",
    "        X_train, X_valid = train[train_idx], train[test_idx]\n",
    "        y_train, y_valid = targets[train_idx], targets[test_idx]\n",
    "\n",
    "        model = BiLSTM_model()\n",
    "        \n",
    "        scheduler = ExponentialDecay(1e-3, 40*((len(train)*0.8)/BATCH_SIZE), 1e-5)\n",
    "        lr = LearningRateScheduler(scheduler, verbose=1)\n",
    "        \n",
    "        #lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n",
    "        #lr = WarmupExponentialDecay(lr_base=1e-3, decay=1e-5, warmup_epochs=30)\n",
    "        es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, mode=\"min\", restore_best_weights=True)\n",
    "    \n",
    "        checkpoint_filepath = f\"folds{fold}.hdf5\"\n",
    "        sv = keras.callbacks.ModelCheckpoint(\n",
    "            checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n",
    "            save_weights_only=False, mode='auto', save_freq='epoch',\n",
    "            options=None\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr, es, sv])\n",
    "        #model.save(f'Fold{fold+1} RNN Weights')\n",
    "        test_preds.append(model.predict(test).squeeze().reshape(-1, 1).squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0a9da",
   "metadata": {
    "papermill": {
     "duration": 46.415669,
     "end_time": "2021-10-10T12:37:05.620396",
     "exception": false,
     "start_time": "2021-10-10T12:36:19.204727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Median method from [Chris Deotte](https://www.kaggle.com/cdeotte/ensemble-folds-with-median-0-153)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6a1c553",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T12:38:38.743155Z",
     "iopub.status.busy": "2021-10-10T12:38:38.742502Z",
     "iopub.status.idle": "2021-10-10T12:39:18.880910Z",
     "shell.execute_reply": "2021-10-10T12:39:18.881475Z"
    },
    "papermill": {
     "duration": 86.774057,
     "end_time": "2021-10-10T12:39:18.881685",
     "exception": false,
     "start_time": "2021-10-10T12:37:52.107628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission[\"pressure\"] = sum(test_preds)/NUM_FOLDS\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# ENSEMBLE FOLDS WITH MEDIAN\n",
    "#å–ä¸­ä½æ•°\n",
    "submission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\n",
    "submission.to_csv('submission_median.csv', index=False)\n",
    "\n",
    "\n",
    "# ENSEMBLE FOLDS WITH MEDIAN AND ROUND PREDICTIONS\n",
    "submission[\"pressure\"] =\\\n",
    "    np.round( (submission.pressure - pressure_min)/pressure_step ) * pressure_step + pressure_min\n",
    "submission.pressure = np.clip(submission.pressure, pressure_min, pressure_max)\n",
    "submission.to_csv('submission_median_round.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aad105",
   "metadata": {
    "papermill": {
     "duration": 46.532435,
     "end_time": "2021-10-10T12:40:52.966726",
     "exception": false,
     "start_time": "2021-10-10T12:40:06.434291",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30505.971461,
   "end_time": "2021-10-10T12:41:42.389980",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-10T04:13:16.418519",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
